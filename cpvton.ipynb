{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6683799,"sourceType":"datasetVersion","datasetId":3855472},{"sourceId":11754157,"sourceType":"datasetVersion","datasetId":7379123},{"sourceId":12491233,"sourceType":"datasetVersion","datasetId":7882684},{"sourceId":12498247,"sourceType":"datasetVersion","datasetId":7887744}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T12:21:55.386246Z","iopub.execute_input":"2025-07-17T12:21:55.386567Z","iopub.status.idle":"2025-07-17T12:21:55.394580Z","shell.execute_reply.started":"2025-07-17T12:21:55.386544Z","shell.execute_reply":"2025-07-17T12:21:55.394043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T12:21:57.087589Z","iopub.execute_input":"2025-07-17T12:21:57.087850Z","iopub.status.idle":"2025-07-17T12:23:42.428157Z","shell.execute_reply.started":"2025-07-17T12:21:57.087832Z","shell.execute_reply":"2025-07-17T12:23:42.426861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dataclasses import dataclass, asdict\n\n# @dataclass\n# class Config:\n#     checkpoint: str = '../result/GMM/epoch_99.pth'\n#     data_root: str   = 'data'\n#     out_dir: str     = '../result'\n#     name: str        = 'GMM'\n#     batch_size: int  = 16\n#     n_worker: int    = 16\n#     gpu_id: str      = '0'\n#     log_freq: int    = 100\n#     fine_width: int  = 192\n#     fine_height: int = 256\n#     radius: int      = 5\n#     grid_size: int   = 5\n\n\n@dataclass\nclass Config:\n    checkpoint_gmm: str   = '/kaggle/working/result/GMM/epoch_00.pth'\n    checkpoint_tom: str   = '/kaggle/working/result/TOM/gen_epoch_00.pth'\n    n_epoch: int          = 1\n    data_root: str        = '/kaggle/input/clothes_tryon_dataset/'\n    out_dir: str          = '/kaggle/working/result/'\n    name_gmm: str         = 'GMM'\n    name_tom: str         = 'TOM'\n    batch_size: int       = 2\n    n_worker: int         = 4\n    gpu_id: str           = '0'\n    log_freq: int         = 100\n    fine_width: int       = 192\n    fine_height: int      = 256\n    radius: int           = 5\n    grid_size: int        = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:44:56.965794Z","iopub.execute_input":"2025-07-17T14:44:56.966624Z","iopub.status.idle":"2025-07-17T14:44:56.975037Z","shell.execute_reply.started":"2025-07-17T14:44:56.966597Z","shell.execute_reply":"2025-07-17T14:44:56.974342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nfrom PIL import Image\nimport os\n\ndef tensor_for_board(img_tensor):\n    # map into [0,1]\n    tensor = (img_tensor.clone()+1) * 0.5\n    tensor.cpu().clamp(0,1)\n\n    if tensor.size(1) == 1:\n        tensor = tensor.repeat(1,3,1,1)\n\n    return tensor\n\ndef tensor_list_for_board(img_tensors_list):\n    grid_h = len(img_tensors_list)\n    grid_w = max(len(img_tensors)  for img_tensors in img_tensors_list)\n    \n    batch_size, channel, height, width = tensor_for_board(img_tensors_list[0][0]).size()\n    canvas_h = grid_h * height\n    canvas_w = grid_w * width\n    canvas = torch.FloatTensor(batch_size, channel, canvas_h, canvas_w).fill_(0.5)\n    for i, img_tensors in enumerate(img_tensors_list):\n        for j, img_tensor in enumerate(img_tensors):\n            offset_h = i * height\n            offset_w = j * width\n            tensor = tensor_for_board(img_tensor)\n            canvas[:, :, offset_h : offset_h + height, offset_w : offset_w + width].copy_(tensor)\n\n    return canvas\n\ndef board_add_images(img_tensors_list, epoch, iter, save_dir):\n    tensor = tensor_list_for_board(img_tensors_list)\n    array = tensor_for_image(tensor[0]) # Save first image\n    Image.fromarray(array).save(os.path.join(save_dir, 'ep{:02}_iter{:03}.jpg'.format(epoch,iter)))\n\n\n\n\ndef board_add_image(board, tag_name, img_tensor, step_count):\n    tensor = tensor_for_board(img_tensor)\n\n    for i, img in enumerate(tensor):\n        board.add_image('%s/%03d' % (tag_name, i), img, step_count)\n\n\n\n\ndef tensor_for_image(img_tensor):\n    tensor = img_tensor.clone() * 255\n    tensor = tensor.cpu().clamp(0,255)\n    array = tensor.detach().numpy().astype('uint8')\n    if array.shape[0] == 1:\n        array = array.squeeze(0)\n    elif array.shape[0] == 3:\n        array = array.swapaxes(0, 1).swapaxes(1, 2)\n    return array\n\n\ndef save_images(img_tensors, img_names, save_dir):\n    for img_tensor, img_name in zip(img_tensors, img_names):\n        array = tensor_for_image(img_tensor)        \n        Image.fromarray(array).save(os.path.join(save_dir, img_name))\n\ndef save_visual(img_tensors_list, img_names, save_dir):\n    img_tensors = tensor_list_for_board(img_tensors_list)\n\n    for img_tensor, img_name in zip(img_tensors, img_names):\n        array = tensor_for_image(img_tensor)\n        Image.fromarray(array).save(os.path.join(save_dir, img_name))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef create_directory(path: str):\n    \"\"\"\n    Creates a directory and handles potential errors gracefully.\n    \"\"\"\n    try:\n        # This will create the directory and any parent directories needed.\n        # `exist_ok=True` prevents an error if the directory already exists.\n        os.makedirs(path, exist_ok=True)\n        print(f\"Directory ensured at: {path}\")\n    except PermissionError:\n        print(f\" Error: Permission denied to create directory at: {path}\")\n    except Exception as e:\n        print(f\" An unexpected error occurred: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:45:00.320680Z","iopub.execute_input":"2025-07-17T14:45:00.320965Z","iopub.status.idle":"2025-07-17T14:45:00.325931Z","shell.execute_reply.started":"2025-07-17T14:45:00.320941Z","shell.execute_reply":"2025-07-17T14:45:00.325338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom dataclasses import dataclass, asdict\n\n# --- 1. Directory Creation Utility ---\ndef create_directory(path: str):\n    \"\"\"\n    Creates a directory and any necessary parent directories without raising an error if it already exists.\n    \"\"\"\n    try:\n        os.makedirs(path, exist_ok=True)\n        print(f\" Directory ensured at: {path}\")\n    except PermissionError:\n        print(f\" Error: Permission denied to create directory at: {path}\")\n    except Exception as e:\n        print(f\" An unexpected error occurred: {e}\")\n\n\n# --- 2. Complete Configuration Class ---\n@dataclass\nclass Config:\n    \"\"\"Configuration class for the entire training pipeline.\"\"\"\n    # --- Paths ---\n    data_root: str = '/kaggle/input/clothes_tryon_dataset/'\n    out_dir: str = '/kaggle/working/result/'\n    checkpoint_gmm: str = '/kaggle/working/result/GMM/epoch_00.pth'\n    checkpoint_tom: str = '/kaggle/working/result/TOM/gen_epoch_00.pth'\n    \n    # CORRECTED PATH: Changed 'grid-img' to your new folder 'gridpng'\n    grid_path: str = '/kaggle/input/gridpng/grid.png' \n\n    # --- Model & Directory Names ---\n    name_gmm: str = 'GMM'\n    name_tom: str = 'TOM'\n    \n    # --- Training Settings ---\n    n_epoch: int = 10 \n    batch_size: int = 4 \n    n_worker: int = 2 \n    gpu_id: str = '0'\n    log_freq: int = 100\n    \n    # --- Image/Model Parameters ---\n    fine_width: int = 192\n    fine_height: int = 256\n    radius: int = 5\n    grid_size: int = 5\n    \n    def to_dict(self):\n        \"\"\"Converts the dataclass to a dictionary.\"\"\"\n        return asdict(self)\n\n# --- 3. Execution ---\nconfig = Config()\n\nprint(\"--- Creating Project Directories ---\")\ncreate_directory(config.out_dir)\ncreate_directory(os.path.join(config.out_dir, config.name_gmm))\ncreate_directory(os.path.join(config.out_dir, config.name_tom))\nprint(\"------------------------------------\")\n\nprint(\"\\n--- Project Configuration ---\")\nprint(config)\nprint(\"-----------------------------\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:45:03.360024Z","iopub.execute_input":"2025-07-17T14:45:03.360559Z","iopub.status.idle":"2025-07-17T14:45:03.368882Z","shell.execute_reply.started":"2025-07-17T14:45:03.360535Z","shell.execute_reply":"2025-07-17T14:45:03.368348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport random\nfrom PIL import Image, ImageDraw\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n\ndef binarized_tensor(numpy_array):\n    \"\"\"Converts a numpy array to a binarized tensor.\"\"\"\n    mask = (numpy_array >= 128).astype(np.float32)\n    return torch.from_numpy(mask).unsqueeze(0)\n\ndef random_horizontal_flip(data_dict):\n    \n    if random.random() < 0.5:\n        return data_dict\n        \n    for key, value in data_dict.items():\n        if 'name' not in key and isinstance(value, torch.Tensor):\n            data_dict[key] = torch.flip(value, [2]) # Flip along the width dimension\n    return data_dict\n\n\nclass DatasetBase(Dataset):\n    \n    def __init__(self, opt: Config, mode: str, data_list_fn: str, is_train: bool = True):\n        super(DatasetBase, self).__init__()\n        self.opt = opt\n        self.data_path = os.path.join(opt.data_root, mode)\n        self.is_train = is_train\n        self.fine_height = opt.fine_height\n        self.fine_width = opt.fine_width\n        self.radius = opt.radius\n        \n        # Standard image transformation pipeline\n        self.transform = transforms.Compose([\n            transforms.Resize((self.fine_height, self.fine_width)),\n            transforms.Lambda(lambda img: img.convert(\"RGB\")),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n\n        # Load the list of training/testing pairs\n        person_names = []\n        cloth_names = []\n        with open(os.path.join(opt.data_root, data_list_fn), 'r') as f:\n            for line in f.readlines():\n                p_name, c_name = line.strip().split()\n                person_names.append(p_name)\n                cloth_names.append(c_name)\n\n        self.person_names = person_names\n        self.cloth_names = cloth_names\n\n    def __len__(self):\n        return len(self.person_names)\n\n    def _get_mask_arrays(self, parse_array):\n        \"\"\"Splits a parse map into individual binary masks for different body parts.\"\"\"\n        # Using bitwise OR for clarity and efficiency\n        shape = (parse_array > 0).astype(np.float32)\n        head = ((parse_array == 1) | (parse_array == 2) | (parse_array == 4) | (parse_array == 13)).astype(np.float32)\n        cloth = ((parse_array == 5) | (parse_array == 6) | (parse_array == 7)).astype(np.float32)\n        # Anything that is not the background or the target clothing\n        body = ((parse_array > 0) & (cloth == 0)).astype(np.float32)\n        return shape, head, cloth, body\n\n    def _load_pose_heatmap(self, pose_fn):\n        \"\"\"Loads pose keypoints from a JSON file and converts them into a multi-channel heatmap.\"\"\"\n        with open(os.path.join(self.data_path, 'openpose_json', pose_fn), 'r') as f:\n            pose_label = json.load(f)\n            pose_data = np.array(pose_label['people'][0]['pose_keypoints_2d']).reshape((-1, 3))\n\n        point_num = pose_data.shape[0] # Should be 18 for COCO\n        feature_pose_tensor = torch.zeros(point_num, self.fine_height, self.fine_width)\n        pose_vis_img = Image.new('L', (self.fine_width, self.fine_height))\n        pose_draw = ImageDraw.Draw(pose_vis_img)\n\n        for i in range(point_num):\n            px, py = pose_data[i, 0], pose_data[i, 1]\n            if px > 1 and py > 1:\n                # Draw a rectangle on a single-channel image to create a heatmap for one keypoint\n                one_map = Image.new('L', (self.fine_width, self.fine_height))\n                draw = ImageDraw.Draw(one_map)\n                draw.rectangle((px - self.radius, py - self.radius, px + self.radius, py + self.radius), 'white', 'white')\n                # Also draw on the visualization image\n                pose_draw.rectangle((px - self.radius, py - self.radius, px + self.radius, py + self.radius), 'white', 'white')\n                # Transform to tensor and add to the stack\n                feature_pose_tensor[i] = transforms.ToTensor()(one_map)[0]\n        \n        # Transform the full pose visualization image\n        pose_vis_tensor = self.transform(pose_vis_img)\n        return feature_pose_tensor, pose_vis_tensor\n\n    def _get_base_item(self, index):\n        \"\"\"Loads and processes all data related to the person.\"\"\"\n        person_fn = self.person_names[index]\n        \n        # Person Image\n        person_img = Image.open(os.path.join(self.data_path, 'image', person_fn))\n        person_tensor = self.transform(person_img)\n\n        # Person Parse Map\n        parse_fn = person_fn.replace('.jpg', '.png')\n        parse_img = Image.open(os.path.join(self.data_path, 'image-parse-v3', parse_fn))\n        parse_img = parse_img.resize((self.fine_width, self.fine_height), Image.NEAREST)\n        parse_array = np.array(parse_img)\n        \n        shape_mask, head_mask, cloth_mask, body_mask = self._get_mask_arrays(parse_array)\n        \n        # Shape image (person silhouette)\n        shape_img = Image.fromarray((shape_mask * 255).astype(np.uint8))\n        feature_shape_tensor = self.transform(shape_img)\n        \n        # Head feature\n        head_mask_tensor = torch.from_numpy(head_mask)\n        feature_head_tensor = person_tensor * head_mask_tensor - (1 - head_mask_tensor) # Keep head, mask rest\n        \n        # Ground truth cloth for loss calculation\n        cloth_mask_tensor = torch.from_numpy(cloth_mask)\n        cloth_parse_tensor = person_tensor * cloth_mask_tensor + (1 - cloth_mask_tensor) # Keep cloth, mask rest\n        \n        # Body mask (for final composition)\n        body_mask_tensor = torch.from_numpy(body_mask).unsqueeze(0)\n\n        # Pose Keypoints\n        pose_fn = person_fn.replace('.jpg', '_keypoints.json')\n        feature_pose_tensor, pose_tensor = self._load_pose_heatmap(pose_fn)\n        \n        # --- Final Feature Tensor for GMM ---\n        # This is the crucial part that defines the input channels for the network\n        # 3 (shape) + 3 (head) + 18 (pose) = 24 channels\n        feature_tensor = torch.cat([feature_shape_tensor, feature_head_tensor, feature_pose_tensor], 0)\n\n        return {\n            'person_name': person_fn,\n            'person': person_tensor,\n            'feature': feature_tensor,\n            'pose': pose_tensor,\n            'head': feature_head_tensor,\n            'shape': feature_shape_tensor,\n            'cloth_parse': cloth_parse_tensor,\n            'body_mask': body_mask_tensor,\n        }\n\n# --- 3. Specific Dataset Classes ---\n\nclass GMMDataset(DatasetBase):\n    \"\"\"Dataset for the Geometric Matching Module (GMM).\"\"\"\n    def __init__(self, opt: Config, mode: str, data_list_fn: str, is_train: bool = True):\n        super().__init__(opt, mode, data_list_fn, is_train)\n\n    def __getitem__(self, index):\n        # Get all person-related data from the base class\n        data = self._get_base_item(index)\n        \n        # Load cloth data\n        cloth_fn = self.cloth_names[index]\n        cloth_img = Image.open(os.path.join(self.data_path, 'cloth', cloth_fn))\n        cloth_tensor = self.transform(cloth_img)\n        \n        # Load cloth mask\n        # Assuming cloth-mask is grayscale, if not, add .convert('L')\n        cloth_mask_img = Image.open(os.path.join(self.data_path, 'cloth-mask', cloth_fn)).convert('L')\n        cloth_mask_tensor = binarized_tensor(np.array(cloth_mask_img))\n        \n        # Load grid image for visualization\n        # Make sure config.grid_path is set correctly in your config cell\n        grid_img = Image.open(self.opt.grid_path)\n        grid_tensor = self.transform(grid_img)\n        \n        # --- THE FIX IS HERE ---\n        # The trainer expects the target image under the key 'image'.\n        # The base class provides it under the key 'person'.\n        # We simply add the 'image' key here to bridge the gap.\n        data['image'] = data['person']\n        \n        # Add cloth-specific data to the dictionary\n        data.update({\n            'cloth_name': cloth_fn,\n            'cloth': cloth_tensor,\n            'cloth_mask': cloth_mask_tensor,\n            'grid': grid_tensor,\n        })\n        \n        # Apply data augmentation if in training mode\n        if self.is_train:\n            data = random_horizontal_flip(data)\n            \n        return data\nclass TOMDataset(DatasetBase):\n    \"\"\"Dataset for the Try-On Module (TOM).\"\"\"\n    def __init__(self, opt: Config, mode: str, data_list_fn: str, is_train: bool = True):\n        super().__init__(opt, mode, data_list_fn, is_train)\n\n    def __getitem__(self, index):\n        # Get all person-related data from the base class\n        data = self._get_base_item(index)\n        \n        # Load the *warped* cloth data (output from GMM)\n        cloth_fn = self.cloth_names[index]\n        warped_cloth_img = Image.open(os.path.join(self.opt.out_dir, self.opt.name_gmm, 'warp-cloth', cloth_fn))\n        warped_cloth_tensor = self.transform(warped_cloth_img)\n        \n        # Load the *warped* cloth mask\n        warped_mask_img = Image.open(os.path.join(self.opt.out_dir, self.opt.name_gmm, 'warp-mask', cloth_fn))\n        warped_mask_tensor = binarized_tensor(np.array(warped_mask_img))\n        \n        # Add warped cloth data to the dictionary\n        data.update({\n            'cloth_name': cloth_fn,\n            'cloth': warped_cloth_tensor, # Use warped cloth as input for TOM\n            'cloth_mask': warped_mask_tensor,\n        })\n        \n        # Apply data augmentation if in training mode\n        if self.is_train:\n            data = random_horizontal_flip(data)\n            \n        return data\n\nprint(\"--- Verifying GMMDataset ---\")\ntry:\n    # Instantiate the dataset\n    gmm_dataset_train = GMMDataset(config, mode='train', data_list_fn='train_pairs.txt')\n    \n    # Create a DataLoader\n    gmm_dataloader_train = DataLoader(gmm_dataset_train, batch_size=config.batch_size, num_workers=0, shuffle=True)\n    \n    # Fetch one batch of data\n    one_batch = next(iter(gmm_dataloader_train))\n    \n    print(\"✅ Successfully loaded one batch of data.\")\n    print(\"\\nData keys:\", list(one_batch.keys()))\n    \n    # Check the shape of the crucial 'feature' tensor\n    feature_tensor = one_batch['feature']\n    print(f\"\\nShape of the 'feature' tensor: {feature_tensor.shape}\")\n    print(f\"  - Batch Size: {feature_tensor.shape[0]}\")\n    print(f\"  - Channels:   {feature_tensor.shape[1]} (This must match the network's input)\")\n    print(f\"  - Height:     {feature_tensor.shape[2]}\")\n    print(f\"  - Width:      {feature_tensor.shape[3]}\")\n\nexcept FileNotFoundError as e:\n    print(f\"\\n❌ ERROR: Could not find a required file. Please check paths in your Config.\")\n    print(f\"  - Details: {e}\")\nexcept Exception as e:\n    print(f\"\\n❌ An unexpected error occurred during dataset verification: {e}\")\n\nprint(\"----------------------------\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:45:08.028797Z","iopub.execute_input":"2025-07-17T14:45:08.029059Z","iopub.status.idle":"2025-07-17T14:45:18.846332Z","shell.execute_reply.started":"2025-07-17T14:45:08.029039Z","shell.execute_reply":"2025-07-17T14:45:18.845636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef mkdir(path: str):\n    \"\"\"\n    Creates a directory and any necessary parent directories \n    without raising an error if it already exists.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:45:27.012146Z","iopub.execute_input":"2025-07-17T14:45:27.012755Z","iopub.status.idle":"2025-07-17T14:45:27.016155Z","shell.execute_reply.started":"2025-07-17T14:45:27.012732Z","shell.execute_reply":"2025-07-17T14:45:27.015515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os\nimport functools\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom torchvision import models\n\ndef weights_init_normal(m):\n    \"\"\"Initializes weights with a normal distribution for Conv and Linear layers.\"\"\"\n    classname = m.__class__.__name__\n    if 'Conv' in classname:\n        init.normal_(m.weight.data, 0.0, 0.02)\n    elif 'Linear' in classname:\n        init.normal_(m.weight.data, 0.0, 0.02)\n    elif 'BatchNorm2d' in classname:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n# --- 2. Core GMM Components (Corrected) ---\n\nclass FeatureExtraction(nn.Module):\n    \"\"\"Downsampling CNN for feature extraction.\"\"\"\n    def __init__(self, input_nc, ngf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(FeatureExtraction, self).__init__()\n        model = [nn.Conv2d(input_nc, ngf, kernel_size=4, stride=2, padding=1),\n                 nn.ReLU(True), norm_layer(ngf)]\n        for i in range(n_layers):\n            in_ngf = 2**i * ngf if 2**i * ngf < 512 else 512\n            out_ngf = 2**(i+1) * ngf if 2**i * ngf < 512 else 512\n            model += [nn.Conv2d(in_ngf, out_ngf, kernel_size=4, stride=2, padding=1),\n                      nn.ReLU(True), norm_layer(out_ngf)]\n        model += [nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.ReLU(True), norm_layer(512)]\n        model += [nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1), nn.ReLU(True)]\n        self.model = nn.Sequential(*model)\n        self.model.apply(weights_init_normal)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass FeatureL2Norm(nn.Module):\n    \"\"\"L2 normalizes features across the channel dimension.\"\"\"\n    def __init__(self):\n        super(FeatureL2Norm, self).__init__()\n\n    def forward(self, feature):\n        epsilon = 1e-6\n        norm = torch.pow(torch.sum(torch.pow(feature, 2), 1, keepdim=True) + epsilon, 0.5)\n        return torch.div(feature, norm)\n\nclass FeatureCorrelation(nn.Module):\n    \"\"\"Calculates a correlation volume between two feature maps.\"\"\"\n    def __init__(self):\n        super(FeatureCorrelation, self).__init__()\n    \n    def forward(self, feature_A, feature_B):\n        b, c, h, w = feature_A.size()\n        feature_A = feature_A.view(b, c, h * w)\n        feature_B = feature_B.view(b, c, h * w).transpose(1, 2)\n        correlation = torch.bmm(feature_B, feature_A)\n        correlation_tensor = correlation.view(b, h, w, h * w).permute(0, 3, 1, 2)\n        return correlation_tensor\n\nclass FeatureRegression(nn.Module):\n    \"\"\"\n    Regresses a correlation volume to transformation parameters.\n    --- THIS IS THE ROBUST, CORRECTED IMPLEMENTATION ---\n    \"\"\"\n    def __init__(self, input_nc=192, output_dim=6, use_cuda=True):\n        super(FeatureRegression, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(input_nc, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            # --- THE FIX: Adaptive Pooling makes the network robust to input size variations ---\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n        # The linear layer now takes a fixed-size input of 64 channels.\n        self.linear = nn.Linear(64, output_dim)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.view(x.size(0), -1) # This is now safe as the input is guaranteed to be [batch, 64, 1, 1]\n        x = self.linear(x)\n        return self.tanh(x)\n\nclass TpsGridGen(nn.Module):\n    \"\"\"Generates a Thin Plate Spline (TPS) sampling grid. This is a robust, corrected implementation.\"\"\"\n    def __init__(self, out_h=256, out_w=192, use_regular_grid=True, grid_size=5, reg_factor=0, use_cuda=True):\n        super(TpsGridGen, self).__init__()\n        self.out_h, self.out_w, self.N = out_h, out_w, grid_size * grid_size\n        axis_coords = np.linspace(-1, 1, grid_size)\n        P_Y, P_X = np.meshgrid(axis_coords, axis_coords)\n        P_X, P_Y = torch.FloatTensor(P_X.flatten()), torch.FloatTensor(P_Y.flatten())\n        self.register_buffer('P_X_base', P_X)\n        self.register_buffer('P_Y_base', P_Y)\n        self.register_buffer('L_inv', self.compute_L_inverse(P_X, P_Y).unsqueeze(0))\n        grid_X, grid_Y = np.meshgrid(np.linspace(-1, 1, out_w), np.linspace(-1, 1, out_h))\n        self.register_buffer('grid_X', torch.FloatTensor(grid_X).unsqueeze(0))\n        self.register_buffer('grid_Y', torch.FloatTensor(grid_Y).unsqueeze(0))\n\n    def compute_L_inverse(self, X, Y):\n        N = X.size(0)\n        X_mat = X.expand(N, N)\n        Y_mat = Y.expand(N, N)\n        dist_sq = torch.pow(X_mat - X_mat.t(), 2) + torch.pow(Y_mat - Y_mat.t(), 2)\n        dist_sq.masked_fill_(dist_sq == 0, 1)\n        K = torch.mul(dist_sq, torch.log(dist_sq))\n        O = torch.ones(N, 1)\n        Z = torch.zeros(3, 3)\n        P = torch.cat((O, X.unsqueeze(1), Y.unsqueeze(1)), 1)\n        L = torch.cat((torch.cat((K, P), 1), torch.cat((P.t(), Z), 1)), 0)\n        return torch.inverse(L)\n\n    def forward(self, theta):\n        batch_size = theta.size(0)\n        theta = theta.contiguous().view(batch_size, self.N, 2)\n        Q_X, Q_Y = theta[:, :, 0] + self.P_X_base, theta[:, :, 1] + self.P_Y_base\n        zeros = torch.zeros(batch_size, 3, 2, dtype=theta.dtype, device=theta.device)\n        v = torch.cat([torch.stack([Q_X, Q_Y], dim=2), zeros], dim=1)\n        w = torch.bmm(self.L_inv.expand(batch_size, -1, -1), v)\n        W, A = w[:, :self.N, :], w[:, self.N:, :]\n        grid_X, grid_Y = self.grid_X.expand(batch_size, -1, -1), self.grid_Y.expand(batch_size, -1, -1)\n        P_X_r, P_Y_r = self.P_X_base.view(1, self.N, 1, 1), self.P_Y_base.view(1, self.N, 1, 1)\n        dist_sq = torch.pow(grid_X.unsqueeze(1) - P_X_r, 2) + torch.pow(grid_Y.unsqueeze(1) - P_Y_r, 2)\n        dist_sq.masked_fill_(dist_sq == 0, 1)\n        U = torch.mul(dist_sq, torch.log(dist_sq))\n        W_r, U_r = W.view(batch_size, self.N, 1, 1, 2), U.unsqueeze(4)\n        non_linear_term = torch.sum(W_r * U_r, dim=1)\n        affine_basis = torch.stack([torch.ones_like(grid_X), grid_X, grid_Y], dim=3)\n        affine_term = torch.bmm(affine_basis.view(batch_size, -1, 3), A).view(batch_size, self.out_h, self.out_w, 2)\n        return affine_term + non_linear_term\n\n# --- 3. Assembled GMM Network (Corrected) ---\n\nclass GMM(nn.Module):\n    \"\"\" Geometric Matching Module \"\"\"\n    def __init__(self, opt):\n        super(GMM, self).__init__()\n        self.extractionA = FeatureExtraction(input_nc=31)\n        self.extractionB = FeatureExtraction(input_nc=3)\n        self.l2norm = FeatureL2Norm()\n        self.correlation = FeatureCorrelation()\n        self.regression = FeatureRegression(input_nc=192, output_dim=2 * opt.grid_size**2)\n        self.gridGen = TpsGridGen(opt.fine_height, opt.fine_width, grid_size=opt.grid_size)\n\n    def forward(self, inputA, inputB):\n        featureA = self.extractionA(inputA)\n        featureB = self.extractionB(inputB)\n        featureA = self.l2norm(featureA)\n        featureB = self.l2norm(featureB)\n        correlation = self.correlation(featureA, featureB)\n        theta = self.regression(correlation)\n        grid = self.gridGen(theta)\n        return grid, theta\n\n# --- 4. Checkpoint Utilities ---\n\ndef save_checkpoint(model, save_path):\n    if not os.path.exists(os.path.dirname(save_path)):\n        os.makedirs(os.path.dirname(save_path))\n    torch.save(model.cpu().state_dict(), save_path)\n\ndef load_checkpoint(model, checkpoint_path):\n    if not os.path.exists(checkpoint_path):\n        return\n    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n    print(f\"✅ Checkpoint loaded from '{checkpoint_path}'\")\n\n# --- 5. Verification Step ---\nprint(\"--- Verifying Network Definitions ---\")\ntry:\n    # Use the 'config' object defined in a previous cell\n    gmm_model = GMM(config)\n    \n    # Create dummy tensors with the correct shapes to simulate a batch\n    batch_size = 2\n    dummy_inputA = torch.randn(batch_size, 31, config.fine_height, config.fine_width)\n    dummy_inputB = torch.randn(batch_size, 3, config.fine_height, config.fine_width)\n    \n    # Perform a forward pass to check for runtime errors\n    print(\"Performing a test forward pass...\")\n    grid, theta = gmm_model(dummy_inputA, dummy_inputB)\n    \n    print(\"\\n✅ Verification successful! No runtime errors found.\")\n    print(f\"  - Input A shape: {dummy_inputA.shape}\")\n    print(f\"  - Input B shape: {dummy_inputB.shape}\")\n    print(f\"  - Output grid shape: {grid.shape}\")\n    print(f\"  - Output theta shape: {theta.shape}\")\n\nexcept Exception as e:\n    print(f\"\\n❌ An unexpected error occurred during network verification: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"---------------------------------\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:45:40.836628Z","iopub.execute_input":"2025-07-17T14:45:40.837155Z","iopub.status.idle":"2025-07-17T14:45:41.826780Z","shell.execute_reply.started":"2025-07-17T14:45:40.837127Z","shell.execute_reply":"2025-07-17T14:45:41.825964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dataclasses import dataclass, field\n\n@dataclass\nclass Config:\n    \"\"\"\n    The final, complete configuration for the GMM training pipeline.\n    This version includes all necessary parameters for the dataset, network, and trainer.\n    \"\"\"\n    # --- Path and Naming ---\n    data_root: str = '/kaggle/input/clothes_tryon_dataset/'\n    out_dir: str = '/kaggle/working/result/'\n    name: str = 'GMM_final_run'                 \n    checkpoint: str = ''                        \n    grid_path: str = '/kaggle/input/gridpng/grid.png'              \n    # --- Data Loading ---\n    batch_size: int = 4\n    n_worker: int = 2                          \n    # --- Model Architecture ---\n    fine_height: int = 256\n    fine_width: int = 192\n    radius: int = 5                             \n    grid_size: int = 5                          \n\n    # --- Training Parameters ---\n    n_epoch: int = 50                           \n    lr: float = 0.0002                        \n    gpu_id: int = 0                            \n    display_freq: int = 100                    \nconfig = Config()\n\n# --- Verification ---\nprint(\"✅ Final configuration loaded successfully.\")\nprint(f\"Run Name: {config.name}\")\nprint(f\"Data Root: {config.data_root}\")\nprint(f\"Learning Rate: {config.lr}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:53:38.312406Z","iopub.execute_input":"2025-07-17T14:53:38.312949Z","iopub.status.idle":"2025-07-17T14:53:38.319932Z","shell.execute_reply.started":"2025-07-17T14:53:38.312924Z","shell.execute_reply":"2025-07-17T14:53:38.319216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom typing import List, Dict\n\n\nclass GMMTrainer:\n    \"\"\"A class to handle the training and validation of the GMM model.\"\"\"\n    def __init__(self, model: nn.Module, dataloader_train: DataLoader, dataloader_val: DataLoader, opt):\n        self.opt = opt\n        # The model is now passed in on the correct device\n        self.model = model \n        # We can infer the device from the model's parameters\n        self.device = next(model.parameters()).device\n        \n        self.dataloader_train = dataloader_train\n        self.dataloader_val = dataloader_val\n        # The optimizer is created with parameters that are already on the correct device\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n        self.criterionL1 = nn.L1Loss()\n        self.save_dir = os.path.join(opt.out_dir, opt.name)\n        \n        print(f\"Trainer initialized on device: {self.device}\")\n        print(f\"Total Parameters: {sum(p.nelement() for p in self.model.parameters()):,}\")\n\n    def _run_iteration(self, epoch: int, data_loader: DataLoader, is_train: bool) -> float:\n        \"\"\"Core logic for one epoch of training or validation.\"\"\"\n        loop_type = \"Train\" if is_train else \"Val\"\n        data_iter = tqdm(data_loader, desc=f'Epoch {epoch:02d} | {loop_type}', total=len(data_loader))\n        total_loss = 0.0\n\n        for i, data_batch in enumerate(data_iter):\n            # Move data to the same device as the model\n            data = {k: v.to(self.device, non_blocking=True) for k, v in data_batch.items() if isinstance(v, torch.Tensor)}\n            \n            grid, _ = self.model(data['feature'], data['cloth'])\n            warped_cloth = F.grid_sample(data['cloth'], grid, padding_mode='border', align_corners=False)\n            \n            loss = self.criterionL1(warped_cloth, data['image'])\n\n            if is_train:\n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n\n            total_loss += loss.item()\n            data_iter.set_postfix(avg_loss=f'{total_loss / (i + 1):.4f}')\n\n            # --- Visualization logic has been completely removed from the training loop ---\n        \n        return total_loss / len(data_loader)\n\n    def train_one_epoch(self, epoch: int) -> float:\n        self.model.train()\n        return self._run_iteration(epoch, self.dataloader_train, is_train=True)\n\n    def validate_one_epoch(self, epoch: int) -> float:\n        self.model.eval()\n        with torch.no_grad():\n            return self._run_iteration(epoch, self.dataloader_val, is_train=False)\n\n# --- 3. Main Training Function (Finalized) ---\ndef train_gmm():\n    \"\"\"Main function to set up and run the GMM training.\"\"\"\n    opt = config \n    print(\"--- Starting GMM Training ---\")\n\n    # Define device and move model to device BEFORE creating trainer\n    device = torch.device(f'cuda:{opt.gpu_id}' if torch.cuda.is_available() else 'cpu')\n\n    # Setup Dataloaders\n    print('Loading datasets...')\n    dataset_train = GMMDataset(opt, mode='train', data_list_fn='train_pairs.txt')\n    dataloader_train = DataLoader(dataset_train, batch_size=opt.batch_size, num_workers=opt.n_worker, shuffle=True, pin_memory=True)\n    \n    dataset_val = GMMDataset(opt, mode='test', data_list_fn='test_pairs.txt', is_train=False)\n    dataloader_val = DataLoader(dataset_val, batch_size=opt.batch_size, num_workers=opt.n_worker, shuffle=False, pin_memory=True)\n\n    # Setup Logging\n    log_dir = os.path.join(opt.out_dir, 'logs')\n    mkdir(log_dir)\n    log_name = os.path.join(log_dir, opt.name + '_log.csv')\n    with open(log_name, 'w') as f:\n        f.write('epoch,train_loss,val_loss\\n')\n\n    # Build Model and move to device\n    print('Building GMM model...')\n    model = GMM(opt).to(device)\n    \n    # Load checkpoint (state_dict is loaded into the model which is already on the correct device)\n    if opt.checkpoint and os.path.exists(opt.checkpoint):\n        load_checkpoint(model, opt.checkpoint)\n\n    # Instantiate Trainer (the model is already on the target device)\n    trainer = GMMTrainer(model, dataloader_train, dataloader_val, opt)\n\n    # --- Training Loop ---\n    print('Starting training loop...')\n    best_val_loss = float('inf')\n\n    for epoch in range(opt.n_epoch):\n        train_loss = trainer.train_one_epoch(epoch)\n        val_loss = trainer.validate_one_epoch(epoch)\n        \n        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n\n        # Log losses to CSV\n        with open(log_name, 'a') as f:\n            f.write(f'{epoch},{train_loss:.4f},{val_loss:.4f}\\n')\n            \n        # Efficient Checkpointing\n        latest_save_path = os.path.join(trainer.save_dir, 'latest.pth')\n        save_checkpoint(model, latest_save_path)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_save_path = os.path.join(trainer.save_dir, 'best.pth')\n            save_checkpoint(model, best_save_path)\n            print(f\"Validation loss improved to {val_loss:.4f}. Best model saved to {best_save_path}\")\n        \n\n        model.to(device)\n\n    print('--- Finished GMM Training ---')\n\n\n# --- 4. Start the Training Process ---\nif __name__ == \"__main__\":\n    train_gmm()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:47:21.449369Z","iopub.execute_input":"2025-07-17T14:47:21.450030Z","iopub.status.idle":"2025-07-17T14:47:41.221034Z","shell.execute_reply.started":"2025-07-17T14:47:21.450007Z","shell.execute_reply":"2025-07-17T14:47:41.219595Z"}},"outputs":[],"execution_count":null}]}