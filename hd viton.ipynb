{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12302277,"sourceType":"datasetVersion","datasetId":7754175},{"sourceId":478881,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":384719,"modelId":404033}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 1.1: Clone a reliable HD-VITON repository ---\nprint(\"Cloning the HD-VITON repository...\")\n!git clone https://github.com/shadow2496/VITON-HD\nprint(\"✅ Repository cloned.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:46:21.576932Z","iopub.execute_input":"2025-07-19T21:46:21.577518Z","iopub.status.idle":"2025-07-19T21:46:22.471526Z","shell.execute_reply.started":"2025-07-19T21:46:21.577494Z","shell.execute_reply":"2025-07-19T21:46:22.470602Z"}},"outputs":[{"name":"stdout","text":"Cloning the HD-VITON repository...\nCloning into 'VITON-HD'...\nremote: Enumerating objects: 52, done.\u001b[K\nremote: Counting objects: 100% (19/19), done.\u001b[K\nremote: Compressing objects: 100% (13/13), done.\u001b[K\nremote: Total 52 (delta 12), reused 6 (delta 6), pack-reused 33 (from 3)\u001b[K\nReceiving objects: 100% (52/52), 5.03 MiB | 24.89 MiB/s, done.\nResolving deltas: 100% (19/19), done.\n✅ Repository cloned.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\n\n# --- Environment Setup ---\n\nprint(\"🚀 Setting up the environment...\")\n\n# 1.1: Navigate into the repository directory\nrepo_path = \"/kaggle/working/VITON-HD\"\n%cd {repo_path}\nprint(f\"Current directory: {os.getcwd()}\")\n\n# 1.2: Link your dataset folder\ntarget_data_dir = os.path.join(repo_path, \"data\")\nsource_data_dir = \"/kaggle/input/clothe/clothes_tryon_dataset\"\nif not os.path.exists(target_data_dir):\n    print(\"\\nLinking dataset...\")\n    os.symlink(source_data_dir, target_data_dir)\n    print(\"✅ Dataset linked successfully.\")\nelse:\n    print(\"\\nDataset link check: OK.\")\n\nprint(\"\\n---------------------------------\")\nprint(\"✅ Environment is ready.\")\nprint(\"---------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:51:39.153816Z","iopub.execute_input":"2025-07-19T20:51:39.154429Z","iopub.status.idle":"2025-07-19T20:51:39.227754Z","shell.execute_reply.started":"2025-07-19T20:51:39.154402Z","shell.execute_reply":"2025-07-19T20:51:39.226817Z"}},"outputs":[{"name":"stdout","text":"🚀 Setting up the environment...\n/kaggle/working/VITON-HD\nCurrent directory: /kaggle/working/VITON-HD\n\nDataset link check: OK.\n\n---------------------------------\n✅ Environment is ready.\n---------------------------------\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# --- Part 1: GMM Inference ---\n!pip install torchgeometry\nprint(\"\\n🚀 Starting Part 1: GMM Inference (Geometric Warping)...\")\n\n# Define the absolute path to YOUR uploaded GMM model.\n# The single quotes are important to handle the space and parentheses.\ngmm_checkpoint_path = \"'/kaggle/input/weights/pytorch/default/1/gmm_final (1).pth'\"\n# Define a name for this experiment.\ngmm_experiment_name = \"GMM_test_inference_run\"\n\nprint(f\"Loading GMM model from: {gmm_checkpoint_path}\")\n\n# Run the test script for the GMM stage\n!python test.py \\\n    --name {gmm_experiment_name} \\\n    --stage GMM \\\n    --dataroot ./data \\\n    --test_pairs ./data/test_pairs.txt \\\n    --workers 4 \\\n    --batch_size 4 \\\n    --checkpoint {gmm_checkpoint_path}\n\nprint(\"\\n---------------------------------\")\nprint(\"✅ GMM inference complete.\")\nprint(\"Warped clothes for the test set have been generated.\")\nprint(\"---------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:52:06.960829Z","iopub.execute_input":"2025-07-19T20:52:06.961134Z","iopub.status.idle":"2025-07-19T20:54:04.481226Z","shell.execute_reply.started":"2025-07-19T20:52:06.961108Z","shell.execute_reply":"2025-07-19T20:54:04.479428Z"}},"outputs":[{"name":"stdout","text":"Collecting torchgeometry\n  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torchgeometry) (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->torchgeometry) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->torchgeometry) (3.0.2)\nDownloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchgeometry\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchgeometry-0.1.2\n\n🚀 Starting Part 1: GMM Inference (Geometric Warping)...\nLoading GMM model from: '/kaggle/input/weights/pytorch/default/1/gmm_final (1).pth'\nusage: test.py [-h] --name NAME [-b BATCH_SIZE] [-j WORKERS]\n               [--load_height LOAD_HEIGHT] [--load_width LOAD_WIDTH]\n               [--shuffle] [--dataset_dir DATASET_DIR]\n               [--dataset_mode DATASET_MODE] [--dataset_list DATASET_LIST]\n               [--checkpoint_dir CHECKPOINT_DIR] [--save_dir SAVE_DIR]\n               [--display_freq DISPLAY_FREQ] [--seg_checkpoint SEG_CHECKPOINT]\n               [--gmm_checkpoint GMM_CHECKPOINT]\n               [--alias_checkpoint ALIAS_CHECKPOINT]\n               [--semantic_nc SEMANTIC_NC]\n               [--init_type {normal,xavier,xavier_uniform,kaiming,orthogonal,none}]\n               [--init_variance INIT_VARIANCE] [--grid_size GRID_SIZE]\n               [--norm_G NORM_G] [--ngf NGF]\n               [--num_upsampling_layers {normal,more,most}]\ntest.py: error: unrecognized arguments: --stage GMM --dataroot ./data --test_pairs ./data/test_pairs.txt\n\n---------------------------------\n✅ GMM inference complete.\nWarped clothes for the test set have been generated.\n---------------------------------\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\n\nprint(\"--- Data Integrity Debugger ---\")\nprint(\"This script will check the first 5 entries of your pairs file against your data.\")\n\n# Define the paths to the source data directories\nsource_test_dir = \"/kaggle/input/clothe/clothes_tryon_dataset/test\"\njson_dir = os.path.join(source_test_dir, \"openpose_json\")\nimg_dir = os.path.join(source_test_dir, \"openpose_img\")\npairs_file_path = \"/kaggle/input/clothe/clothes_tryon_dataset/test_pairs.txt\"\n\n# Let's check the first 5 entries from the pairs file\nnum_to_check = 5\n\nprint(f\"\\nSource JSON directory being checked: {json_dir}\")\nprint(f\"Source IMG directory being checked: {img_dir}\")\nprint(\"-\" * 50)\n\n# Verify the directories themselves exist\nif not os.path.exists(json_dir):\n    print(f\"FATAL ERROR: JSON directory not found at {json_dir}\")\nif not os.path.exists(img_dir):\n    print(f\"FATAL ERROR: IMG directory not found at {img_dir}\")\nif not os.path.exists(pairs_file_path):\n    print(f\"FATAL ERROR: Pairs file not found at {pairs_file_path}\")\n\nprint(\"\\n--- Checking first 5 pairs from test_pairs.txt ---\")\n\ntry:\n    with open(pairs_file_path, \"r\") as f:\n        for i, line in enumerate(f):\n            if i >= num_to_check:\n                break\n\n            print(f\"\\n--- Pair #{i+1} ---\")\n            print(f\"Original line from pairs file: '{line.strip()}'\")\n\n            person_fn, cloth_fn = line.strip().split()\n            base_name, _ = os.path.splitext(person_fn)\n            print(f\"Extracted base name for checks: '{base_name}'\")\n\n            # --- Check 1: The JSON file ---\n            # My script assumed the JSON file is named like '01234_00.json'. Let's verify.\n            expected_json_path = os.path.join(json_dir, f\"{base_name}.json\")\n            print(f\"Checking for JSON file at: {expected_json_path}\")\n            json_exists = os.path.exists(expected_json_path)\n            print(f\"Found? -> {json_exists}\")\n\n            # --- Check 2: The rendered PNG file ---\n            # My script assumed the IMG file is named like '01234_00_rendered.png'. Let's verify.\n            expected_img_path = os.path.join(img_dir, f\"{base_name}_rendered.png\")\n            print(f\"Checking for IMG file at: {expected_img_path}\")\n            img_exists = os.path.exists(expected_img_path)\n            print(f\"Found? -> {img_exists}\")\n\n            if not json_exists or not img_exists:\n                print(\">>> STATUS: This pair is INVALID and would be filtered out. <<<\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred while reading the pairs file: {e}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"--- For comparison, here are the ACTUAL filenames ---\")\nprint(\"\\nActual JSON filenames (first 5):\")\n!ls {json_dir} | head -n 5\n\nprint(\"\\nActual IMG filenames (first 5):\")\n!ls {img_dir} | head -n 5\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T22:12:12.453171Z","iopub.execute_input":"2025-07-19T22:12:12.453788Z","iopub.status.idle":"2025-07-19T22:12:12.707102Z","shell.execute_reply.started":"2025-07-19T22:12:12.453763Z","shell.execute_reply":"2025-07-19T22:12:12.706398Z"}},"outputs":[{"name":"stdout","text":"--- Data Integrity Debugger ---\nThis script will check the first 5 entries of your pairs file against your data.\n\nSource JSON directory being checked: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_json\nSource IMG directory being checked: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_img\n--------------------------------------------------\n\n--- Checking first 5 pairs from test_pairs.txt ---\n\n--- Pair #1 ---\nOriginal line from pairs file: '05006_00.jpg 11001_00.jpg'\nExtracted base name for checks: '05006_00'\nChecking for JSON file at: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_json/05006_00.json\nFound? -> False\nChecking for IMG file at: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_img/05006_00_rendered.png\nFound? -> True\n>>> STATUS: This pair is INVALID and would be filtered out. <<<\n\n--- Pair #2 ---\nOriginal line from pairs file: '02532_00.jpg 14096_00.jpg'\nExtracted base name for checks: '02532_00'\nChecking for JSON file at: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_json/02532_00.json\nFound? -> False\nChecking for IMG file at: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_img/02532_00_rendered.png\nFound? -> True\n>>> STATUS: This pair is INVALID and would be filtered out. <<<\n\n--- Pair #3 ---\nOriginal line from pairs file: '03921_00.jpg 08015_00.jpg'\nExtracted base name for checks: '03921_00'\nChecking for JSON file at: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_json/03921_00.json\nFound? -> False\nChecking for IMG file at: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_img/03921_00_rendered.png\nFound? -> True\n>>> STATUS: This pair is INVALID and would be filtered out. <<<\n\n--- Pair #4 ---\nOriginal line from pairs file: '12419_00.jpg 01944_00.jpg'\nExtracted base name for checks: '12419_00'\nChecking for JSON file at: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_json/12419_00.json\nFound? -> False\nChecking for IMG file at: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_img/12419_00_rendered.png\nFound? -> True\n>>> STATUS: This pair is INVALID and would be filtered out. <<<\n\n--- Pair #5 ---\nOriginal line from pairs file: '12562_00.jpg 14025_00.jpg'\nExtracted base name for checks: '12562_00'\nChecking for JSON file at: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_json/12562_00.json\nFound? -> False\nChecking for IMG file at: /kaggle/input/clothe/clothes_tryon_dataset/test/openpose_img/12562_00_rendered.png\nFound? -> True\n>>> STATUS: This pair is INVALID and would be filtered out. <<<\n\n==================================================\n--- For comparison, here are the ACTUAL filenames ---\n\nActual JSON filenames (first 5):\n00006_00_keypoints.json\n00008_00_keypoints.json\n00013_00_keypoints.json\n00017_00_keypoints.json\n00034_00_keypoints.json\nls: write error: Broken pipe\n\nActual IMG filenames (first 5):\n00006_00_rendered.png\n00008_00_rendered.png\n00013_00_rendered.png\n00017_00_rendered.png\n00034_00_rendered.png\nls: write error: Broken pipe\n==================================================\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nimport shutil\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# --- Step 1: Nuke, Pave, and Link ---\n# This setup part is now correct and stable.\n\nprint(\"🚀 Preparing complete environment...\")\n\nrepo_path = \"/kaggle/working/VITON-HD\"\n%cd {repo_path}\n\n# NUKE previous attempts\ndata_root_to_nuke = os.path.join(repo_path, \"data\")\nif os.path.lexists(data_root_to_nuke):\n    shutil.rmtree(data_root_to_nuke)\n\n# Safe Model Links\nsafe_link_dir = \"/kaggle/working/safe_model_links\"\nos.makedirs(safe_link_dir, exist_ok=True)\nsource_gmm_path = \"/kaggle/input/weights/pytorch/default/1/gmm_final (1).pth\"\nsource_alias_path = \"/kaggle/input/weights/pytorch/default/1/alias_final.pth\"\nsource_seg_path = \"/kaggle/input/weights/pytorch/default/1/seg_final.pth\"\nsafe_gmm_path = os.path.join(safe_link_dir, \"gmm_final.pth\")\nsafe_alias_path = os.path.join(safe_link_dir, \"alias_final.pth\")\nsafe_seg_path = os.path.join(safe_link_dir, \"seg_final.pth\")\nif not os.path.exists(safe_gmm_path): os.symlink(source_gmm_path, safe_gmm_path)\nif not os.path.exists(safe_alias_path): os.symlink(source_alias_path, safe_alias_path)\nif not os.path.exists(safe_seg_path): os.symlink(source_seg_path, safe_seg_path)\nprint(\"✅ Safe model links are ready.\")\n\n# PAVE & LINK with CORRECT NAMING\ndata_root = os.path.join(repo_path, \"data\")\ntest_dir = os.path.join(data_root, \"test\")\nos.makedirs(test_dir, exist_ok=True)\nsource_test_dir = \"/kaggle/input/clothe/clothes_tryon_dataset/test\"\nfor folder_name in os.listdir(source_test_dir):\n    source_path = os.path.join(source_test_dir, folder_name)\n    dest_path = os.path.join(test_dir, folder_name if folder_name != \"openpose_img\" else \"openpose-img\")\n    if os.path.isdir(source_path) and not os.path.lexists(dest_path):\n        os.symlink(source_path, dest_path)\nprint(\"✅ Data links are ready.\")\n\n# --- THE FINAL FIX: CORRECTED DATA CLEANING LOGIC ---\nprint(\"\\nCleaning test pairs file with CORRECTED filename logic...\")\noriginal_pairs_path = \"/kaggle/input/clothe/clothes_tryon_dataset/test_pairs.txt\"\nclean_pairs_path = os.path.join(data_root, \"test_pairs_clean.txt\")\njson_dir = os.path.join(source_test_dir, \"openpose_json\")\nimg_dir = os.path.join(source_test_dir, \"openpose_img\")\nvalid_pairs = []\ntotal_pairs = 0\n\nwith open(original_pairs_path, \"r\") as f:\n    pairs = f.readlines()\n    total_pairs = len(pairs)\n    for pair in tqdm(pairs, desc=\"Verifying pairs\"):\n        person_fn, _ = pair.strip().split()\n        base_name, _ = os.path.splitext(person_fn)\n        \n        # THE FIX IS HERE: We now check for the CORRECT json filename format\n        expected_json = os.path.join(json_dir, f\"{base_name}_keypoints.json\")\n        expected_img = os.path.join(img_dir, f\"{base_name}_rendered.png\")\n        \n        if os.path.exists(expected_json) and os.path.exists(expected_img):\n            valid_pairs.append(pair)\n\nif not valid_pairs:\n    raise ValueError(f\"CRITICAL ERROR: Data cleaning resulted in 0 valid pairs out of {total_pairs}.\\n\"\n                     \"Even with the corrected filename check, no valid pairs were found. Please manually inspect your dataset.\")\n\nwith open(clean_pairs_path, \"w\") as f:\n    f.writelines(valid_pairs)\nprint(f\"✅ Data cleaning complete. Found {len(valid_pairs)} valid pairs out of {total_pairs}.\")\n\n\n# --- Step 2: Run Inference with the CLEAN file ---\nprint(\"\\n🚀 Starting the Full End-to-End Inference Pipeline...\")\nexperiment_name = \"final_inference_run_final_fix\"\nfinal_output_dir = \"/kaggle/working/final_tryon_images/\"\nos.makedirs(final_output_dir, exist_ok=True)\n\n!python test.py \\\n    --name {experiment_name} \\\n    --dataset_dir {data_root} \\\n    --dataset_list {clean_pairs_path} \\\n    --gmm_checkpoint {safe_gmm_path} \\\n    --alias_checkpoint {safe_alias_path} \\\n    --seg_checkpoint {safe_seg_path} \\\n    --save_dir {final_output_dir} \\\n    --workers 4 \\\n    --batch_size 4\n\nprint(\"\\n---------------------------------\")\nprint(\"✅ Full inference pipeline complete.\")\nprint(f\"Check for final images in: {final_output_dir}\")\nprint(\"---------------------------------\")\n\n\n# --- Step 3: Visualize the Results ---\nprint(\"\\n🖼️ Displaying Final Results...\")\nnum_examples = 3\nwith open(clean_pairs_path, \"r\") as f:\n    test_pairs = [line.strip().split() for line in f.readlines()]\nfor i in range(min(num_examples, len(test_pairs))):\n    person_fn, cloth_fn = test_pairs[i]\n    result_path = os.path.join(final_output_dir, person_fn)\n    person_path = os.path.join(test_dir, \"image\", person_fn)\n    cloth_path = os.path.join(test_dir, \"cloth\", cloth_fn)\n    if not os.path.exists(result_path):\n        print(f\"Result file not found, skipping: {result_path}\")\n        continue\n    person_img = Image.open(person_path).convert(\"RGB\")\n    cloth_img = Image.open(cloth_path).convert(\"RGB\")\n    result_img = Image.open(result_path).convert(\"RGB\")\n    fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n    axes[0].imshow(person_img); axes[0].set_title(f\"Original Person\\n({person_fn})\"); axes[0].axis('off')\n    axes[1].imshow(cloth_img); axes[1].set_title(f\"Garment\\n({cloth_fn})\"); axes[1].axis('off')\n    axes[2].imshow(result_img); axes[2].set_title(\"Generated Try-On Result\"); axes[2].axis('off')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T22:13:43.270894Z","iopub.execute_input":"2025-07-19T22:13:43.271311Z","iopub.status.idle":"2025-07-19T22:13:52.936016Z","shell.execute_reply.started":"2025-07-19T22:13:43.271283Z","shell.execute_reply":"2025-07-19T22:13:52.935109Z"}},"outputs":[{"name":"stdout","text":"🚀 Preparing complete environment...\n/kaggle/working/VITON-HD\n✅ Safe model links are ready.\n✅ Data links are ready.\n\nCleaning test pairs file with CORRECTED filename logic...\n","output_type":"stream"},{"name":"stderr","text":"Verifying pairs: 100%|██████████| 2032/2032 [00:02<00:00, 977.18it/s] \n","output_type":"stream"},{"name":"stdout","text":"✅ Data cleaning complete. Found 2032 valid pairs out of 2032.\n\n🚀 Starting the Full End-to-End Inference Pipeline...\nNamespace(name='final_inference_run_final_fix', batch_size=4, workers=4, load_height=1024, load_width=768, shuffle=False, dataset_dir='/kaggle/working/VITON-HD/data', dataset_mode='test', dataset_list='/kaggle/working/VITON-HD/data/test_pairs_clean.txt', checkpoint_dir='./checkpoints/', save_dir='/kaggle/working/final_tryon_images/', display_freq=1, seg_checkpoint='/kaggle/working/safe_model_links/seg_final.pth', gmm_checkpoint='/kaggle/working/safe_model_links/gmm_final.pth', alias_checkpoint='/kaggle/working/safe_model_links/alias_final.pth', semantic_nc=13, init_type='xavier', init_variance=0.02, grid_size=5, norm_G='spectralaliasinstance', ngf=64, num_upsampling_layers='most')\nNetwork [SegGenerator] was created. Total number of parameters: 34.5 million. To see the architecture, do print(network).\nNetwork [ALIASGenerator] was created. Total number of parameters: 100.5 million. To see the architecture, do print(network).\nTraceback (most recent call last):\n  File \"/kaggle/working/VITON-HD/test.py\", line 155, in <module>\n    main()\n  File \"/kaggle/working/VITON-HD/test.py\", line 151, in main\n    test(opt, seg, gmm, alias)\n  File \"/kaggle/working/VITON-HD/test.py\", line 64, in test\n    for i, inputs in enumerate(test_loader.data_loader):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n    return self._process_data(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n    data.reraise()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n    raise exception\nFileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/kaggle/working/VITON-HD/datasets.py\", line 143, in __getitem__\n    with open(osp.join(self.data_path, 'openpose-json', pose_name), 'r') as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/kaggle/working/VITON-HD/data/test/openpose-json/02887_00_keypoints.json'\n\n\n---------------------------------\n✅ Full inference pipeline complete.\nCheck for final images in: /kaggle/working/final_tryon_images/\n---------------------------------\n\n🖼️ Displaying Final Results...\nResult file not found, skipping: /kaggle/working/final_tryon_images/05006_00.jpg\nResult file not found, skipping: /kaggle/working/final_tryon_images/02532_00.jpg\nResult file not found, skipping: /kaggle/working/final_tryon_images/03921_00.jpg\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"%%writefile dataset.py\n# In dataset.py\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nimport json\nimport numpy as np\n\nclass VitonHDDataset(Dataset):\n    \"\"\"\n    VitonHD Dataset for training and testing.\n    Loads all necessary data inputs for GMM and TOM.\n    \"\"\"\n    def __init__(self, opt, is_train=True):\n        self.opt = opt\n        self.data_root = opt.dataroot\n        self.data_mode = 'train' if is_train else 'test'\n\n        # Define paths to data directories\n        self.image_dir = os.path.join(self.data_root, self.data_mode, 'image')\n        self.cloth_dir = os.path.join(self.data_root, self.data_mode, 'cloth')\n        self.cloth_mask_dir = os.path.join(self.data_root, self.data_mode, 'cloth-mask')\n        self.image_parse_dir = os.path.join(self.data_root, self.data_mode, 'image-parse-v3')\n        self.openpose_img_dir = os.path.join(self.data_root, self.data_mode, 'openpose_img')\n\n        # Load the list of image pairs (person, cloth) from a text file\n        self.pair_list_path = os.path.join(self.data_root, f'{self.data_mode}_pairs.txt')\n        self.image_pairs = self._load_pairs()\n\n        # Define standard image transformations\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        self.to_tensor = transforms.ToTensor()\n\n    def _load_pairs(self):\n        pairs = []\n        with open(self.pair_list_path, 'r') as f:\n            for line in f.readlines():\n                person_name, cloth_name = line.strip().split()\n                pairs.append((person_name, cloth_name))\n        return pairs\n\n    def __len__(self):\n        return len(self.image_pairs)\n\n    def __getitem__(self, idx):\n        person_name, cloth_name = self.image_pairs[idx]\n\n        # 1. Load Cloth image and its mask\n        cloth_path = os.path.join(self.cloth_dir, cloth_name)\n        cloth_image = Image.open(cloth_path).convert('RGB').resize((self.opt.load_width, self.opt.load_height))\n        cloth_mask_path = os.path.join(self.cloth_mask_dir, cloth_name)\n        cloth_mask = Image.open(cloth_mask_path).convert('L').resize((self.opt.load_width, self.opt.load_height))\n\n        # 2. Load Person Image\n        person_path = os.path.join(self.image_dir, person_name)\n        person_image = Image.open(person_path).convert('RGB').resize((self.opt.load_width, self.opt.load_height))\n\n        # 3. Load Person Segmentation Map (Parse Map)\n        parse_path = os.path.join(self.image_parse_dir, person_name.replace('.jpg', '.png'))\n        parse_map = Image.open(parse_path).convert('L').resize((self.opt.load_width, self.opt.load_height), Image.NEAREST)\n        parse_array = np.array(parse_map)\n\n        # Create the agnostic person image (person with original clothes masked out)\n        # These parse labels correspond to clothing parts\n        parse_cloth_labels = [5, 6, 7] \n        parse_cloth_mask = np.isin(parse_array, parse_cloth_labels)\n        agnostic_image = Image.fromarray((np.array(person_image) * (1 - np.expand_dims(parse_cloth_mask, -1))).astype(np.uint8))\n        # 4. Load Pose Map\n        pose_img_path = os.path.join(self.openpose_img_dir, person_name.replace('.jpg', '_rendered.png'))\n        pose_map = Image.open(pose_img_path).convert('RGB').resize((self.opt.load_width, self.opt.load_height))\n\n        # Create the person representation for the GMM\n        # This combines the person's shape (from parse map) and pose\n        # In dataset.py, inside the VitonHDDataset class's __getitem__ method\n\n# ... (all the loading code remains the same) ...\n\n# Apply final transformations to all images\n        cloth_tensor = self.transform(cloth_image)\n        agnostic_image_tensor = self.transform(agnostic_image)\n        pose_map_tensor = self.transform(pose_map)\n        person_image_tensor = self.transform(person_image)\n        cloth_mask_tensor = self.to_tensor(cloth_mask)\n        \n        # Create the agnostic person parse map (1 channel)\n        agnostic_parse_array = parse_array * (1 - parse_cloth_mask)\n        agnostic_parse_map = Image.fromarray(agnostic_parse_array.astype(np.uint8))\n        agnostic_parse_tensor = self.to_tensor(agnostic_parse_map) # This is already 1 channel\n        \n        # Create the NEW 7-channel person representation for the GMM\n        # It combines the agnostic image (3ch), pose map (3ch), and agnostic parse map (1ch)\n        gmm_person_representation = torch.cat([agnostic_image_tensor, pose_map_tensor, agnostic_parse_tensor], 0)\n        \n        data = {\n            'cloth': cloth_tensor,\n            'cloth_mask': cloth_mask_tensor,\n            'person_image': person_image_tensor,\n            'agnostic_person': agnostic_image_tensor,\n            'gmm_person_representation': gmm_person_representation, # This is now 7 channels\n            'person_name': person_name,\n            'cloth_name': cloth_name,\n}\n\n        return data\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T03:37:15.915514Z","iopub.execute_input":"2025-07-20T03:37:15.915847Z","iopub.status.idle":"2025-07-20T03:37:15.923931Z","shell.execute_reply.started":"2025-07-20T03:37:15.915818Z","shell.execute_reply":"2025-07-20T03:37:15.923305Z"}},"outputs":[{"name":"stdout","text":"Overwriting dataset.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile models.py\n# In models.py\n# In models.py\nimport torch.nn as nn  # <-- ADD THIS LINE\nimport torch.nn.functional as F\n\nimport torch\n# ... (imports remain the same) ...\n\n# --- Geometric Matching Module (GMM) - FINAL ARCHITECTURE ---\n\nclass GMM(nn.Module):\n    \"\"\"\n    This GMM architecture EXACTLY matches the layer names and structure\n    from the pre-trained weights file, solving both the size mismatch\n    and runtime errors.\n    \"\"\"\n    def __init__(self, opt):\n        super(GMM, self).__init__()\n        \n        # Person Representation Feature Extraction ('extractionA')\n        # Input: 7 channels (as per original error). Output: progresses to 512 channels.\n        extractionA_model = [\n            nn.Conv2d(opt.person_rep_channels, 64, kernel_size=4, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(512), nn.LeakyReLU(0.2, inplace=True),\n        ]\n        self.extractionA = nn.Sequential(*extractionA_model)\n\n        # Cloth Feature Extraction ('extractionB')\n        # Input: 3 channels. Output: progresses to 512 channels.\n        extractionB_model = [\n            nn.Conv2d(opt.cloth_channels, 64, kernel_size=4, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(512), nn.LeakyReLU(0.2, inplace=True),\n        ]\n        self.extractionB = nn.Sequential(*extractionB_model)\n        \n        # Flow Regression Network ('regression')\n        # This part requires more careful reconstruction based on typical architectures.\n        # It likely takes the two 512-channel feature maps and processes them.\n        # Let's assume a feature-correlation and upsampling path.\n        # A simple concatenation and regression is a good starting point.\n        regression_conv = [\n            # The input will be the concatenated features: 512 (person) + 512 (cloth) = 1024\n            nn.Conv2d(1024, 512, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n            nn.Conv2d(512, 256, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n            # Upsampling layers to restore original dimensions\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n            # Final layers to get to the 2-channel flow field\n            nn.Conv2d(64, 32, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n            nn.Conv2d(32, 2, kernel_size=3, padding=1),\n            nn.Tanh()\n        ]\n        self.regression = nn.Sequential(*regression_conv)\n        \n    def forward(self, person_rep, cloth):\n        # Pass inputs through their respective feature extractors\n        featureA = self.extractionA(person_rep)\n        featureB = self.extractionB(cloth)\n        \n        # Concatenate the final feature maps\n        x = torch.cat([featureA, featureB], 1)\n        \n        # Regress the flow field\n        flow = self.regression(x)\n        \n        # We need to upsample the flow to the original image size\n        flow = F.interpolate(flow, size=(person_rep.size(2), person_rep.size(3)), mode='bilinear', align_corners=True)\n        \n        return flow\n\n\n# --- Try-On Module (TOM) ---\n# ... (The TOM code can remain the same) ...\n    # ...\n# ... (rest of TOM code) ...\n\n# --- Try-On Module (TOM) ---\n# ... (The TOM code remains the same as it was not part of the error) ...\nclass UnetGenerator(nn.Module):\n    \"\"\"A standard U-Net generator for image-to-image translation.\"\"\"\n    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetGenerator, self).__init__()\n        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n        for i in range(num_downs - 5):\n            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, submodule=unet_block, norm_layer=norm_layer)\n        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass UnetSkipConnectionBlock(nn.Module):\n    def __init__(self, outer_nc, inner_nc, input_nc=None, submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n        if input_nc is None:\n            input_nc = outer_nc\n        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4, stride=2, padding=1, bias=False)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc)\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1)\n            down = [downconv]\n            up = [uprelu, upconv, nn.Tanh()]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc, kernel_size=4, stride=2, padding=1, bias=False)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1, bias=False)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n            model = down + [submodule] + up\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([x, self.model(x)], 1)\n\nclass TOM(nn.Module):\n    def __init__(self, opt):\n        super(TOM, self).__init__()\n        # Input to TOM: agnostic person (3ch) + warped cloth (3ch) = 6 channels\n        input_nc = opt.agnostic_channels + opt.cloth_channels\n        self.generator = UnetGenerator(input_nc, opt.output_channels, num_downs=7, ngf=64) # Increased num_downs for higher res\n\n    def forward(self, agnostic_person, warped_cloth):\n        x = torch.cat([agnostic_person, warped_cloth], 1)\n        return self.generator(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T03:37:47.297186Z","iopub.execute_input":"2025-07-20T03:37:47.298008Z","iopub.status.idle":"2025-07-20T03:37:47.305854Z","shell.execute_reply.started":"2025-07-20T03:37:47.297976Z","shell.execute_reply":"2025-07-20T03:37:47.305159Z"}},"outputs":[{"name":"stdout","text":"Overwriting models.py\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"%%writefile train.py\n# In train.py\n\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport os\nimport torch.nn as nn  # <-- ADD THIS LINE\n\ndef train_gmm(opt, gmm_model, dataset):\n    gmm_model.train()\n    dataloader = DataLoader(dataset, batch_size=opt.batch_size, shuffle=True, num_workers=4)\n    optimizer = optim.Adam(gmm_model.parameters(), lr=opt.lr)\n    \n    # GMM loss function: L1 loss between the warped cloth mask and the true cloth mask\n    criterionL1 = nn.L1Loss()\n\n    for epoch in range(opt.gmm_epochs):\n        for i, data in enumerate(dataloader):\n            cloth = data['cloth'].cuda()\n            cloth_mask = data['cloth_mask'].cuda()\n            person_rep = data['gmm_person_representation'].cuda()\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            flow = gmm_model(person_rep, cloth)\n            \n            # Warp the cloth mask using the predicted flow\n            warped_mask = F.grid_sample(cloth_mask, flow.permute(0, 2, 3, 1), mode='bilinear', padding_mode='zeros', align_corners=True)\n\n            # The loss encourages the warped cloth to align with the ground truth clothing area\n            loss = criterionL1(warped_mask, cloth_mask)\n            \n            loss.backward()\n            optimizer.step()\n\n            if (i+1) % 100 == 0:\n                print(f\"GMM - Epoch [{epoch+1}/{opt.gmm_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n\n        # Save model checkpoint after each epoch\n        torch.save(gmm_model.state_dict(), os.path.join(opt.checkpoint_dir, opt.name, f'gmm_epoch_{epoch+1}.pth'))\n\ndef train_tom(opt, tom_model, gmm_model, dataset):\n    tom_model.train()\n    gmm_model.eval()  # GMM is frozen and used for inference only\n    dataloader = DataLoader(dataset, batch_size=opt.batch_size, shuffle=True, num_workers=4)\n    optimizer = optim.Adam(tom_model.parameters(), lr=opt.lr)\n\n    # TOM loss: L1 loss + Perceptual loss (VGG) for realism\n    criterionL1 = nn.L1Loss()\n    # For a real project, implementing a VGGPerceptualLoss is highly recommended\n    # criterionVGG = VGGPerceptualLoss().cuda() \n\n    for epoch in range(opt.tom_epochs):\n        for i, data in enumerate(dataloader):\n            cloth = data['cloth'].cuda()\n            agnostic_person = data['agnostic_person'].cuda()\n            person_image = data['person_image'].cuda()\n            person_rep = data['gmm_person_representation'].cuda()\n\n            # Get the warped cloth from the (pre-trained or newly trained) GMM\n            with torch.no_grad():\n                flow = gmm_model(person_rep, cloth)\n                warped_cloth = F.grid_sample(cloth, flow.permute(0, 2, 3, 1), mode='bilinear', padding_mode='border', align_corners=True)\n\n            optimizer.zero_grad()\n\n            # Forward pass through TOM\n            generated_image = tom_model(agnostic_person, warped_cloth)\n\n            # Calculate loss against the ground truth person image\n            loss = criterionL1(generated_image, person_image)\n\n            loss.backward()\n            optimizer.step()\n\n            if (i+1) % 100 == 0:\n                print(f\"TOM - Epoch [{epoch+1}/{opt.tom_epochs}], Step [{i+1}/{len(dataloader)}], L1 Loss: {loss.item():.4f}\")\n\n        # Save model checkpoint\n        torch.save(tom_model.state_dict(), os.path.join(opt.checkpoint_dir, opt.name, f'tom_epoch_{epoch+1}.pth'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T03:47:07.776470Z","iopub.execute_input":"2025-07-20T03:47:07.777071Z","iopub.status.idle":"2025-07-20T03:47:07.784097Z","shell.execute_reply.started":"2025-07-20T03:47:07.777040Z","shell.execute_reply":"2025-07-20T03:47:07.783290Z"}},"outputs":[{"name":"stdout","text":"Overwriting train.py\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"%%writefile test.py\n# In test.py\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\nimport os\n\ndef test(opt, gmm_model, tom_model, dataset):\n    gmm_model.eval()\n    tom_model.eval()\n    dataloader = DataLoader(dataset, batch_size=opt.batch_size_test, shuffle=False, num_workers=4)\n    \n    output_dir = os.path.join(opt.result_dir, opt.name)\n    os.makedirs(output_dir, exist_ok=True)\n\n    for i, data in enumerate(dataloader):\n        cloth = data['cloth'].cuda()\n        agnostic_person = data['agnostic_person'].cuda()\n        person_rep = data['gmm_person_representation'].cuda()\n        person_name = data['person_name']\n        cloth_name = data['cloth_name']\n        \n        with torch.no_grad():\n            # Run GMM to get the warped cloth\n            flow = gmm_model(person_rep, cloth)\n            warped_cloth = F.grid_sample(cloth, flow.permute(0, 2, 3, 1), mode='bilinear', padding_mode='border', align_corners=True)\n            \n            # Run TOM to get the final try-on image\n            generated_image = tom_model(agnostic_person, warped_cloth)\n\n            # Save each result in the batch\n            for j in range(len(person_name)):\n                p_name = os.path.splitext(person_name[j])[0]\n                c_name = os.path.splitext(cloth_name[j])[0]\n                \n                # Create a visual comparison grid and save\n                visuals = torch.cat([\n                    (data['person_image'][j].cpu() + 1) / 2, # Original Person\n                    (cloth[j].cpu() + 1) / 2,               # Target Cloth\n                    (warped_cloth[j].cpu() + 1) / 2,         # Warped Cloth\n                    (generated_image[j].cpu() + 1) / 2       # Final Result\n                ], dim=2) # Concatenate horizontally for comparison\n                \n                save_path = os.path.join(output_dir, f\"{p_name}_tries_{c_name}.png\")\n                save_image(visuals, save_path)\n        \n        print(f\"Processed and saved results for batch {i+1}/{len(dataloader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T03:47:10.077447Z","iopub.execute_input":"2025-07-20T03:47:10.077752Z","iopub.status.idle":"2025-07-20T03:47:10.083149Z","shell.execute_reply.started":"2025-07-20T03:47:10.077730Z","shell.execute_reply":"2025-07-20T03:47:10.082443Z"}},"outputs":[{"name":"stdout","text":"Overwriting test.py\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"%%writefile main.py\n# In main.py\nimport argparse\nimport os\nimport torch\n\nfrom dataset import VitonHDDataset\nfrom models import GMM, TOM\nfrom train import train_gmm, train_tom\nfrom test import test\ndef clean_dataparallel_keys(state_dict):\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        if k.startswith('module.'):\n            name = k[7:]  # remove 'module.'\n            new_state_dict[name] = v\n        else:\n            new_state_dict[k] = v\n    return new_state_dict\ndef get_opt():\n    parser = argparse.ArgumentParser()\n    # --- Experiment and Data Options ---\n    parser.add_argument(\"--name\", default=\"HD-VITON_run\")\n    parser.add_argument(\"--gpu_ids\", default=\"0\", help=\"e.g., 0,1,2. use -1 for CPU\")\n    parser.add_argument(\"--mode\", default=\"test\", help=\"train | test\")\n    parser.add_argument(\"--dataroot\", required=True, help=\"path to the dataset folder\")\n    parser.add_argument(\"--checkpoint_dir\", default=\"./checkpoints\")\n    parser.add_argument(\"--result_dir\", default=\"./results\")\n\n    # --- Data Loading and Model Options ---\n    parser.add_argument(\"--batch_size\", type=int, default=4)\n    parser.add_argument(\"--batch_size_test\", type=int, default=1)\n    parser.add_argument(\"--load_height\", type=int, default=1024)\n    parser.add_argument(\"--load_width\", type=int, default=768)\n    parser.add_argument(\"--person_rep_channels\", type=int, default=4) # 1ch parse map + 3ch pose map\n    parser.add_argument(\"--cloth_channels\", type=int, default=3)\n    parser.add_argument(\"--agnostic_channels\", type=int, default=3)\n    parser.add_argument(\"--output_channels\", type=int, default=3)\n\n    # --- Training Specific Options ---\n    parser.add_argument(\"--lr\", type=float, default=0.0001, help=\"learning rate\")\n    parser.add_argument(\"--gmm_epochs\", type=int, default=50)\n    parser.add_argument(\"--tom_epochs\", type=int, default=50)\n\n    # --- Checkpoint Loading ---\n    parser.add_argument(\"--gmm_checkpoint\", default=None, help=\"path to GMM pre-trained weights\")\n    parser.add_argument(\"--tom_checkpoint\", default=None, help=\"path to TOM pre-trained weights\")\n\n    opt = parser.parse_args()\n    return opt\n\n# In main.py\n\ndef main():\n    opt = get_opt()\n    \n    # Create directories for checkpoints and results\n    os.makedirs(os.path.join(opt.checkpoint_dir, opt.name), exist_ok=True)\n    os.makedirs(os.path.join(opt.result_dir, opt.name), exist_ok=True)\n\n    # --- THIS SECTION IS CORRECTED ---\n    # Correctly parse the GPU IDs string into a list of integers\n    gpu_ids_list = []\n    if opt.gpu_ids:\n        str_ids = opt.gpu_ids.split(',')\n        for str_id in str_ids:\n            try:\n                # Use strip() to remove any accidental whitespace\n                gpu_id = int(str_id.strip())\n                if gpu_id >= 0:\n                    gpu_ids_list.append(gpu_id)\n            except ValueError:\n                print(f\"Warning: Could not parse GPU ID '{str_id}'. Skipping.\")\n    \n    # Set the primary device. For DataParallel, this is typically cuda:0\n    # The wrapper will handle distributing to other GPUs.\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() and gpu_ids_list else \"cpu\")\n\n    # --- Initialize Dataset ---\n    is_train = opt.mode == 'train'\n    dataset = VitonHDDataset(opt, is_train=is_train)\n    print(f\"Dataset created for mode: '{opt.mode}'. Found {len(dataset)} samples.\")\n\n    # --- Initialize Models ---\n    gmm_model = GMM(opt).to(device)\n    tom_model = TOM(opt).to(device)\n    \n    # --- Wrap models for Multi-GPU if more than one ID is provided ---\n    if len(gpu_ids_list) > 1:\n        # This print statement is now correct\n        print(f\"Using {len(gpu_ids_list)} GPUs: {gpu_ids_list}\")\n        gmm_model = torch.nn.DataParallel(gmm_model, device_ids=gpu_ids_list)\n        tom_model = torch.nn.DataParallel(tom_model, device_ids=gpu_ids_list)\n    \n    # --- Checkpoint loading and the rest of the script remain the same ---\n    if opt.gmm_checkpoint:\n        gmm_model.load_state_dict(torch.load(opt.gmm_checkpoint, map_location=device), strict=False)\n        print(f\"GMM checkpoint loaded from: {opt.gmm_checkpoint}\")\n    if opt.tom_checkpoint:\n        tom_model.load_state_dict(torch.load(opt.tom_checkpoint, map_location=device), strict=False)\n        print(f\"TOM checkpoint loaded from: {opt.tom_checkpoint}\")\n\n    # ... (rest of the main function: calling train or test) ...\n    if opt.mode == 'train':\n        print(\"Starting GMM training...\")\n        train_gmm(opt, gmm_model, dataset)\n        print(\"GMM training complete. Starting TOM training...\")\n        \n        # When saving/loading in a multi-gpu setup, it's safer to access the underlying model\n        # However, for this project structure, a simple load will often work.\n        best_gmm_path = os.path.join(opt.checkpoint_dir, opt.name, f'gmm_epoch_{opt.gmm_epochs}.pth')\n        gmm_model.load_state_dict(torch.load(best_gmm_path, map_location=device))\n        \n        train_tom(opt, tom_model, gmm_model, dataset)\n        print(\"All training finished.\")\n        \n    elif opt.mode == 'test':\n        if not opt.gmm_checkpoint or not opt.tom_checkpoint:\n            raise ValueError(\"In test mode, you must provide paths to pre-trained --gmm_checkpoint and --tom_checkpoint.\")\n        print(\"Starting testing...\")\n        test(opt, gmm_model, tom_model, dataset)\n        print(f\"Testing complete. Results saved in: {os.path.join(opt.result_dir, opt.name)}\")\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:36:12.228136Z","iopub.execute_input":"2025-07-20T08:36:12.228657Z","iopub.status.idle":"2025-07-20T08:36:12.235804Z","shell.execute_reply.started":"2025-07-20T08:36:12.228619Z","shell.execute_reply":"2025-07-20T08:36:12.235198Z"}},"outputs":[{"name":"stdout","text":"Overwriting main.py\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"!python main.py \\\n  --mode test \\\n  --dataroot \"/kaggle/input/clothe/clothes_tryon_dataset\" \\\n  --name \"HD-VITON-Final-Attempt\" \\\n  --gpu_ids 0 \\\n  --gmm_checkpoint \"/kaggle/input/weights/pytorch/default/1/gmm_final (1).pth\" \\\n  --tom_checkpoint \"/kaggle/input/weights/pytorch/default/1/alias_final.pth\" \\\n  --result_dir \"/kaggle/working/\" \\\n  --person_rep_channels 7","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"Dataset created for mode: 'test'. Found 2032 samples.\nGMM checkpoint loaded from: /kaggle/input/weights/pytorch/default/1/gmm_final (1).pth\nTOM checkpoint loaded from: /kaggle/input/weights/pytorch/default/1/alias_final.pth\nStarting testing...\n^C\nTraceback (most recent call last):\n  File \"/kaggle/working/main.py\", line 91, in <module>\n    main()\n  File \"/kaggle/working/main.py\", line 87, in main\n    test(opt, gmm_model, tom_model, dataset)\n  File \"/kaggle/working/test.py\", line 17, in test\n    for i, data in enumerate(dataloader):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1458, in _next_data\n    idx, data = self._get_data()\n                ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1420, in _get_data\n    success, data = self._try_get_data()\n                    ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1251, in _try_get_data\n    data = self._data_queue.get(timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 113, in get\n    if not self._poll(timeout):\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 257, in poll\n    return self._poll(timeout)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 440, in _poll\n    r = wait([self], timeout)\n        ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n    ready = selector.select(timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n    fd_event_list = self._selector.poll(timeout)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!python main.py \\\n  --mode train \\\n  --dataroot \"/kaggle/input/clothe/clothes_tryon_dataset\" \\\n  --name \"FineTuned-MultiGPU\" \\\n  --gpu_ids 0,1 \\\n  --person_rep_channels 7 \\\n  --batch_size 4 \\\n  --lr 0.00005 \\\n  --gmm_epochs 15 \\\n  --tom_epochs 5 \\\n  --checkpoint_dir \"/kaggle/working/finetuned_checkpoints_multi_gpu\" \\\n  --gmm_checkpoint \"/kaggle/input/weights/pytorch/default/1/gmm_final (1).pth\" \\\n  --tom_checkpoint \"/kaggle/input/weights/pytorch/default/1/alias_final.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:59:56.621809Z","iopub.execute_input":"2025-07-20T08:59:56.622401Z","iopub.status.idle":"2025-07-20T12:59:57.480665Z","shell.execute_reply.started":"2025-07-20T08:59:56.622375Z","shell.execute_reply":"2025-07-20T12:59:57.479911Z"}},"outputs":[{"name":"stdout","text":"Dataset created for mode: 'train'. Found 11647 samples.\nUsing 2 GPUs: [0, 1]\nGMM checkpoint loaded from: /kaggle/input/weights/pytorch/default/1/gmm_final (1).pth\nTOM checkpoint loaded from: /kaggle/input/weights/pytorch/default/1/alias_final.pth\nStarting GMM training...\nGMM - Epoch [1/15], Step [100/2912], Loss: 0.0284\nGMM - Epoch [1/15], Step [200/2912], Loss: 0.0782\nGMM - Epoch [1/15], Step [300/2912], Loss: 0.0850\nGMM - Epoch [1/15], Step [400/2912], Loss: 0.0438\nGMM - Epoch [1/15], Step [500/2912], Loss: 0.0312\nGMM - Epoch [1/15], Step [600/2912], Loss: 0.0083\nGMM - Epoch [1/15], Step [700/2912], Loss: 0.0168\nGMM - Epoch [1/15], Step [800/2912], Loss: 0.0111\nGMM - Epoch [1/15], Step [900/2912], Loss: 0.0546\nGMM - Epoch [1/15], Step [1000/2912], Loss: 0.0093\nGMM - Epoch [1/15], Step [1100/2912], Loss: 0.0418\nGMM - Epoch [1/15], Step [1200/2912], Loss: 0.0050\nGMM - Epoch [1/15], Step [1300/2912], Loss: 0.0058\nGMM - Epoch [1/15], Step [1400/2912], Loss: 0.0467\nGMM - Epoch [1/15], Step [1500/2912], Loss: 0.0082\nGMM - Epoch [1/15], Step [1600/2912], Loss: 0.0044\nGMM - Epoch [1/15], Step [1700/2912], Loss: 0.0193\nGMM - Epoch [1/15], Step [1800/2912], Loss: 0.0071\nGMM - Epoch [1/15], Step [1900/2912], Loss: 0.0125\nGMM - Epoch [1/15], Step [2000/2912], Loss: 0.0056\nGMM - Epoch [1/15], Step [2100/2912], Loss: 0.0040\nGMM - Epoch [1/15], Step [2200/2912], Loss: 0.0036\nGMM - Epoch [1/15], Step [2300/2912], Loss: 0.0168\nGMM - Epoch [1/15], Step [2400/2912], Loss: 0.0059\nGMM - Epoch [1/15], Step [2500/2912], Loss: 0.0046\nGMM - Epoch [1/15], Step [2600/2912], Loss: 0.0127\nGMM - Epoch [1/15], Step [2700/2912], Loss: 0.0204\nGMM - Epoch [1/15], Step [2800/2912], Loss: 0.0037\nGMM - Epoch [1/15], Step [2900/2912], Loss: 0.0277\nGMM - Epoch [2/15], Step [100/2912], Loss: 0.0204\nGMM - Epoch [2/15], Step [200/2912], Loss: 0.0120\nGMM - Epoch [2/15], Step [300/2912], Loss: 0.0082\nGMM - Epoch [2/15], Step [400/2912], Loss: 0.0099\nGMM - Epoch [2/15], Step [500/2912], Loss: 0.0035\nGMM - Epoch [2/15], Step [600/2912], Loss: 0.0659\nGMM - Epoch [2/15], Step [700/2912], Loss: 0.0217\nGMM - Epoch [2/15], Step [800/2912], Loss: 0.0074\nGMM - Epoch [2/15], Step [900/2912], Loss: 0.0200\nGMM - Epoch [2/15], Step [1000/2912], Loss: 0.0050\nGMM - Epoch [2/15], Step [1100/2912], Loss: 0.0060\nGMM - Epoch [2/15], Step [1200/2912], Loss: 0.0081\nGMM - Epoch [2/15], Step [1300/2912], Loss: 0.0105\nGMM - Epoch [2/15], Step [1400/2912], Loss: 0.0031\nGMM - Epoch [2/15], Step [1500/2912], Loss: 0.0035\nGMM - Epoch [2/15], Step [1600/2912], Loss: 0.0095\nGMM - Epoch [2/15], Step [1700/2912], Loss: 0.0336\nGMM - Epoch [2/15], Step [1800/2912], Loss: 0.0036\nGMM - Epoch [2/15], Step [1900/2912], Loss: 0.0531\nGMM - Epoch [2/15], Step [2000/2912], Loss: 0.0039\nGMM - Epoch [2/15], Step [2100/2912], Loss: 0.0040\nGMM - Epoch [2/15], Step [2200/2912], Loss: 0.0029\nGMM - Epoch [2/15], Step [2300/2912], Loss: 0.0108\nGMM - Epoch [2/15], Step [2400/2912], Loss: 0.0039\nGMM - Epoch [2/15], Step [2500/2912], Loss: 0.0043\nGMM - Epoch [2/15], Step [2600/2912], Loss: 0.0153\nGMM - Epoch [2/15], Step [2700/2912], Loss: 0.0169\nGMM - Epoch [2/15], Step [2800/2912], Loss: 0.0084\nGMM - Epoch [2/15], Step [2900/2912], Loss: 0.0037\nGMM - Epoch [3/15], Step [100/2912], Loss: 0.0055\nGMM - Epoch [3/15], Step [200/2912], Loss: 0.0084\nGMM - Epoch [3/15], Step [300/2912], Loss: 0.0028\nGMM - Epoch [3/15], Step [400/2912], Loss: 0.0245\nGMM - Epoch [3/15], Step [500/2912], Loss: 0.0036\nGMM - Epoch [3/15], Step [600/2912], Loss: 0.0061\nGMM - Epoch [3/15], Step [700/2912], Loss: 0.0050\nGMM - Epoch [3/15], Step [800/2912], Loss: 0.0216\nGMM - Epoch [3/15], Step [900/2912], Loss: 0.0093\nGMM - Epoch [3/15], Step [1000/2912], Loss: 0.0157\nGMM - Epoch [3/15], Step [1100/2912], Loss: 0.0109\nGMM - Epoch [3/15], Step [1200/2912], Loss: 0.0060\nGMM - Epoch [3/15], Step [1300/2912], Loss: 0.0041\nGMM - Epoch [3/15], Step [1400/2912], Loss: 0.1469\nGMM - Epoch [3/15], Step [1500/2912], Loss: 0.0053\nGMM - Epoch [3/15], Step [1600/2912], Loss: 0.0061\nGMM - Epoch [3/15], Step [1700/2912], Loss: 0.0085\nGMM - Epoch [3/15], Step [1800/2912], Loss: 0.0897\nGMM - Epoch [3/15], Step [1900/2912], Loss: 0.0055\nGMM - Epoch [3/15], Step [2000/2912], Loss: 0.0035\nGMM - Epoch [3/15], Step [2100/2912], Loss: 0.0150\nGMM - Epoch [3/15], Step [2200/2912], Loss: 0.0042\nGMM - Epoch [3/15], Step [2300/2912], Loss: 0.0031\nGMM - Epoch [3/15], Step [2400/2912], Loss: 0.0038\nGMM - Epoch [3/15], Step [2500/2912], Loss: 0.0033\nGMM - Epoch [3/15], Step [2600/2912], Loss: 0.0219\nGMM - Epoch [3/15], Step [2700/2912], Loss: 0.0266\nGMM - Epoch [3/15], Step [2800/2912], Loss: 0.0161\nGMM - Epoch [3/15], Step [2900/2912], Loss: 0.0048\nGMM - Epoch [4/15], Step [100/2912], Loss: 0.0069\nGMM - Epoch [4/15], Step [200/2912], Loss: 0.0046\nGMM - Epoch [4/15], Step [300/2912], Loss: 0.0026\nGMM - Epoch [4/15], Step [400/2912], Loss: 0.0039\nGMM - Epoch [4/15], Step [500/2912], Loss: 0.0197\nGMM - Epoch [4/15], Step [600/2912], Loss: 0.0070\nGMM - Epoch [4/15], Step [700/2912], Loss: 0.0066\nGMM - Epoch [4/15], Step [800/2912], Loss: 0.0103\nGMM - Epoch [4/15], Step [900/2912], Loss: 0.0099\nGMM - Epoch [4/15], Step [1000/2912], Loss: 0.0037\nGMM - Epoch [4/15], Step [1100/2912], Loss: 0.0062\nGMM - Epoch [4/15], Step [1200/2912], Loss: 0.0077\nGMM - Epoch [4/15], Step [1300/2912], Loss: 0.0100\nGMM - Epoch [4/15], Step [1400/2912], Loss: 0.0092\nGMM - Epoch [4/15], Step [1500/2912], Loss: 0.0029\nGMM - Epoch [4/15], Step [1600/2912], Loss: 0.0049\nGMM - Epoch [4/15], Step [1700/2912], Loss: 0.0037\nGMM - Epoch [4/15], Step [1800/2912], Loss: 0.0081\nGMM - Epoch [4/15], Step [1900/2912], Loss: 0.0212\nGMM - Epoch [4/15], Step [2000/2912], Loss: 0.0039\nGMM - Epoch [4/15], Step [2100/2912], Loss: 0.0041\nGMM - Epoch [4/15], Step [2200/2912], Loss: 0.0049\nGMM - Epoch [4/15], Step [2300/2912], Loss: 0.0037\nGMM - Epoch [4/15], Step [2400/2912], Loss: 0.0039\nGMM - Epoch [4/15], Step [2500/2912], Loss: 0.0025\nGMM - Epoch [4/15], Step [2600/2912], Loss: 0.0049\nGMM - Epoch [4/15], Step [2700/2912], Loss: 0.0391\nGMM - Epoch [4/15], Step [2800/2912], Loss: 0.0023\nGMM - Epoch [4/15], Step [2900/2912], Loss: 0.0052\nGMM - Epoch [5/15], Step [100/2912], Loss: 0.0038\nGMM - Epoch [5/15], Step [200/2912], Loss: 0.0059\nGMM - Epoch [5/15], Step [300/2912], Loss: 0.0059\nGMM - Epoch [5/15], Step [400/2912], Loss: 0.0030\nGMM - Epoch [5/15], Step [500/2912], Loss: 0.0034\nGMM - Epoch [5/15], Step [600/2912], Loss: 0.0091\nGMM - Epoch [5/15], Step [700/2912], Loss: 0.0055\nGMM - Epoch [5/15], Step [800/2912], Loss: 0.0444\nGMM - Epoch [5/15], Step [900/2912], Loss: 0.0083\nGMM - Epoch [5/15], Step [1000/2912], Loss: 0.0028\nGMM - Epoch [5/15], Step [1100/2912], Loss: 0.0030\nGMM - Epoch [5/15], Step [1200/2912], Loss: 0.0036\nGMM - Epoch [5/15], Step [1300/2912], Loss: 0.0027\nGMM - Epoch [5/15], Step [1400/2912], Loss: 0.0156\nGMM - Epoch [5/15], Step [1500/2912], Loss: 0.0040\nGMM - Epoch [5/15], Step [1600/2912], Loss: 0.0089\nGMM - Epoch [5/15], Step [1700/2912], Loss: 0.0078\nGMM - Epoch [5/15], Step [1800/2912], Loss: 0.0050\nGMM - Epoch [5/15], Step [1900/2912], Loss: 0.0066\nGMM - Epoch [5/15], Step [2000/2912], Loss: 0.0032\nGMM - Epoch [5/15], Step [2100/2912], Loss: 0.0040\nGMM - Epoch [5/15], Step [2200/2912], Loss: 0.0486\nGMM - Epoch [5/15], Step [2300/2912], Loss: 0.0035\nGMM - Epoch [5/15], Step [2400/2912], Loss: 0.0034\nGMM - Epoch [5/15], Step [2500/2912], Loss: 0.0076\nGMM - Epoch [5/15], Step [2600/2912], Loss: 0.0064\nGMM - Epoch [5/15], Step [2700/2912], Loss: 0.0068\nGMM - Epoch [5/15], Step [2800/2912], Loss: 0.0035\nGMM - Epoch [5/15], Step [2900/2912], Loss: 0.0046\nGMM - Epoch [6/15], Step [100/2912], Loss: 0.0091\nGMM - Epoch [6/15], Step [200/2912], Loss: 0.0091\nGMM - Epoch [6/15], Step [300/2912], Loss: 0.0035\nGMM - Epoch [6/15], Step [400/2912], Loss: 0.0038\nGMM - Epoch [6/15], Step [500/2912], Loss: 0.0065\nGMM - Epoch [6/15], Step [600/2912], Loss: 0.0089\nGMM - Epoch [6/15], Step [700/2912], Loss: 0.0033\nGMM - Epoch [6/15], Step [800/2912], Loss: 0.0307\nGMM - Epoch [6/15], Step [900/2912], Loss: 0.0028\nGMM - Epoch [6/15], Step [1000/2912], Loss: 0.0043\nGMM - Epoch [6/15], Step [1100/2912], Loss: 0.0070\nGMM - Epoch [6/15], Step [1200/2912], Loss: 0.0034\nGMM - Epoch [6/15], Step [1300/2912], Loss: 0.0133\nGMM - Epoch [6/15], Step [1400/2912], Loss: 0.0045\nGMM - Epoch [6/15], Step [1500/2912], Loss: 0.0058\nGMM - Epoch [6/15], Step [1600/2912], Loss: 0.0160\nGMM - Epoch [6/15], Step [1700/2912], Loss: 0.0026\nGMM - Epoch [6/15], Step [1800/2912], Loss: 0.0043\nGMM - Epoch [6/15], Step [1900/2912], Loss: 0.0038\nGMM - Epoch [6/15], Step [2000/2912], Loss: 0.0030\nGMM - Epoch [6/15], Step [2100/2912], Loss: 0.0096\nGMM - Epoch [6/15], Step [2200/2912], Loss: 0.0028\nGMM - Epoch [6/15], Step [2300/2912], Loss: 0.0083\nGMM - Epoch [6/15], Step [2400/2912], Loss: 0.0361\nGMM - Epoch [6/15], Step [2500/2912], Loss: 0.0029\nGMM - Epoch [6/15], Step [2600/2912], Loss: 0.0312\nGMM - Epoch [6/15], Step [2700/2912], Loss: 0.0024\nGMM - Epoch [6/15], Step [2800/2912], Loss: 0.0063\nGMM - Epoch [6/15], Step [2900/2912], Loss: 0.0062\nGMM - Epoch [7/15], Step [100/2912], Loss: 0.0050\nGMM - Epoch [7/15], Step [200/2912], Loss: 0.0039\nGMM - Epoch [7/15], Step [300/2912], Loss: 0.0029\nGMM - Epoch [7/15], Step [400/2912], Loss: 0.0170\nGMM - Epoch [7/15], Step [500/2912], Loss: 0.0106\nGMM - Epoch [7/15], Step [600/2912], Loss: 0.0040\nGMM - Epoch [7/15], Step [700/2912], Loss: 0.0187\nGMM - Epoch [7/15], Step [800/2912], Loss: 0.0203\nGMM - Epoch [7/15], Step [900/2912], Loss: 0.0029\nGMM - Epoch [7/15], Step [1000/2912], Loss: 0.0085\nGMM - Epoch [7/15], Step [1100/2912], Loss: 0.0041\nGMM - Epoch [7/15], Step [1200/2912], Loss: 0.0436\nGMM - Epoch [7/15], Step [1300/2912], Loss: 0.0054\nGMM - Epoch [7/15], Step [1400/2912], Loss: 0.0036\nGMM - Epoch [7/15], Step [1500/2912], Loss: 0.0057\nGMM - Epoch [7/15], Step [1600/2912], Loss: 0.0039\nGMM - Epoch [7/15], Step [1700/2912], Loss: 0.0244\nGMM - Epoch [7/15], Step [1800/2912], Loss: 0.0032\nGMM - Epoch [7/15], Step [1900/2912], Loss: 0.0074\nGMM - Epoch [7/15], Step [2000/2912], Loss: 0.0099\nGMM - Epoch [7/15], Step [2100/2912], Loss: 0.0031\nGMM - Epoch [7/15], Step [2200/2912], Loss: 0.0226\nGMM - Epoch [7/15], Step [2300/2912], Loss: 0.0039\nGMM - Epoch [7/15], Step [2400/2912], Loss: 0.0115\nGMM - Epoch [7/15], Step [2500/2912], Loss: 0.0037\nGMM - Epoch [7/15], Step [2600/2912], Loss: 0.0046\nGMM - Epoch [7/15], Step [2700/2912], Loss: 0.0377\nGMM - Epoch [7/15], Step [2800/2912], Loss: 0.0085\nGMM - Epoch [7/15], Step [2900/2912], Loss: 0.0036\nGMM - Epoch [8/15], Step [100/2912], Loss: 0.0110\nGMM - Epoch [8/15], Step [200/2912], Loss: 0.0032\nGMM - Epoch [8/15], Step [300/2912], Loss: 0.0027\nGMM - Epoch [8/15], Step [400/2912], Loss: 0.0104\nGMM - Epoch [8/15], Step [500/2912], Loss: 0.0174\nGMM - Epoch [8/15], Step [600/2912], Loss: 0.0089\nGMM - Epoch [8/15], Step [700/2912], Loss: 0.0034\nGMM - Epoch [8/15], Step [800/2912], Loss: 0.0052\nGMM - Epoch [8/15], Step [900/2912], Loss: 0.0060\nGMM - Epoch [8/15], Step [1000/2912], Loss: 0.0038\nGMM - Epoch [8/15], Step [1100/2912], Loss: 0.0051\nGMM - Epoch [8/15], Step [1200/2912], Loss: 0.0028\nGMM - Epoch [8/15], Step [1300/2912], Loss: 0.0170\nGMM - Epoch [8/15], Step [1400/2912], Loss: 0.0020\nGMM - Epoch [8/15], Step [1500/2912], Loss: 0.0125\nGMM - Epoch [8/15], Step [1600/2912], Loss: 0.0042\nGMM - Epoch [8/15], Step [1700/2912], Loss: 0.0312\nGMM - Epoch [8/15], Step [1800/2912], Loss: 0.0057\nGMM - Epoch [8/15], Step [1900/2912], Loss: 0.0022\nGMM - Epoch [8/15], Step [2000/2912], Loss: 0.1065\nGMM - Epoch [8/15], Step [2100/2912], Loss: 0.0043\nGMM - Epoch [8/15], Step [2200/2912], Loss: 0.0045\nGMM - Epoch [8/15], Step [2300/2912], Loss: 0.0035\nGMM - Epoch [8/15], Step [2400/2912], Loss: 0.0381\nGMM - Epoch [8/15], Step [2500/2912], Loss: 0.0033\nGMM - Epoch [8/15], Step [2600/2912], Loss: 0.0083\nGMM - Epoch [8/15], Step [2700/2912], Loss: 0.0021\nGMM - Epoch [8/15], Step [2800/2912], Loss: 0.0083\nGMM - Epoch [8/15], Step [2900/2912], Loss: 0.0050\nGMM - Epoch [9/15], Step [100/2912], Loss: 0.0087\nGMM - Epoch [9/15], Step [200/2912], Loss: 0.0051\nGMM - Epoch [9/15], Step [300/2912], Loss: 0.0039\nGMM - Epoch [9/15], Step [400/2912], Loss: 0.0029\nGMM - Epoch [9/15], Step [500/2912], Loss: 0.0024\nGMM - Epoch [9/15], Step [600/2912], Loss: 0.0032\nGMM - Epoch [9/15], Step [700/2912], Loss: 0.0031\nGMM - Epoch [9/15], Step [800/2912], Loss: 0.0138\nGMM - Epoch [9/15], Step [900/2912], Loss: 0.0024\nGMM - Epoch [9/15], Step [1000/2912], Loss: 0.0039\nGMM - Epoch [9/15], Step [1100/2912], Loss: 0.0069\nGMM - Epoch [9/15], Step [1200/2912], Loss: 0.0026\nGMM - Epoch [9/15], Step [1300/2912], Loss: 0.0025\nGMM - Epoch [9/15], Step [1400/2912], Loss: 0.0066\nGMM - Epoch [9/15], Step [1500/2912], Loss: 0.0023\nGMM - Epoch [9/15], Step [1600/2912], Loss: 0.0035\nGMM - Epoch [9/15], Step [1700/2912], Loss: 0.0032\nGMM - Epoch [9/15], Step [1800/2912], Loss: 0.0083\nGMM - Epoch [9/15], Step [1900/2912], Loss: 0.0034\nGMM - Epoch [9/15], Step [2000/2912], Loss: 0.0308\nGMM - Epoch [9/15], Step [2100/2912], Loss: 0.0036\nGMM - Epoch [9/15], Step [2200/2912], Loss: 0.0035\nGMM - Epoch [9/15], Step [2300/2912], Loss: 0.0076\nGMM - Epoch [9/15], Step [2400/2912], Loss: 0.0029\nGMM - Epoch [9/15], Step [2500/2912], Loss: 0.0077\nGMM - Epoch [9/15], Step [2600/2912], Loss: 0.0217\nGMM - Epoch [9/15], Step [2700/2912], Loss: 0.0022\nGMM - Epoch [9/15], Step [2800/2912], Loss: 0.0023\nGMM - Epoch [9/15], Step [2900/2912], Loss: 0.0170\nGMM - Epoch [10/15], Step [100/2912], Loss: 0.0106\nGMM - Epoch [10/15], Step [200/2912], Loss: 0.0047\nGMM - Epoch [10/15], Step [300/2912], Loss: 0.0031\nGMM - Epoch [10/15], Step [400/2912], Loss: 0.0195\nGMM - Epoch [10/15], Step [500/2912], Loss: 0.0034\nGMM - Epoch [10/15], Step [600/2912], Loss: 0.0094\nGMM - Epoch [10/15], Step [700/2912], Loss: 0.0071\nGMM - Epoch [10/15], Step [800/2912], Loss: 0.0036\nGMM - Epoch [10/15], Step [900/2912], Loss: 0.0036\nGMM - Epoch [10/15], Step [1000/2912], Loss: 0.0021\nGMM - Epoch [10/15], Step [1100/2912], Loss: 0.0122\nGMM - Epoch [10/15], Step [1200/2912], Loss: 0.0051\nGMM - Epoch [10/15], Step [1300/2912], Loss: 0.0048\nGMM - Epoch [10/15], Step [1400/2912], Loss: 0.0129\nGMM - Epoch [10/15], Step [1500/2912], Loss: 0.0051\nGMM - Epoch [10/15], Step [1600/2912], Loss: 0.0029\nGMM - Epoch [10/15], Step [1700/2912], Loss: 0.0041\nGMM - Epoch [10/15], Step [1800/2912], Loss: 0.0034\nGMM - Epoch [10/15], Step [1900/2912], Loss: 0.0039\nGMM - Epoch [10/15], Step [2000/2912], Loss: 0.0030\nGMM - Epoch [10/15], Step [2100/2912], Loss: 0.0022\nGMM - Epoch [10/15], Step [2200/2912], Loss: 0.0065\nGMM - Epoch [10/15], Step [2300/2912], Loss: 0.0211\nGMM - Epoch [10/15], Step [2400/2912], Loss: 0.0026\nGMM - Epoch [10/15], Step [2500/2912], Loss: 0.0057\nGMM - Epoch [10/15], Step [2600/2912], Loss: 0.0365\nGMM - Epoch [10/15], Step [2700/2912], Loss: 0.0052\nGMM - Epoch [10/15], Step [2800/2912], Loss: 0.0041\nGMM - Epoch [10/15], Step [2900/2912], Loss: 0.0019\nGMM - Epoch [11/15], Step [100/2912], Loss: 0.0029\nGMM - Epoch [11/15], Step [200/2912], Loss: 0.0110\nGMM - Epoch [11/15], Step [300/2912], Loss: 0.0044\nGMM - Epoch [11/15], Step [400/2912], Loss: 0.0071\nGMM - Epoch [11/15], Step [500/2912], Loss: 0.0026\nGMM - Epoch [11/15], Step [600/2912], Loss: 0.0058\nGMM - Epoch [11/15], Step [700/2912], Loss: 0.0041\nGMM - Epoch [11/15], Step [800/2912], Loss: 0.0028\nGMM - Epoch [11/15], Step [900/2912], Loss: 0.0045\nGMM - Epoch [11/15], Step [1000/2912], Loss: 0.0042\nGMM - Epoch [11/15], Step [1100/2912], Loss: 0.0017\nGMM - Epoch [11/15], Step [1200/2912], Loss: 0.0107\nGMM - Epoch [11/15], Step [1300/2912], Loss: 0.0065\nGMM - Epoch [11/15], Step [1400/2912], Loss: 0.0054\nGMM - Epoch [11/15], Step [1500/2912], Loss: 0.0054\nGMM - Epoch [11/15], Step [1600/2912], Loss: 0.0047\nGMM - Epoch [11/15], Step [1700/2912], Loss: 0.0029\nGMM - Epoch [11/15], Step [1800/2912], Loss: 0.0081\nGMM - Epoch [11/15], Step [1900/2912], Loss: 0.0058\nGMM - Epoch [11/15], Step [2000/2912], Loss: 0.0025\nGMM - Epoch [11/15], Step [2100/2912], Loss: 0.0072\nGMM - Epoch [11/15], Step [2200/2912], Loss: 0.0043\nGMM - Epoch [11/15], Step [2300/2912], Loss: 0.0040\nGMM - Epoch [11/15], Step [2400/2912], Loss: 0.0033\nGMM - Epoch [11/15], Step [2500/2912], Loss: 0.0048\nGMM - Epoch [11/15], Step [2600/2912], Loss: 0.0034\nGMM - Epoch [11/15], Step [2700/2912], Loss: 0.0095\nGMM - Epoch [11/15], Step [2800/2912], Loss: 0.0024\nGMM - Epoch [11/15], Step [2900/2912], Loss: 0.0073\nGMM - Epoch [12/15], Step [100/2912], Loss: 0.0034\nGMM - Epoch [12/15], Step [200/2912], Loss: 0.0026\nGMM - Epoch [12/15], Step [300/2912], Loss: 0.0085\nGMM - Epoch [12/15], Step [400/2912], Loss: 0.0029\nGMM - Epoch [12/15], Step [500/2912], Loss: 0.0030\nGMM - Epoch [12/15], Step [600/2912], Loss: 0.0189\nGMM - Epoch [12/15], Step [700/2912], Loss: 0.0031\nGMM - Epoch [12/15], Step [800/2912], Loss: 0.0038\nGMM - Epoch [12/15], Step [900/2912], Loss: 0.0028\nGMM - Epoch [12/15], Step [1000/2912], Loss: 0.0034\nGMM - Epoch [12/15], Step [1100/2912], Loss: 0.0021\nGMM - Epoch [12/15], Step [1200/2912], Loss: 0.0033\nGMM - Epoch [12/15], Step [1300/2912], Loss: 0.0064\nGMM - Epoch [12/15], Step [1400/2912], Loss: 0.0026\nGMM - Epoch [12/15], Step [1500/2912], Loss: 0.0046\nGMM - Epoch [12/15], Step [1600/2912], Loss: 0.0072\nGMM - Epoch [12/15], Step [1700/2912], Loss: 0.0031\nGMM - Epoch [12/15], Step [1800/2912], Loss: 0.0025\nGMM - Epoch [12/15], Step [1900/2912], Loss: 0.0037\nGMM - Epoch [12/15], Step [2000/2912], Loss: 0.0027\nGMM - Epoch [12/15], Step [2100/2912], Loss: 0.0218\nGMM - Epoch [12/15], Step [2200/2912], Loss: 0.0039\nGMM - Epoch [12/15], Step [2300/2912], Loss: 0.0050\nGMM - Epoch [12/15], Step [2400/2912], Loss: 0.0029\nGMM - Epoch [12/15], Step [2500/2912], Loss: 0.0026\nGMM - Epoch [12/15], Step [2600/2912], Loss: 0.0411\nGMM - Epoch [12/15], Step [2700/2912], Loss: 0.0060\nGMM - Epoch [12/15], Step [2800/2912], Loss: 0.0026\nGMM - Epoch [12/15], Step [2900/2912], Loss: 0.0040\nGMM - Epoch [13/15], Step [100/2912], Loss: 0.0021\nGMM - Epoch [13/15], Step [200/2912], Loss: 0.0057\nGMM - Epoch [13/15], Step [300/2912], Loss: 0.0054\nGMM - Epoch [13/15], Step [400/2912], Loss: 0.0022\nGMM - Epoch [13/15], Step [500/2912], Loss: 0.0064\nGMM - Epoch [13/15], Step [600/2912], Loss: 0.0172\nGMM - Epoch [13/15], Step [700/2912], Loss: 0.0029\nGMM - Epoch [13/15], Step [800/2912], Loss: 0.0039\nGMM - Epoch [13/15], Step [900/2912], Loss: 0.0032\nGMM - Epoch [13/15], Step [1000/2912], Loss: 0.0065\nGMM - Epoch [13/15], Step [1100/2912], Loss: 0.0027\nGMM - Epoch [13/15], Step [1200/2912], Loss: 0.0018\nGMM - Epoch [13/15], Step [1300/2912], Loss: 0.0031\nGMM - Epoch [13/15], Step [1400/2912], Loss: 0.0111\nGMM - Epoch [13/15], Step [1500/2912], Loss: 0.0034\nGMM - Epoch [13/15], Step [1600/2912], Loss: 0.0021\nGMM - Epoch [13/15], Step [1700/2912], Loss: 0.0033\nGMM - Epoch [13/15], Step [1800/2912], Loss: 0.0087\nGMM - Epoch [13/15], Step [1900/2912], Loss: 0.0043\nGMM - Epoch [13/15], Step [2000/2912], Loss: 0.0150\nGMM - Epoch [13/15], Step [2100/2912], Loss: 0.0035\nGMM - Epoch [13/15], Step [2200/2912], Loss: 0.0374\nGMM - Epoch [13/15], Step [2300/2912], Loss: 0.0033\nGMM - Epoch [13/15], Step [2400/2912], Loss: 0.0052\nGMM - Epoch [13/15], Step [2500/2912], Loss: 0.0150\nGMM - Epoch [13/15], Step [2600/2912], Loss: 0.0028\nGMM - Epoch [13/15], Step [2700/2912], Loss: 0.0074\nGMM - Epoch [13/15], Step [2800/2912], Loss: 0.0108\nGMM - Epoch [13/15], Step [2900/2912], Loss: 0.0034\nGMM - Epoch [14/15], Step [100/2912], Loss: 0.0034\nGMM - Epoch [14/15], Step [200/2912], Loss: 0.0028\nGMM - Epoch [14/15], Step [300/2912], Loss: 0.0048\nGMM - Epoch [14/15], Step [400/2912], Loss: 0.0218\nGMM - Epoch [14/15], Step [500/2912], Loss: 0.0022\nGMM - Epoch [14/15], Step [600/2912], Loss: 0.0183\nGMM - Epoch [14/15], Step [700/2912], Loss: 0.0179\nGMM - Epoch [14/15], Step [800/2912], Loss: 0.0028\nGMM - Epoch [14/15], Step [900/2912], Loss: 0.0091\nGMM - Epoch [14/15], Step [1000/2912], Loss: 0.0018\nGMM - Epoch [14/15], Step [1100/2912], Loss: 0.0021\nGMM - Epoch [14/15], Step [1200/2912], Loss: 0.0077\nGMM - Epoch [14/15], Step [1300/2912], Loss: 0.0103\nGMM - Epoch [14/15], Step [1400/2912], Loss: 0.0054\nGMM - Epoch [14/15], Step [1500/2912], Loss: 0.0061\nGMM - Epoch [14/15], Step [1600/2912], Loss: 0.0078\nGMM - Epoch [14/15], Step [1700/2912], Loss: 0.0030\nGMM - Epoch [14/15], Step [1800/2912], Loss: 0.0380\nGMM - Epoch [14/15], Step [1900/2912], Loss: 0.0044\nGMM - Epoch [14/15], Step [2000/2912], Loss: 0.0035\nGMM - Epoch [14/15], Step [2100/2912], Loss: 0.0018\nGMM - Epoch [14/15], Step [2200/2912], Loss: 0.0026\nGMM - Epoch [14/15], Step [2300/2912], Loss: 0.0484\nGMM - Epoch [14/15], Step [2400/2912], Loss: 0.0023\nGMM - Epoch [14/15], Step [2500/2912], Loss: 0.0199\nGMM - Epoch [14/15], Step [2600/2912], Loss: 0.0051\nGMM - Epoch [14/15], Step [2700/2912], Loss: 0.0075\nGMM - Epoch [14/15], Step [2800/2912], Loss: 0.0132\nGMM - Epoch [14/15], Step [2900/2912], Loss: 0.0022\nGMM - Epoch [15/15], Step [100/2912], Loss: 0.0027\nGMM - Epoch [15/15], Step [200/2912], Loss: 0.0028\nGMM - Epoch [15/15], Step [300/2912], Loss: 0.0060\nGMM - Epoch [15/15], Step [400/2912], Loss: 0.0037\nGMM - Epoch [15/15], Step [500/2912], Loss: 0.0025\nGMM - Epoch [15/15], Step [600/2912], Loss: 0.0316\nGMM - Epoch [15/15], Step [700/2912], Loss: 0.0046\nGMM - Epoch [15/15], Step [800/2912], Loss: 0.0063\nGMM - Epoch [15/15], Step [900/2912], Loss: 0.0028\nGMM - Epoch [15/15], Step [1000/2912], Loss: 0.0025\nGMM - Epoch [15/15], Step [1100/2912], Loss: 0.0129\nGMM - Epoch [15/15], Step [1200/2912], Loss: 0.0055\nGMM - Epoch [15/15], Step [1300/2912], Loss: 0.0024\nGMM - Epoch [15/15], Step [1400/2912], Loss: 0.0085\nGMM - Epoch [15/15], Step [1500/2912], Loss: 0.0098\nGMM - Epoch [15/15], Step [1600/2912], Loss: 0.0040\nGMM - Epoch [15/15], Step [1700/2912], Loss: 0.0031\nGMM - Epoch [15/15], Step [1800/2912], Loss: 0.0037\nGMM - Epoch [15/15], Step [1900/2912], Loss: 0.0029\nGMM - Epoch [15/15], Step [2000/2912], Loss: 0.0045\nGMM - Epoch [15/15], Step [2100/2912], Loss: 0.0039\nGMM - Epoch [15/15], Step [2200/2912], Loss: 0.0032\nGMM - Epoch [15/15], Step [2300/2912], Loss: 0.0056\nGMM - Epoch [15/15], Step [2400/2912], Loss: 0.0025\nGMM - Epoch [15/15], Step [2500/2912], Loss: 0.0037\nGMM - Epoch [15/15], Step [2600/2912], Loss: 0.0027\nGMM - Epoch [15/15], Step [2700/2912], Loss: 0.0020\nGMM - Epoch [15/15], Step [2800/2912], Loss: 0.0023\nGMM - Epoch [15/15], Step [2900/2912], Loss: 0.0082\nGMM training complete. Starting TOM training...\nTOM - Epoch [1/5], Step [100/2912], L1 Loss: 0.1268\nTOM - Epoch [1/5], Step [200/2912], L1 Loss: 0.0876\nTOM - Epoch [1/5], Step [300/2912], L1 Loss: 0.0770\nTOM - Epoch [1/5], Step [400/2912], L1 Loss: 0.0585\nTOM - Epoch [1/5], Step [500/2912], L1 Loss: 0.0616\nTOM - Epoch [1/5], Step [600/2912], L1 Loss: 0.0601\nTOM - Epoch [1/5], Step [700/2912], L1 Loss: 0.0555\nTOM - Epoch [1/5], Step [800/2912], L1 Loss: 0.0539\nTOM - Epoch [1/5], Step [900/2912], L1 Loss: 0.0430\nTOM - Epoch [1/5], Step [1000/2912], L1 Loss: 0.0385\nTOM - Epoch [1/5], Step [1100/2912], L1 Loss: 0.0439\nTOM - Epoch [1/5], Step [1200/2912], L1 Loss: 0.0312\nTOM - Epoch [1/5], Step [1300/2912], L1 Loss: 0.0469\nTOM - Epoch [1/5], Step [1400/2912], L1 Loss: 0.0380\nTOM - Epoch [1/5], Step [1500/2912], L1 Loss: 0.0538\nTOM - Epoch [1/5], Step [1600/2912], L1 Loss: 0.0321\n^C\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"!python main.py \\\n  --mode test \\\n  --dataroot \"/kaggle/input/clothe/clothes_tryon_dataset\" \\\n  --name \"Results_Epoch_5\" \\\n  --gpu_ids 0 \\\n  --person_rep_channels 7 \\\n  --result_dir \"/kaggle/working/visualization_results\" \\\n  --gmm_checkpoint \"/kaggle/working/finetuned_checkpoints_multi_gpu/FineTuned-MultiGPU/gmm_epoch_5.pth\" \\\n  --tom_checkpoint \"/kaggle/working/finetuned_checkpoints_multi_gpu/FineTuned-MultiGPU/tom_epoch_5.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:31:48.428947Z","iopub.execute_input":"2025-07-20T07:31:48.429243Z","iopub.status.idle":"2025-07-20T07:31:51.769487Z","shell.execute_reply.started":"2025-07-20T07:31:48.429223Z","shell.execute_reply":"2025-07-20T07:31:51.768770Z"}},"outputs":[{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/kaggle/working/main.py\", line 123, in <module>\n    if opt.gmm_checkpoint:\n       ^^^\nNameError: name 'opt' is not defined. Did you mean: 'oct'?\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Cell 1: --- CONFIGURATION FOR VISUALIZATION ---\n# Run this cell first.\n\nimport argparse\n\n# We use argparse.Namespace to create a simple object that holds all our settings.\nopt = argparse.Namespace(\n    \n    # --- Mode and Paths ---\n    # Set the mode to 'test' to generate images\n    mode=\"test\", \n    \n    # Path to the dataset to get test images from\n    dataroot=\"/kaggle/input/clothe/clothes_tryon_dataset\", \n    \n    # Give a unique name for the output folder\n    name=\"Visualization_From_FineTuned_Epoch_5\", \n    \n    # Where to save the final images\n    result_dir=\"/kaggle/working/my_final_results2\", \n    \n    # --- Checkpoints to Load for Visualization ---\n    # Path to YOUR new GMM weight file\n    gmm_checkpoint=\"/kaggle/working/finetuned_checkpoints_multi_gpu/FineTuned-MultiGPU/gmm_epoch_15.pth\", \n    \n    # Path to YOUR new TOM weight file\n    tom_checkpoint=\"/kaggle/working/finetuned_checkpoints_multi_gpu/FineTuned-MultiGPU/tom_epoch_5.pth\",\n\n    # --- Hardware and Model Shape ---\n    gpu_ids=\"0\", # Use a single GPU for testing\n    person_rep_channels=7,\n    cloth_channels=3,\n    agnostic_channels=3,\n    output_channels=3,\n\n    # --- Data Loading Params (can be left as is for testing) ---\n    batch_size_test=1,\n    load_height=1024,\n    load_width=768\n)\n\nprint(\"✅ Configuration loaded successfully!\")\nprint(f\"   Mode set to: {opt.mode}\")\nprint(f\"   Loading GMM from: {opt.gmm_checkpoint}\")\nprint(f\"   Loading TOM from: {opt.tom_checkpoint}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:01:05.977392Z","iopub.execute_input":"2025-07-20T13:01:05.977684Z","iopub.status.idle":"2025-07-20T13:01:05.984074Z","shell.execute_reply.started":"2025-07-20T13:01:05.977632Z","shell.execute_reply":"2025-07-20T13:01:05.983275Z"}},"outputs":[{"name":"stdout","text":"✅ Configuration loaded successfully!\n   Mode set to: test\n   Loading GMM from: /kaggle/working/finetuned_checkpoints_multi_gpu/FineTuned-MultiGPU/gmm_epoch_15.pth\n   Loading TOM from: /kaggle/working/finetuned_checkpoints_multi_gpu/FineTuned-MultiGPU/tom_epoch_5.pth\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Cell 2: --- VISUALIZATION EXECUTION CELL ---\n# Run this cell AFTER running the configuration cell above.\n\n# 1. All necessary imports\nimport os\nimport torch\nimport torch.nn as nn\nfrom dataset import VitonHDDataset\nfrom models import GMM, TOM\nfrom test import test # We only need the 'test' function\n\n# 2. Helper function to clean model keys from multi-GPU training\ndef clean_dataparallel_keys(state_dict):\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        if k.startswith('module.'):\n            name = k[7:]  # remove 'module.'\n            new_state_dict[name] = v\n        else:\n            new_state_dict[k] = v\n    return new_state_dict\n\n# --- Main Logic Starts Here ---\n# The 'opt' object is used directly from the cell above.\n\n# 3. Setup GPU and create the results directory\nos.makedirs(os.path.join(opt.result_dir, opt.name), exist_ok=True)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# 4. Load the test dataset\n# We set is_train=False because opt.mode is 'test'\ntest_dataset = VitonHDDataset(opt, is_train=False) \nprint(f\"Test dataset loaded. Found {len(test_dataset)} samples.\")\n\n# 5. Initialize the models\ngmm_model = GMM(opt).to(device)\ntom_model = TOM(opt).to(device)\n\n# 6. Load your fine-tuned weights\nprint(\"Loading fine-tuned weights...\")\nif opt.gmm_checkpoint:\n    state_dict = torch.load(opt.gmm_checkpoint, map_location=device)\n    state_dict = clean_dataparallel_keys(state_dict)\n    gmm_model.load_state_dict(state_dict)\n    print(\"   ✅ GMM weights loaded.\")\n    \nif opt.tom_checkpoint:\n    state_dict = torch.load(opt.tom_checkpoint, map_location=device)\n    state_dict = clean_dataparallel_keys(state_dict)\n    tom_model.load_state_dict(state_dict)\n    print(\"   ✅ TOM weights loaded.\")\n\n# 7. Run the visualization function\nprint(\"\\nStarting visualization...\")\ntest(opt, gmm_model, tom_model, test_dataset)\n\nprint(f\"\\n✅ Visualization complete!\")\nprint(f\"Check for your results in the directory: {os.path.join(opt.result_dir, opt.name)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:01:17.309901Z","iopub.execute_input":"2025-07-20T13:01:17.310445Z","iopub.status.idle":"2025-07-20T13:02:33.213478Z","shell.execute_reply.started":"2025-07-20T13:01:17.310421Z","shell.execute_reply":"2025-07-20T13:02:33.212352Z"}},"outputs":[{"name":"stdout","text":"Test dataset loaded. Found 2032 samples.\nLoading fine-tuned weights...\n   ✅ GMM weights loaded.\n   ✅ TOM weights loaded.\n\nStarting visualization...\nProcessed and saved results for batch 1/2032\nProcessed and saved results for batch 2/2032\nProcessed and saved results for batch 3/2032\nProcessed and saved results for batch 4/2032\nProcessed and saved results for batch 5/2032\nProcessed and saved results for batch 6/2032\nProcessed and saved results for batch 7/2032\nProcessed and saved results for batch 8/2032\nProcessed and saved results for batch 9/2032\nProcessed and saved results for batch 10/2032\nProcessed and saved results for batch 11/2032\nProcessed and saved results for batch 12/2032\nProcessed and saved results for batch 13/2032\nProcessed and saved results for batch 14/2032\nProcessed and saved results for batch 15/2032\nProcessed and saved results for batch 16/2032\nProcessed and saved results for batch 17/2032\nProcessed and saved results for batch 18/2032\nProcessed and saved results for batch 19/2032\nProcessed and saved results for batch 20/2032\nProcessed and saved results for batch 21/2032\nProcessed and saved results for batch 22/2032\nProcessed and saved results for batch 23/2032\nProcessed and saved results for batch 24/2032\nProcessed and saved results for batch 25/2032\nProcessed and saved results for batch 26/2032\nProcessed and saved results for batch 27/2032\nProcessed and saved results for batch 28/2032\nProcessed and saved results for batch 29/2032\nProcessed and saved results for batch 30/2032\nProcessed and saved results for batch 31/2032\nProcessed and saved results for batch 32/2032\nProcessed and saved results for batch 33/2032\nProcessed and saved results for batch 34/2032\nProcessed and saved results for batch 35/2032\nProcessed and saved results for batch 36/2032\nProcessed and saved results for batch 37/2032\nProcessed and saved results for batch 38/2032\nProcessed and saved results for batch 39/2032\nProcessed and saved results for batch 40/2032\nProcessed and saved results for batch 41/2032\nProcessed and saved results for batch 42/2032\nProcessed and saved results for batch 43/2032\nProcessed and saved results for batch 44/2032\nProcessed and saved results for batch 45/2032\nProcessed and saved results for batch 46/2032\nProcessed and saved results for batch 47/2032\nProcessed and saved results for batch 48/2032\nProcessed and saved results for batch 49/2032\nProcessed and saved results for batch 50/2032\nProcessed and saved results for batch 51/2032\nProcessed and saved results for batch 52/2032\nProcessed and saved results for batch 53/2032\nProcessed and saved results for batch 54/2032\nProcessed and saved results for batch 55/2032\nProcessed and saved results for batch 56/2032\nProcessed and saved results for batch 57/2032\nProcessed and saved results for batch 58/2032\nProcessed and saved results for batch 59/2032\nProcessed and saved results for batch 60/2032\nProcessed and saved results for batch 61/2032\nProcessed and saved results for batch 62/2032\nProcessed and saved results for batch 63/2032\nProcessed and saved results for batch 64/2032\nProcessed and saved results for batch 65/2032\nProcessed and saved results for batch 66/2032\nProcessed and saved results for batch 67/2032\nProcessed and saved results for batch 68/2032\nProcessed and saved results for batch 69/2032\nProcessed and saved results for batch 70/2032\nProcessed and saved results for batch 71/2032\nProcessed and saved results for batch 72/2032\nProcessed and saved results for batch 73/2032\nProcessed and saved results for batch 74/2032\nProcessed and saved results for batch 75/2032\nProcessed and saved results for batch 76/2032\nProcessed and saved results for batch 77/2032\nProcessed and saved results for batch 78/2032\nProcessed and saved results for batch 79/2032\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/185954676.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# 7. Run the visualization function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting visualization...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtom_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n✅ Visualization complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/test.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(opt, gmm_model, tom_model, dataset)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{p_name}_tries_{c_name}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisuals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processed and saved results for batch {i+1}/{len(dataloader)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2582\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopen_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1490\u001b[0m         )\n\u001b[1;32m   1491\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msingle_im\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m         ImageFile._save(\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0msingle_im\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0m_encode_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0m_encode_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"flush\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    671\u001b[0m                     \u001b[0;31m# compress to Python file-compatible object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m                         \u001b[0merrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m                         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":49},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}