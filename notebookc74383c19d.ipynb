{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6683799,"sourceType":"datasetVersion","datasetId":3855472}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os.path as osp\nimport numpy as np\n\nclass CustomDataset(data.Dataset):\n\n    def __init__(self, opt):\n        super(CustomDataset, self).__init__()\n        self.opt = opt\n        self.dataroot = opt.dataroot\n        self.datamode = opt.datamode\n        self.data_list = opt.data_list\n        self.fine_height = opt.fine_height\n        self.fine_width = opt.fine_width\n        self.data_path = osp.join(self.dataroot, self.datamode)\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n\n        # Load data list\n        im_names = []\n        c_names = []\n        with open(osp.join(self.dataroot, self.data_list), 'r') as f:\n            for line in f.readlines():\n                im_name, c_name = line.strip().split()\n                im_names.append(im_name)\n                c_names.append(c_name)\n\n        self.im_names = im_names\n        self.c_names = c_names\n\n    def __getitem__(self, index):\n        c_name = self.c_names[index]\n        im_name = self.im_names[index]\n\n        # Clothing image\n        cloth_path = osp.join(self.data_path, 'cloth', c_name)\n        c = Image.open(cloth_path).convert('RGB')\n        c = self.transform(c)\n\n        # Clothing mask\n        cloth_mask_path = osp.join(self.data_path, 'cloth-mask', c_name)\n        cm = Image.open(cloth_mask_path)\n        cm_array = np.array(cm)\n        cm_array = (cm_array >= 128).astype(np.float32)\n        cm = torch.from_numpy(cm_array)\n        cm.unsqueeze_(0)\n\n        # Person image (for visualization and ground truth)\n        image_path = osp.join(self.data_path, 'image', im_name)\n        im = Image.open(image_path).convert('RGB')\n        im = self.transform(im)\n\n        # Person's parse-agnostic representation\n        parse_agnostic_path = osp.join(self.data_path, 'image-parse-agnostic-v3.2', im_name.replace('.jpg', '.png'))\n        parse_agnostic = Image.open(parse_agnostic_path).convert('L')\n        parse_agnostic = transforms.ToTensor()(parse_agnostic)\n\n        # Densepose\n        densepose_path = osp.join(self.data_path, 'image-densepose', im_name)\n        densepose = Image.open(densepose_path).convert('RGB')\n        densepose = self.transform(densepose)\n\n        # OpenPose image (rendered keypoints)\n        openpose_path = osp.join(self.data_path, 'openpose_img', im_name.replace('.jpg', '_rendered.png'))\n        pose_map = Image.open(openpose_path).convert('RGB')\n        pose_map = self.transform(pose_map)\n\n        result = {\n            # Inputs for the Condition Generator (TOCG)\n            'c_name':         c_name,\n            'im_name':        im_name,\n            'cloth':          c,\n            'cloth_mask':     cm,\n            'parse_agnostic': parse_agnostic,\n            'densepose':      densepose,\n            'pose':           pose_map,\n            # Ground truth image for the final Generator\n            'image':          im,\n        }\n\n        return result\n\n    def __len__(self):\n        return len(self.im_names)\n\n\nclass CPDataLoader:\n\n    def __init__(self, opt, dataset):\n        super(CPDataLoader, self).__init__()\n\n        is_shuffle = opt.shuffle\n\n        self.data_loader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=opt.batch_size,\n            shuffle=is_shuffle,\n            num_workers=opt.workers,\n            pin_memory=True,\n            drop_last=True\n        )\n        self.dataset = dataset\n        self.data_iter = self.data_loader.__iter__()\n\n    def next_batch(self):\n        try:\n            batch = self.data_iter.__next__()\n        except StopIteration:\n            self.data_iter = self.data_loader.__iter__()\n            batch = self.data_iter.__next__()\n\n        return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T17:25:35.853760Z","iopub.execute_input":"2025-07-16T17:25:35.853934Z","iopub.status.idle":"2025-07-16T17:25:48.002952Z","shell.execute_reply.started":"2025-07-16T17:25:35.853918Z","shell.execute_reply":"2025-07-16T17:25:48.002390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import argparse\nimport torch\n\n\nYOUR_KAGGLE_DATASET_NAME = 'clothes_tryon_dataset'\n\n\nopt = argparse.Namespace()\n\nopt.dataroot = f'/kaggle/input/clothes_tryon_dataset'\nopt.datamode = 'train' \nopt.data_list = 'train_pairs.txt'\n\n# --- Image & Batch Settings ---\nopt.fine_height = 1024\nopt.fine_width = 768\nopt.batch_size = 4  \nopt.workers = 2     \n\n\nopt.shuffle = False \n\n\nprint(\"--- Test Script Initialized ---\")\nprint(f\"Dataroot set to: {opt.dataroot}\")\nprint(f\"Loading data list: {opt.data_list}\")\n\n\ntry:\n    # --- Initialize the Dataset ---\n    print(\"Initializing CustomDataset...\")\n    # This assumes CustomDataset and CPDataLoader are defined in the previous cell\n    train_dataset = CustomDataset(opt)\n    print(f\"Dataset initialized successfully. Found {len(train_dataset)} image pairs.\")\n\n    # --- Initialize the DataLoader ---\n    print(\"\\nInitializing CPDataLoader...\")\n    train_loader = CPDataLoader(opt, train_dataset)\n    print(\"Dataloader initialized successfully.\")\n\n    # --- Fetch One Batch ---\n    print(\"\\nAttempting to load one batch of data...\")\n    first_batch = train_loader.next_batch()\n    print(\"Successfully loaded one batch!\")\n\n    # --- Inspect the Batch Content ---\n    print(\"\\n--- Batch Content Verification ---\")\n    for key, value in first_batch.items():\n        if isinstance(value, torch.Tensor):\n            print(f\"  - Key: '{key}', Shape: {value.shape}, DType: {value.dtype}\")\n        else:\n            # This will print the list of image/cloth names\n            print(f\"  - Key: '{key}', Type: {type(value)}, Length: {len(value)}\")\n    print(\"-\" * 30)\n    print(\"\\n DATA LOADING TEST PASSED! \")\n    print(\"You are now ready to define the network architectures.\")\n\nexcept FileNotFoundError as e:\n    print(\"\\n--- ❌ DATA LOADING TEST FAILED: File Not Found ❌ ---\")\n    print(f\"ERROR: {e}\")\n    print(\"\\nPlease double-check the following:\")\n    print(f\"1. Is YOUR_KAGGLE_DATASET_NAME set correctly to '{YOUR_KAGGLE_DATASET_NAME}'?\")\n    print(\"2. Does your Kaggle dataset have the exact folder structure we discussed?\")\n    print(\"   (e.g., /train/cloth, /train/image, etc.)\")\n\nexcept Exception as e:\n    print(f\"\\n---  AN UNEXPECTED ERROR OCCURRED  ---\")\n    print(f\"ERROR: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T17:28:29.145845Z","iopub.execute_input":"2025-07-16T17:28:29.146126Z","iopub.status.idle":"2025-07-16T17:28:30.920889Z","shell.execute_reply.started":"2025-07-16T17:28:29.146107Z","shell.execute_reply":"2025-07-16T17:28:30.919699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport re\nimport functools\nimport numpy as np\nfrom torch.nn.utils import spectral_norm\n\n#==============================================================================\n# Helper Functions and Classes\n#==============================================================================\n\nclass UnetSkipConnectionBlock(nn.Module):\n\n    def __init__(self, outer_nc, inner_nc, input_nc=None,\n                 submodule=None, outermost=False, innermost=False,\n                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n        if input_nc is None:\n            input_nc = outer_nc\n        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n                             stride=2, padding=1, bias=use_bias)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downconv]\n            up = [uprelu, upconv, nn.Tanh()]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([x, self.model(x)], 1)\n\n\n\nclass UnetGenerator(nn.Module):\n    \"\"\"\n    Defines the U-Net generator architecture.\n    This is used for the Try-On Condition Generator (TOCG).\n    \"\"\"\n    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetGenerator, self).__init__()\n\n        # construct unet structure\n        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n        for i in range(num_downs - 5):\n            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n\n    def forward(self, input):\n        return self.model(input)\n\n\nclass NLayerDiscriminator(nn.Module):\n\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\n\n\nclass SPADE(nn.Module):\n    def __init__(self, config_text, norm_nc, label_nc):\n        super().__init__()\n        parsed = re.search(r'spade(\\d+)x(\\d+)_norm_(\\w+)', config_text)\n        param_free_norm_type = parsed.group(3)\n        ks = int(parsed.group(1))\n        pw = ks // 2\n        self.param_free_norm = nn.InstanceNorm2d(norm_nc, affine=False)\n\n        nhidden = 128\n        self.mlp_shared = nn.Sequential(\n            nn.Conv2d(label_nc, nhidden, kernel_size=ks, padding=pw),\n            nn.ReLU()\n        )\n        self.mlp_gamma = nn.Conv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n        self.mlp_beta = nn.Conv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n\n    def forward(self, x, segmap):\n        normalized = self.param_free_norm(x)\n        segmap = F.interpolate(segmap, size=x.size()[2:], mode='nearest')\n        actv = self.mlp_shared(segmap)\n        gamma = self.mlp_gamma(actv)\n        beta = self.mlp_beta(actv)\n        out = normalized * (1 + gamma) + beta\n        return out\n\nclass SPADEResnetBlock(nn.Module):\n    def __init__(self, fin, fout, opt):\n        super().__init__()\n        self.learned_shortcut = (fin != fout)\n        fmiddle = min(fin, fout)\n        self.conv_0 = nn.Conv2d(fin, fmiddle, kernel_size=3, padding=1)\n        self.conv_1 = nn.Conv2d(fmiddle, fout, kernel_size=3, padding=1)\n        if self.learned_shortcut:\n            self.conv_s = nn.Conv2d(fin, fout, kernel_size=1, bias=False)\n\n        spade_config_str = 'spade3x3_norm_in'\n        self.norm_0 = SPADE(spade_config_str, fin, opt.semantic_nc)\n        self.norm_1 = SPADE(spade_config_str, fmiddle, opt.semantic_nc)\n        if self.learned_shortcut:\n            self.norm_s = SPADE(spade_config_str, fin, opt.semantic_nc)\n\n    def forward(self, x, seg):\n        x_s = self.shortcut(x, seg)\n        dx = self.conv_0(self.actvn(self.norm_0(x, seg)))\n        dx = self.conv_1(self.actvn(self.norm_1(dx, seg)))\n        out = x_s + dx\n        return out\n\n    def shortcut(self, x, seg):\n        if self.learned_shortcut:\n            x_s = self.conv_s(self.norm_s(x, seg))\n        else:\n            x_s = x\n        return x_s\n\n    def actvn(self, x):\n        return F.leaky_relu(x, 2e-1)\n\n\nclass SPADEGenerator(nn.Module):\n    \"\"\"\n    The main SPADE-based generator for the final image synthesis.\n    \"\"\"\n    def __init__(self, opt):\n        super().__init__()\n        self.opt = opt\n        nf = opt.ngf\n        self.sw, self.sh = self.compute_latent_vector_size(opt)\n\n        self.fc = nn.Conv2d(self.opt.semantic_nc, 16 * nf, 3, padding=1)\n        self.head_0 = SPADEResnetBlock(16 * nf, 16 * nf, opt)\n        self.G_middle_0 = SPADEResnetBlock(16 * nf, 16 * nf, opt)\n        self.G_middle_1 = SPADEResnetBlock(16 * nf, 16 * nf, opt)\n        self.up_0 = SPADEResnetBlock(16 * nf, 8 * nf, opt)\n        self.up_1 = SPADEResnetBlock(8 * nf, 4 * nf, opt)\n        self.up_2 = SPADEResnetBlock(4 * nf, 2 * nf, opt)\n        self.up_3 = SPADEResnetBlock(2 * nf, 1 * nf, opt)\n        self.conv_img = nn.Conv2d(nf, 3, 3, padding=1)\n        self.up = nn.Upsample(scale_factor=2)\n\n    def compute_latent_vector_size(self, opt):\n        num_up_layers = 5\n        sw = opt.fine_width // (2**num_up_layers)\n        sh = round(sw / (opt.fine_width / opt.fine_height))\n        return sw, sh\n\n    def forward(self, input, z=None):\n        seg = input\n        x = self.fc(seg)\n        x = self.head_0(x, seg)\n        x = self.up(x)\n        x = self.G_middle_0(x, seg)\n        x = self.G_middle_1(x, seg)\n        x = self.up(x)\n        x = self.up_0(x, seg)\n        x = self.up(x)\n        x = self.up_1(x, seg)\n        x = self.up(x)\n        x = self.up_2(x, seg)\n        x = self.up(x)\n        x = self.up_3(x, seg)\n        x = self.conv_img(F.leaky_relu(x, 2e-1))\n        x = F.tanh(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T17:28:50.983825Z","iopub.execute_input":"2025-07-16T17:28:50.984708Z","iopub.status.idle":"2025-07-16T17:28:51.015816Z","shell.execute_reply.started":"2025-07-16T17:28:50.984664Z","shell.execute_reply":"2025-07-16T17:28:51.015135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import argparse\nimport time\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nYOUR_KAGGLE_DATASET_NAME = 'clothes_tryon_dataset\n\nopt = argparse.Namespace()\n\n# --- Path Settings ---\nopt.dataroot = f'/kaggle/input/clothes_tryon_dataset'\nopt.name = 'TOCG_Training_Run' \nopt.checkpoint_dir = '/kaggle/working/checkpoints' \n\nopt.datamode = 'train'\nopt.data_list = 'train_pairs.txt'\nopt.workers = 2\nopt.batch_size = 4 \nopt.shuffle = True\n\n# --- Model & Training Settings ---\nopt.fine_height = 1024\nopt.fine_width = 768\n\nopt.input_nc = 11\nopt.output_nc = 1 \nopt.ngf = 64 \nopt.ndf = 64 \nopt.num_downs = 7 \nopt.norm = 'batch' \nopt.lr = 0.0004 \nopt.beta1 = 0.5 \nopt.lambda_l1 = 10.0 \n\nopt.display_freq = 200 \nopt.save_epoch_freq = 1 \nopt.niter = 10 \nopt.niter_decay = 20\nopt.tensorboard_dir = '/kaggle/working/tensorboard_logs'\n\n\n\nclass GANLoss(nn.Module):\n    \"\"\"Defines the GAN loss which uses either LSGAN or the regular GAN.\n    \"\"\"\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n        super(GANLoss, self).__init__()\n        self.register_buffer('real_label', torch.tensor(target_real_label))\n        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(input)\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0.0)\nprint(\"--- Initializing Stage 1 Training: Try-On Condition Generator (TOCG) ---\")\n\n# Create directories for checkpoints and logs\nos.makedirs(os.path.join(opt.checkpoint_dir, opt.name), exist_ok=True)\nos.makedirs(opt.tensorboard_dir, exist_ok=True)\nwriter = SummaryWriter(log_dir=os.path.join(opt.tensorboard_dir, opt.name))\n\ntrain_dataset = CustomDataset(opt)\ntrain_loader = CPDataLoader(opt, train_dataset)\nprint(f\"Dataset loaded with {len(train_dataset)} pairs.\")\n\n# Define the models from Cell 3\n# Generator (TOCG)\nnetG = UnetGenerator(opt.input_nc, opt.output_nc, opt.num_downs, opt.ngf, norm_layer=nn.BatchNorm2d)\nnetG.apply(weights_init)\n\n# Discriminator\n# The discriminator input is the generator input + the generator output\nnetD = NLayerDiscriminator(opt.input_nc + opt.output_nc, opt.ndf, n_layers=3, norm_layer=nn.BatchNorm2d)\nnetD.apply(weights_init)\n\n# Move models to GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnetG.to(device)\nnetD.to(device)\nprint(\"Models initialized and moved to GPU.\")\n\n# Define loss functions\ncriterionGAN = GANLoss().to(device)\ncriterionL1 = nn.L1Loss().to(device)\n\n# Setup optimizers\noptimizer_G = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\noptimizer_D = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n\n# Learning rate schedulers\nscheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda epoch: 1.0 - max(0, epoch + 1 - opt.niter) / float(opt.niter_decay + 1))\nscheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=lambda epoch: 1.0 - max(0, epoch + 1 - opt.niter) / float(opt.niter_decay + 1))\n\n\nprint(\"--- Starting Training Loop ---\")\ntotal_steps = 0\nfor epoch in range(1, opt.niter + opt.niter_decay + 1):\n    epoch_start_time = time.time()\n    for i, data in enumerate(train_loader.data_loader):\n        total_steps += 1\n\n        # Unpack data\n        cloth = data['cloth'].to(device)\n        cloth_mask = data['cloth_mask'].to(device)\n        pose_map = data['pose'].to(device)\n        densepose = data['densepose'].to(device)\n        # CORRECTED: Use the real parse_agnostic from the dataloader\n        parse_agnostic = data['parse_agnostic'].to(device)\n\n        # Concatenate inputs for the generator\n        inputs = torch.cat([pose_map, densepose, cloth, cloth_mask, parse_agnostic], 1)\n        \n        # Ground truth is the cloth mask\n        real_mask = cloth_mask\n\n        # --- Forward pass ---\n        fake_mask = netG(inputs)\n\n        # --- Train Discriminator (D) ---\n        optimizer_D.zero_grad()\n        # Real\n        real_AB = torch.cat((inputs, real_mask), 1)\n        pred_real = netD(real_AB.detach())\n        loss_D_real = criterionGAN(pred_real, True)\n        # Fake\n        fake_AB = torch.cat((inputs, fake_mask), 1)\n        pred_fake = netD(fake_AB.detach())\n        loss_D_fake = criterionGAN(pred_fake, False)\n        # Combine\n        loss_D = (loss_D_real + loss_D_fake) * 0.5\n        loss_D.backward()\n        optimizer_D.step()\n\n        # --- Train Generator (G) ---\n        optimizer_G.zero_grad()\n        # GAN loss\n        fake_AB = torch.cat((inputs, fake_mask), 1)\n        pred_fake = netD(fake_AB)\n        loss_G_GAN = criterionGAN(pred_fake, True)\n        # L1 loss\n        loss_G_L1 = criterionL1(fake_mask, real_mask) * opt.lambda_l1\n        # Combine\n        loss_G = loss_G_GAN + loss_G_L1\n        loss_G.backward()\n        optimizer_G.step()\n\n        # --- Logging and Visualization ---\n        if total_steps % opt.display_freq == 0:\n            print(f\"Epoch: {epoch}, Step: {total_steps}, Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}, Loss G_L1: {loss_G_L1.item():.4f}\")\n            writer.add_scalar('Loss/Discriminator', loss_D.item(), total_steps)\n            writer.add_scalar('Loss/Generator', loss_G.item(), total_steps)\n            writer.add_scalar('Loss/Generator_L1', loss_G_L1.item(), total_steps)\n\n    # --- End of Epoch ---\n    scheduler_G.step()\n    scheduler_D.step()\n    print(f\"End of Epoch {epoch} / {opt.niter + opt.niter_decay} \\t Time Taken: {time.time() - epoch_start_time:.2f}s\")\n\n    # Save checkpoint\n    if epoch % opt.save_epoch_freq == 0:\n        save_path_G = os.path.join(opt.checkpoint_dir, opt.name, f'epoch_{epoch}_net_G.pth')\n        save_path_D = os.path.join(opt.checkpoint_dir, opt.name, f'epoch_{epoch}_net_D.pth')\n        torch.save(netG.cpu().state_dict(), save_path_G)\n        torch.save(netD.cpu().state_dict(), save_path_D)\n        netG.to(device)\n        netD.to(device)\n        print(f\"Saved checkpoint for epoch {epoch} at {save_path_G}\")\n\n\n# Save final model\nfinal_save_path_G = os.path.join(opt.checkpoint_dir, opt.name, 'tocg_final.pth')\ntorch.save(netG.cpu().state_dict(), final_save_path_G)\nprint(f\"--- Training Complete. Final model saved to {final_save_path_G} ---\")\nwriter.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T17:31:38.631261Z","iopub.execute_input":"2025-07-16T17:31:38.631579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}