{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12302277,"sourceType":"datasetVersion","datasetId":7754175},{"sourceId":12523830,"sourceType":"datasetVersion","datasetId":7905481},{"sourceId":12527097,"sourceType":"datasetVersion","datasetId":7907780},{"sourceId":480088,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":385359,"modelId":404590}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nimport os\n\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, df, image_dir, tokenizer, transform):\n        self.df = df.reset_index(drop=True)\n        self.image_dir = image_dir\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.image_dir, row[\"filename\"])\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transform(image)\n\n        caption = row[\"caption\"]\n        text_inputs = self.tokenizer(\n        caption,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=32,\n        return_tensors=\"pt\")\n\n        input_ids = text_inputs.input_ids.squeeze(0).long()\n        attention_mask = text_inputs.attention_mask.squeeze(0)\n\n        return image, input_ids, attention_mask\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\n\nclass VisionEncoder(nn.Module):\n    def __init__(self, embed_dim=512):\n        super().__init__()\n        base = models.resnet18(pretrained=True)\n        base.fc = nn.Identity()\n        self.backbone = base\n        self.head = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, embed_dim)\n        )\n\n    def forward(self, x):\n        x = self.backbone(x)\n        return nn.functional.normalize(self.head(x), dim=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nclass TextEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=512, hidden_size=768, num_layers=1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n        self.gru = nn.GRU(hidden_size, embed_dim, num_layers=num_layers, batch_first=True)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embedding(input_ids)\n        packed_out, _ = self.gru(x)\n        x = packed_out[:, -1, :]  # last token\n        return nn.functional.normalize(x, dim=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MiniCLIP(nn.Module):\n    def __init__(self, vision_encoder, text_encoder):\n        super().__init__()\n        self.vision_encoder = vision_encoder\n        self.text_encoder = text_encoder\n        self.logit_scale = nn.Parameter(torch.ones([]) * torch.log(torch.tensor(1 / 0.07)))\n\n    def forward(self, images, input_ids, attention_mask):\n        img_embed = self.vision_encoder(images)        # [B, D]\n        txt_embed = self.text_encoder(input_ids, attention_mask)  # [B, D]\n\n        # Cosine similarity scaled\n        logit_scale = self.logit_scale.exp()\n        logits_per_image = logit_scale * img_embed @ txt_embed.T\n        logits_per_text = logits_per_image.T\n\n        return logits_per_image, logits_per_text\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef clip_loss(logits_per_image, logits_per_text):\n    B1, B2 = logits_per_image.shape\n    assert B1 == B2, f\"Expected square logits: got {logits_per_image.shape}\"\n    labels = torch.arange(B1).to(logits_per_image.device)\n    loss_i = F.cross_entropy(logits_per_image, labels)\n    loss_t = F.cross_entropy(logits_per_text, labels)\n    return (loss_i + loss_t) / 2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndef compute_recall_at_k(model, val_loader, device, k_list=[1, 5]):\n    model.eval()\n    all_img_embeds = []\n    all_txt_embeds = []\n\n    with torch.no_grad():\n        for images, input_ids, attention_mask in val_loader:\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n\n            img_embed = model.vision_encoder(images)       # [B, D]\n            txt_embed = model.text_encoder(input_ids, attention_mask)  # [B, D]\n\n            all_img_embeds.append(img_embed)\n            all_txt_embeds.append(txt_embed)\n\n    img_embeds = torch.cat(all_img_embeds, dim=0).cpu().numpy()\n    txt_embeds = torch.cat(all_txt_embeds, dim=0).cpu().numpy()\n\n    # Cosine similarity\n    sim_matrix = cosine_similarity(txt_embeds, img_embeds)  # [N, N]\n    ranks = np.argsort(-sim_matrix, axis=1)  # Descending\n\n    recalls = {}\n    for k in k_list:\n        hits = [(i in ranks[i, :k]) for i in range(len(ranks))]\n        recalls[f\"Recall@{k}\"] = np.mean(hits) * 100\n\n    return recalls\n\n\ndef train_clip_model(model, train_loader, val_loader, epochs=40, lr=1e-4, save_path=\"/kaggle/working/miniclip_best.pth\"):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\n    best_loss = float(\"inf\")\n    train_losses = []\n    val_recalls1 = []\n    val_recalls5 = []\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0.0\n        loop = tqdm(train_loader, desc=f\"📚 Epoch {epoch+1}/{epochs}\")\n\n        for images, input_ids, attention_mask in loop:\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n\n            optimizer.zero_grad()\n\n            logits_i, logits_t = model(images, input_ids, attention_mask)\n\n            # Contrastive loss\n            B1, B2 = logits_i.shape\n            assert B1 == B2, f\"Expected square logits: got {logits_i.shape}\"\n            labels = torch.arange(B1).to(device)\n            loss_i = F.cross_entropy(logits_i, labels)\n            loss_t = F.cross_entropy(logits_t, labels)\n            loss = (loss_i + loss_t) / 2\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            avg_loss = total_loss / (loop.n + 1)\n            loop.set_postfix(loss=loss.item(), avg=avg_loss)\n\n        avg_epoch_loss = total_loss / len(train_loader)\n        train_losses.append(avg_epoch_loss)\n        scheduler.step(avg_epoch_loss)\n        print(f\"\\n✅ Epoch {epoch+1}: Avg Loss = {avg_epoch_loss:.4f}\")\n\n        # 🧪 Validation\n        recalls = compute_recall_at_k(model, val_loader, device)\n        val_recalls1.append(recalls[\"Recall@1\"])\n        val_recalls5.append(recalls[\"Recall@5\"])\n        print(f\"🎯 Recall@1: {recalls['Recall@1']:.2f}%  |  Recall@5: {recalls['Recall@5']:.2f}%\")\n\n        # 💾 Save best\n        if avg_epoch_loss < best_loss:\n            best_loss = avg_epoch_loss\n            torch.save(model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict(), save_path)\n            print(f\"💾 Best model saved (loss: {best_loss:.4f}) → {save_path}\")\n\n    return train_losses, val_recalls1, val_recalls5\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\nimport torch\nfrom PIL import Image\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\nimport pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your caption CSV\ndf = pd.read_csv(\"/kaggle/input/blip-captions/blip_captions (1).csv\")\nimage_dir = \"/kaggle/input/clothe/clothes_tryon_dataset/train/cloth\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_embeddings = []\nall_filenames = []\n\nbatch_size = 32\nfor i in tqdm(range(0, len(df), batch_size), desc=\" Encoding Images\"):\n    batch_df = df.iloc[i:i+batch_size]\n    batch_imgs = [Image.open(os.path.join(image_dir, fname)).convert(\"RGB\") for fname in batch_df[\"filename\"]]\n    \n    inputs = processor(images=batch_imgs, return_tensors=\"pt\", padding=True).to(device)\n    with torch.no_grad():\n        img_feats = model.get_image_features(**inputs)\n        img_feats = torch.nn.functional.normalize(img_feats, dim=1)\n    \n    all_embeddings.append(img_feats.cpu())\n    all_filenames.extend(batch_df[\"filename\"].tolist())\n\n# Save\nimage_embeds = torch.cat(all_embeddings, dim=0)  # [N, 512]\ntorch.save({\"embeds\": image_embeds, \"files\": all_filenames}, \"/kaggle/working/clip_image_embeds.pt\")\nprint(\" Saved CLIP image embeddings.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef search_clip(query, model, processor, image_embeds, image_filenames, top_k=5):\n    inputs = processor(text=[query], return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        text_feat = model.get_text_features(**inputs)\n        text_feat = torch.nn.functional.normalize(text_feat, dim=1)\n\n    sims = cosine_similarity(text_feat.cpu().numpy(), image_embeds.numpy())[0]\n    top_idx = sims.argsort()[::-1][:top_k]\n    return [(image_filenames[i], sims[i]) for i in top_idx]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef show_results(results, image_dir):\n    plt.figure(figsize=(15, 3))\n    for idx, (fname, score) in enumerate(results):\n        path = os.path.join(image_dir, fname)\n        img = Image.open(path).convert(\"RGB\")\n        plt.subplot(1, len(results), idx + 1)\n        plt.imshow(img)\n        plt.title(f\"{score:.2f}\")\n        plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = torch.load(\"/kaggle/input/clip-embed/pytorch/default/1/clip_image_embeds (1).pt\")\nimage_embeds = data[\"embeds\"]\nimage_filenames = data[\"files\"]\n\nquery = \"a red frock\"\nresults = search_clip(query, model, processor, image_embeds, image_filenames, top_k=5)\nshow_results(results, image_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load and preprocess the query image\nquery_image_path = \"/kaggle/input/clothe/clothes_tryon_dataset/test/cloth/00094_00.jpg\"\nimg = Image.open(query_image_path).convert(\"RGB\")\ninputs = processor(images=img, return_tensors=\"pt\").to(device)\n\n#  Get normalized CLIP image embedding\nquery_embed = model.get_image_features(**inputs)\nquery_embed = torch.nn.functional.normalize(query_embed, dim=1)\n\n#  Detach before converting to NumPy\nsims = cosine_similarity(query_embed.detach().cpu().numpy(), image_embeds.numpy())[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\n# Config\ntop_k = 5  # change as needed\nimage_dir = \"/kaggle/input/clothe/clothes_tryon_dataset/train/cloth\"\n\n# Get Top-K similar indices\ntop_indices = sims.argsort()[::-1][:top_k]\n\n# Top-k filenames and scores\ntop_results = [(image_filenames[i], sims[i]) for i in top_indices]\n\n# Display the query image\nplt.figure(figsize=(3, 3))\nplt.imshow(img)\nplt.title(\"🔍 Query Image\")\nplt.axis(\"off\")\nplt.show()\n\n# Show results\nplt.figure(figsize=(12, 4))\nfor idx, (fname, score) in enumerate(top_results):\n    img_path = os.path.join(image_dir, fname)\n    img = Image.open(img_path).convert(\"RGB\")\n    plt.subplot(1, top_k, idx + 1)\n    plt.imshow(img)\n    plt.title(f\"{score:.2f}\")\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nimport torch.nn.functional as F\n\ndef multi_modal_search(\n    query_text, \n    query_image, \n    model, \n    processor, \n    image_embeds, \n    image_filenames, \n    top_k=5,\n    image_weight=0.5,\n    text_weight=0.5\n):\n    \"\"\"\n    Performs a multi-modal search by combining text and image queries.\n    \n    Args:\n        query_text (str): The text part of the query.\n        query_image (PIL.Image): The image part of the query.\n        model: The pre-trained CLIP model.\n        processor: The CLIP processor.\n        image_embeds (torch.Tensor): Pre-computed embeddings of the image database.\n        image_filenames (list): List of filenames corresponding to the embeddings.\n        top_k (int): Number of top results to return.\n        image_weight (float): The influence of the image query.\n        text_weight (float): The influence of the text query.\n    \"\"\"\n    device = next(model.parameters()).device\n\n    # 1. Process and encode the text query\n    text_inputs = processor(text=[query_text], return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        text_feat = model.get_text_features(**text_inputs)\n        text_feat = F.normalize(text_feat, dim=1)\n\n    # 2. Process and encode the image query\n    image_inputs = processor(images=query_image, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        img_feat = model.get_image_features(**image_inputs)\n        img_feat = F.normalize(img_feat, dim=1)\n\n    # 3. Combine the embeddings (the core of multi-modal search)\n    # We create the final query vector using a weighted average of the two modalities.\n    combined_feat = (image_weight * img_feat) + (text_weight * text_feat)\n    \n    # 4. Normalize the combined embedding\n    # This is crucial to ensure it's a valid vector for cosine similarity.\n    combined_feat = F.normalize(combined_feat, dim=1)\n\n    # 5. Perform search against the database\n    # We compare our new combined feature vector against all pre-computed image embeddings.\n    sims = cosine_similarity(combined_feat.cpu().numpy(), image_embeds.numpy())[0]\n    \n    # 6. Get top results\n    top_idx = sims.argsort()[::-1][:top_k]\n    \n    return [(image_filenames[i], sims[i]) for i in top_idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport os\nimport matplotlib.pyplot as plt\n\n# --- Setup: Make sure these variables from your previous code are loaded ---\n# model, processor, device\n# data = torch.load(\"/kaggle/input/embdclip/pytorch/default/1/clip_image_embeds.pt\")\n# image_embeds = data[\"embeds\"]\n# image_filenames = data[\"files\"]\n# image_dir = \"/kaggle/input/clothe/clothes_tryon_dataset/train/cloth\"\n# --------------------------------------------------------------------------\n\n\n# 1. Define your multi-modal query\nquery_image_path = \"/kaggle/input/clothe/clothes_tryon_dataset/test/cloth/00075_00.jpg\" # A striped t-shirt\nquery_text = \"Give me similar tshirts but with long sleeves\" # We want to find something like the image, but as a blue sweater.\n\n# 2. Load the query image\nquery_image = Image.open(query_image_path).convert(\"RGB\")\n\n# Display the query image and text\nprint(f\"🔍 Image Query: An item with this pattern/shape.\")\nprint(f\"✍️ Text Query: '{query_text}'\")\nplt.figure(figsize=(3, 3))\nplt.imshow(query_image)\nplt.axis(\"off\")\nplt.show()\n\n# 3. Run the multi-modal search\n# Let's give slightly more weight to the text to emphasize the \"blue sweater\" aspect.\nresults = multi_modal_search(\n    query_text, \n    query_image, \n    model, \n    processor, \n    image_embeds, \n    image_filenames, \n    top_k=5,\n    image_weight=0.4, # Less weight on the original t-shirt pattern\n    text_weight=0.6   # More weight on the \"blue sweater\" concept\n)\n\n# 4. Show the results (using your existing `show_results` function)\nprint(\"\\nTop 5 Multi-Modal Search Results:\")\nshow_results(results, image_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install diffusers transformers accelerate --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom diffusers import StableDiffusionPipeline\n\n# Make sure the device is set correctly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the pre-trained Stable Diffusion model\n# We use float16 for faster inference and less memory usage on the GPU\npipe = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", \n    torch_dtype=torch.float16\n)\npipe = pipe.to(device)\n\nprint(\"Stable Diffusion pipeline loaded successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\ndef generate_clothing_design(prompt, num_inference_steps=50, guidance_scale=7.5):\n    \"\"\"\n    Generates an image of a clothing item from a text prompt using Stable Diffusion.\n    \n    Args:\n        prompt (str): The user's description of the clothing item.\n        num_inference_steps (int): Number of denoising steps. Higher is better quality but slower.\n        guidance_scale (float): How much to adhere to the prompt. Higher is stricter.\n        \n    Returns:\n        PIL.Image: The generated image.\n    \"\"\"\n    \n    # --- Prompt Engineering ---\n    # We add keywords to focus the model on generating a clean product image.\n    # This is a critical step for getting good results.\n    enhanced_prompt = (\n        f\"A photorealistic image of {prompt}, fashion design, \"\n        \"product shot, studio lighting, on a mannequin, white background, 8k, whole outfit should be visible\"\n    )\n    \n    print(f\"Generating image with enhanced prompt: '{enhanced_prompt}'\")\n    \n    # Run the diffusion pipeline\n    with torch.no_grad():\n        # The 'pipe' returns an object with the generated images\n        result = pipe(\n            enhanced_prompt, \n            num_inference_steps=num_inference_steps, \n            guidance_scale=guidance_scale\n        )\n        generated_image = result.images[0]\n    \n    return generated_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# 1. Define the user's dream clothing item\nuser_prompt = \"a black tshirt with graphic detailed logo on it\"\n\n# 2. Generate the design\n# This will take a moment to run on the GPU\nnew_design_image = generate_clothing_design(user_prompt)\n\n# 3. Visualize the newly created design\nprint(\"\\n✨ Generated Design:\")\nplt.figure(figsize=(6, 6))\nplt.imshow(new_design_image)\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We need a function for image-to-image search. \n# We can adapt your previous `search_clip` or create a specific one.\n\ndef search_by_image(query_image, model, processor, image_embeds, image_filenames, top_k=5):\n    \"\"\"Performs an image-to-image search using CLIP.\"\"\"\n    device = next(model.parameters()).device\n    \n    # Preprocess the query image and get its embedding\n    inputs = processor(images=query_image, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        query_embed = model.get_image_features(**inputs)\n        query_embed = F.normalize(query_embed, dim=1)\n\n    # Perform cosine similarity search\n    sims = cosine_similarity(query_embed.cpu().numpy(), image_embeds.numpy())[0]\n    top_indices = sims.argsort()[::-1][:top_k]\n    \n    return [(image_filenames[i], sims[i]) for i in top_indices]\n\n# --- Let's run the integration ---\n\nprint(\"\\nUsing the generated design to find similar items in our database...\")\n\n# 1. Use the 'new_design_image' we just created as the query\n#    (Ensure your other variables like model, processor, image_embeds are loaded)\nsearch_results = search_by_image(\n    new_design_image,\n    model,\n    processor,\n    image_embeds,\n    image_filenames,\n    top_k=5\n)\n\n# 2. Show the results (using your existing show_results function)\nprint(\"\\nTop 5 Real Items Similar to the Generated Design:\")\nshow_results(search_results, image_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom diffusers import StableDiffusionInpaintPipeline\nfrom PIL import Image\nimport numpy as np\nimport cv2 # We'll use OpenCV to create a mask programmatically\n\n# Ensure you are on a GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the inpainting pipeline. This is different from the standard StableDiffusionPipeline.\n# This model is specifically designed to understand image+mask+prompt inputs.\ninpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    torch_dtype=torch.float16,\n).to(device)\n\nprint(\"Stable Diffusion Inpainting pipeline loaded successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# --- Setup: Make sure these variables are loaded from your previous code ---\n# image_dir = \"/kaggle/input/clothe/clothes_tryon_dataset/train/cloth\"\n# --------------------------------------------------------------------------\n\n# 1. Select the base image we want to edit\nbase_image_path = os.path.join(image_dir, \"00010_00.jpg\") # A plain black t-shirt\nbase_image = Image.open(base_image_path).convert(\"RGB\").resize((512, 512))\n\n# 2. Create the mask programmatically\n# In a real app, this mask would be drawn by the user with a brush tool.\n# Here, we create a black image and draw a white rectangle on it to define the edit area.\nmask_image = np.zeros((512, 512), dtype=np.uint8)\n# Let's define a rectangle on the chest of the t-shirt\ncv2.rectangle(mask_image, (160, 150), (350, 300), (255, 255, 255), -1) # -1 fills the rectangle\nmask_image = Image.fromarray(mask_image)\n\n# 3. Visualize the inputs to understand what we're doing\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(base_image)\nplt.title(\"1. Base Image\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(mask_image, cmap='gray')\nplt.title(\"2. Mask (White area will be edited)\")\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_edited_design(base_image, mask_image, prompt, pipe):\n    \"\"\"\n    Generates an edited image using the Stable Diffusion Inpainting pipeline.\n    \n    Args:\n        base_image (PIL.Image): The original image.\n        mask_image (PIL.Image): The mask defining the area to change.\n        prompt (str): The text description of the desired change.\n        pipe: The loaded inpainting pipeline.\n        \n    Returns:\n        PIL.Image: The new, edited image.\n    \"\"\"\n    print(f\"Applying new design with prompt: '{prompt}'\")\n    \n    # The inpainting pipeline takes the prompt, base image, and mask image as input.\n    with torch.no_grad():\n        edited_image = pipe(\n            prompt=prompt, \n            image=base_image, \n            mask_image=mask_image\n        ).images[0]\n        \n    return edited_image\n\n# Let's define what we want to paint onto the t-shirt's chest\nedit_prompt = \"A detailed embroidered patch of a roaring tiger's head, hyperrealistic\"\n\n# Run the creative process!\nedited_design = create_edited_design(base_image, mask_image, edit_prompt, inpaint_pipe)\n\n# Visualize the final result\nplt.figure(figsize=(6, 6))\nplt.imshow(edited_design)\nplt.title(\"3. Generated Design!\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def enhance_prompt(base_prompt):\n    \"\"\"\n    Takes a simple user prompt and expands it into a detailed, high-quality prompt\n    for Stable Diffusion, including a negative prompt.\n    \n    Args:\n        base_prompt (str): The user's simple input (e.g., \"a wolf\").\n        \n    Returns:\n        tuple: A tuple containing the (enhanced_positive_prompt, negative_prompt).\n    \"\"\"\n    \n    # This is the core of our prompt engineering.\n    # We combine descriptive keywords about style, medium, and quality.\n    enhanced_positive_prompt = (\n        f\"A masterpiece, photorealistic, high-quality embroidered patch of a {base_prompt}. \"\n        \"The design features an intricate, hyper-detailed, and sharp focus. \"\n        \"Rendered with cinematic lighting, 8k ultra-high definition, professional product shot on fabric.\"\n    )\n    \n    # The negative prompt is crucial for avoiding common image generation errors.\n    negative_prompt = (\n        \"lowres, blurry, bad anatomy, error, worst quality, low quality, \"\n        \"jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, \"\n        \"extra fingers, mutated hands, poorly drawn hands, poorly drawn face, \"\n        \"malformed limbs, extra limbs, cloned face, disfigured, gross proportions, \"\n        \"watermark, grain, text, signature\"\n    )\n    \n    return enhanced_positive_prompt, negative_prompt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_edited_design_enhanced(base_image, mask_image, user_prompt, pipe):\n    \"\"\"\n    Generates a high-quality edited image by first enhancing the user's prompt.\n    \n    Args:\n        base_image (PIL.Image): The original image.\n        mask_image (PIL.Image): The mask defining the area to change.\n        user_prompt (str): The user's SIMPLE text description.\n        pipe: The loaded inpainting pipeline.\n        \n    Returns:\n        PIL.Image: The new, high-quality edited image.\n    \"\"\"\n    \n    # 1. Enhance the user's simple prompt\n    enhanced_prompt, negative_prompt = enhance_prompt(user_prompt)\n    \n    print(\"--- Prompt Enhancement ---\")\n    print(f\"Original User Prompt: '{user_prompt}'\")\n    print(f\"✨ Enhanced Prompt: '{enhanced_prompt}'\")\n    print(f\"🚫 Negative Prompt: '{negative_prompt}'\")\n    print(\"--------------------------\")\n    \n    # 2. Run the pipeline with the new, powerful prompts\n    with torch.no_grad():\n        edited_image = pipe(\n            prompt=enhanced_prompt, \n            image=base_image, \n            mask_image=mask_image,\n            negative_prompt=negative_prompt, # Add the negative prompt here\n            num_inference_steps=50,          # Can use more steps for higher quality\n            guidance_scale=8.5               # A slightly higher guidance can help with detailed prompts\n        ).images[0]\n        \n    return edited_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The user provides a very simple, \"bad\" prompt.\nuser_prompt = \"a tshirt with a tiger head on it\" \n\n# Our system will automatically turn this into a masterpiece.\n# We use the new function: create_edited_design_enhanced\nedited_design_enhanced = create_edited_design_enhanced(\n    base_image, \n    mask_image, \n    user_prompt, \n    inpaint_pipe\n)\n\n# Visualize the final, high-quality result\nplt.figure(figsize=(8, 8))\nplt.imshow(edited_design_enhanced)\nplt.title(f\"Generated from simple prompt: '{user_prompt}'\")\nplt.axis(\"off\")\nplt.show()\n\n# And of course, you can still use this superior image to search your database\n# final_results = search_by_image(edited_design_enhanced, ...)\n# show_results(final_results, ...)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Setup: Make sure these variables and functions are loaded from your previous code ---\n# model, processor\n# image_embeds, image_filenames\n# image_dir\n# def search_by_image(...)\n# def show_results(...)\n# user_prompt = \"a skull\"  # The simple prompt from the user\n# edited_design_enhanced # The high-quality image we generated\n# ----------------------------------------------------------------------------------------\n\nprint(\"\\n------------------------------------------------------------------\")\nprint(\"✅ Creative Step Complete. Now finding similar items in our database...\")\nprint(\"------------------------------------------------------------------\\n\")\n\n# Use the 'edited_design_enhanced' (the high-quality image) as the query image.\n# This is the key change.\nfinal_results = search_by_image(\n    edited_design_enhanced, # <--- Using the image generated with the enhanced prompt\n    model,\n    processor,\n    image_embeds,\n    image_filenames,\n    top_k=5\n)\n\n# Show the final results from your database\n# We use the original, simple 'user_prompt' in the title for clarity.\nprint(f\"Top 5 Real Items similar to the generated '{user_prompt}' design:\")\nshow_results(final_results, image_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport json\n\n# Load your captions dataset\ndf = pd.read_csv(\"/kaggle/input/blip-captions/blip_captions (1).csv\")\n\n# This dictionary is our Fashion Knowledge Base.\n# We can easily add more keywords or new categories (e.g., 'material') later.\nATTRIBUTE_KEYWORDS = {\n    'brand': [\n        'levi', 'adidas', 'nike', 'calvin', 'tommy', 'champion', 'guess', \n        'hollister', 'vans', 'puma', 'superdry', 'lacoste', 'boss'\n    ],\n    'style': [\n        't-shirt', 'long sleeve', 'sweatshirt', 'hoodie', 'bodysuit', \n        'polo', 'blouse', 'crop top', 'tank top', 'cardigan', 'jumper',\n        'vest', 'cami'\n    ],\n    'color': [\n        'black', 'white', 'red', 'blue', 'green', 'pink', 'yellow', 'grey', \n        'purple', 'orange', 'brown', 'navy', 'silver', 'gold', 'multi',\n        'beige', 'khaki', 'maroon', 'burgundy'\n    ],\n    'pattern': [\n        'floral', 'striped', 'polka dot', 'leopard', 'plaid', 'tie dye', \n        'camo', 'checkered', 'paisley', 'zebra', 'houndstooth', 'print'\n    ],\n    'neckline': [\n        'v-neck', 'scoop neck', 'turtle neck', 'round neck', 'crew neck', \n        'square neck', 'halter neck'\n    ]\n}\n\nprint(\"Fashion attribute keywords defined.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_tags(caption, keywords_dict):\n    \"\"\"\n    Parses a caption and extracts a set of predefined attribute tags.\n    \n    Args:\n        caption (str): The clothing description.\n        keywords_dict (dict): The dictionary of attributes and their keywords.\n        \n    Returns:\n        list: A list of unique tags found in the caption (e.g., ['color_black', 'style_t-shirt']).\n    \"\"\"\n    if not isinstance(caption, str):\n        return []\n        \n    found_tags = set()\n    caption_lower = caption.lower()\n    \n    for attribute, keywords in keywords_dict.items():\n        for keyword in keywords:\n            # Use regex to match whole words to avoid partial matches (e.g., 'red' in 'bordeaux')\n            # The \\b markers stand for word boundaries.\n            if re.search(r'\\b' + re.escape(keyword) + r'\\b', caption_lower):\n                # Create a clean tag format, replacing spaces with underscores\n                clean_keyword = keyword.replace(' ', '_')\n                found_tags.add(f\"{attribute}_{clean_keyword}\")\n                \n    return list(found_tags)\n\n# --- Test the function with one example ---\nexample_caption = df.iloc[15]['caption'] # \"adidas 3 stripes tee - t - shirt - grey heather / white\"\nexample_tags = generate_tags(example_caption, ATTRIBUTE_KEYWORDS)\nprint(f\"Caption: '{example_caption}'\")\nprint(f\"Generated Tags: {example_tags}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Generating tags for all captions... This may take a moment.\")\n\n# Use the .apply() method to efficiently run the function on each row\ndf['tags'] = df['caption'].apply(lambda c: generate_tags(c, ATTRIBUTE_KEYWORDS))\n\nprint(\"Tag generation complete!\")\n\n# --- Let's save our newly tagged data for future use ---\n# Saving as a JSON is often better for columns containing lists.\ndf.to_json('tagged_fashion_data.json', orient='records', lines=True)\n\nprint(\"\\nDataFrame with new 'tags' column:\")\n# Display the filename and the new tags column to see the result\nprint(df[['filename', 'caption', 'tags']].head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For this step, we'll use the DataFrame 'df' we just created.\n# In a real app, you would load the 'tagged_fashion_data.json' file.\ntagged_df = df.set_index('filename') # Setting filename as index for fast lookups\n\ndef filter_results(initial_results, tagged_data, filters):\n    \"\"\"\n    Filters a list of search results based on a dictionary of attribute filters.\n    \n    Args:\n        initial_results (list): A list of tuples (filename, score) from a CLIP search.\n        tagged_data (pd.DataFrame): The DataFrame containing the pre-generated tags.\n        filters (dict): A dictionary of filters, e.g., {'color': 'red', 'style': 't-shirt'}.\n        \n    Returns:\n        list: A new list of tuples that match all the specified filters.\n    \"\"\"\n    if not filters:\n        return initial_results\n        \n    # Get the filenames from the initial search results\n    initial_filenames = {filename for filename, score in initial_results}\n    \n    # Build the list of expected tag strings from the filters dictionary\n    expected_tags = {f\"{attribute}_{value.replace(' ', '_')}\" for attribute, value in filters.items()}\n    \n    matching_filenames = set()\n    \n    # Iterate through only the relevant items from our search\n    for filename in initial_filenames:\n        try:\n            item_tags = set(tagged_data.loc[filename, 'tags'])\n            # Check if all expected tags are a subset of the item's tags\n            if expected_tags.issubset(item_tags):\n                matching_filenames.add(filename)\n        except KeyError:\n            # This can happen if a filename from search isn't in our CSV\n            continue\n            \n    # Rebuild the results list, preserving the original order and scores\n    final_results = [(filename, score) for filename, score in initial_results if filename in matching_filenames]\n    \n    return final_results\n\nprint(\"Filter function created.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom collections import defaultdict\n\n# --- Setup: Load the tagged data we created in the last step ---\n# In a real app, you would load this from 'tagged_fashion_data.json'\n# df = pd.read_json('tagged_fashion_data.json', orient='records', lines=True)\n# --------------------------------------------------------------------\n\ndef generate_dropdown_options(tagged_df):\n    \"\"\"\n    Scans the tagged DataFrame and creates a dictionary of all available\n    filter options for a UI.\n    \n    Args:\n        tagged_df (pd.DataFrame): The DataFrame with the 'tags' column.\n        \n    Returns:\n        dict: A dictionary where keys are attributes (e.g., 'color') and\n              values are sorted lists of unique options (e.g., ['black', 'blue', ...]).\n    \"\"\"\n    # Use defaultdict to automatically handle the creation of new keys\n    options = defaultdict(set)\n    \n    # Iterate through the 'tags' column\n    for tag_list in tagged_df['tags']:\n        if not tag_list:  # Skip empty lists\n            continue\n        for tag in tag_list:\n            try:\n                # Split 'attribute_value' format (e.g., 'color_black')\n                attribute, value = tag.split('_', 1)\n                # Replace underscores back to spaces for display\n                options[attribute].add(value.replace('_', ' '))\n            except ValueError:\n                # Handle cases where a tag might not have an underscore\n                continue\n                \n    # Convert sets to sorted lists for consistent ordering in the UI\n    sorted_options = {attribute: sorted(list(values)) for attribute, values in options.items()}\n    \n    return sorted_options\n\n# Generate the options for our UI\ndropdown_options = generate_dropdown_options(df)\n\n# --- Display the generated options as if they were in a UI ---\nprint(\"--- Dynamically Generated Drop-Down Menus ---\")\nfor attribute, values in dropdown_options.items():\n    print(f\"\\n▼ {attribute.capitalize()} Menu:\")\n    # We'll just print the first 10 for brevity\n    print(values[:10]) \n    if len(values) > 10:\n        print(f\"  (...and {len(values) - 10} more)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make sure the tagged_df is indexed by filename for fast lookups\ntagged_df = df.set_index('filename')\n\ndef flexible_ranked_filter(initial_results, tagged_data, filters):\n    \"\"\"\n    Filters and re-ranks results based on how many tags they match.\n    Items matching more tags are ranked higher. The original search score is\n    used as a tie-breaker.\n    \n    Args:\n        initial_results (list): A list of tuples (filename, score) from CLIP search.\n        tagged_data (pd.DataFrame): The DataFrame with pre-generated tags.\n        filters (dict): A dictionary of desired filters, e.g., {'color': 'red'}.\n        \n    Returns:\n        list: A new list of tuples, re-ranked based on tag matches and original score.\n    \"\"\"\n    if not filters:\n        return initial_results\n        \n    # Build the set of expected tag strings from the filters dictionary\n    expected_tags = {f\"{attribute}_{value.replace(' ', '_')}\" for attribute, value in filters.items()}\n    \n    scored_results = []\n    \n    for filename, original_score in initial_results:\n        try:\n            item_tags = set(tagged_data.loc[filename, 'tags'])\n            \n            # Calculate how many of the desired tags this item has\n            match_count = len(expected_tags.intersection(item_tags))\n            \n            # We only add items that match at least one tag to the list.\n            # You could change this to `if match_count >= 0:` to include everything.\n            if match_count > 0:\n                # Create a new tuple with the match count for sorting\n                scored_results.append((filename, original_score, match_count))\n                \n        except KeyError:\n            continue\n            \n    # Sort the results. This is the key step.\n    # - We sort by 'match_count' in descending order (primary criteria).\n    # - Then, we sort by 'original_score' in descending order (tie-breaker).\n    # The `lambda` function defines this multi-level sorting.\n    sorted_results = sorted(scored_results, key=lambda x: (x[2], x[1]), reverse=True)\n    \n    # Rebuild the list into the original (filename, score) format\n    final_ranked_list = [(filename, score) for filename, score, match_count in sorted_results]\n    \n    return final_ranked_list\n\nprint(\"Flexible ranked filtering function created.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Setup: Make sure all necessary variables and functions are loaded ---\n# model, processor, image_embeds, image_filenames, image_dir\n# def search_clip(...)\n# def show_results(...)\n# -------------------------------------------------------------------------\n\n# 1. USER ACTION: Perform a broad initial search\ninitial_query = \"a casual top\"\nprint(f\"Performing initial search for: '{initial_query}'\")\ninitial_results = search_clip(initial_query, model, processor, image_embeds, image_filenames, top_k=200) # Get a large pool\n\n# 2. USER ACTION: Apply a very specific combination of filters\n# Let's imagine the user is looking for a \"pink, striped, v-neck polo\". \n# This exact item might not exist in our small dataset.\nmy_filters = {\n    'color': 'pink',\n    'pattern': 'striped',\n    'neckline': 'v-neck',\n    'style': 'polo'\n}\nprint(\"\\n---------------------------------------------------------\")\nprint(f\"Applying filters (looking for best matches): {my_filters}\")\nprint(\"---------------------------------------------------------\")\n\n# 3. RUN THE FLEXIBLE FILTERING LOGIC\nranked_results = flexible_ranked_filter(initial_results, tagged_df, my_filters)\n\n# 4. SHOW THE RESULTS\n# The results will now be ordered by how many tags they matched.\n# A pink, striped v-neck (3 matches) will appear before a pink polo (2 matches),\n# which will appear before a generic striped shirt (1 match).\nif ranked_results:\n    print(f\"Showing best matches out of {len(ranked_results)} relevant items found:\")\n    show_results(ranked_results[:5], image_dir) # Show the top 5 best matches\n    \n    # Let's inspect the tags of the top result to see why it was chosen\n    top_result_filename = ranked_results[0][0]\n    top_result_tags = tagged_df.loc[top_result_filename, 'tags']\n    print(f\"\\nTags for the top recommended item ('{top_result_filename}'):\")\n    print(top_result_tags)\nelse:\n    print(\"No items matched any of your selected filters.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------------------------------------------------------------\n# Cell 1: Install All Dependencies and Write the Streamlit App File\n# ----------------------------------------------------------------------------------\n\n# Step 1: Install a Kaggle-compatible PyTorch first.\n!pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118\n\n# Step 2: Install all other packages, using specific, compatible versions for the entire ecosystem.\n# This is the final fix that pins the 'peft' library to the version required by diffusers.\n!pip install streamlit==1.28.2 pyngrok==7.0.0 transformers==4.34.0 diffusers==0.23.1 accelerate==0.24.1 pandas==2.1.3 scikit-learn==1.3.2 opencv-python-headless==4.8.1.78 peft==0.6.2 --quiet\n\n# Step 3: Write the entire Streamlit app to a file named app.py.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%writefile app.py\n# import streamlit as st\n# import torch\n# import pandas as pd\n# import os\n# import re\n# from PIL import Image, ImageDraw, ImageFont\n# from collections import defaultdict\n# from transformers import CLIPProcessor, CLIPModel\n# from diffusers import StableDiffusionInpaintPipeline\n# from sklearn.metrics.pairwise import cosine_similarity\n# import torch.nn.functional as F\n\n# # =============================================================================\n# # 1. PAGE CONFIGURATION & INITIALIZATION\n# # =============================================================================\n# st.set_page_config(\n#     page_title=\"AI Fashion Stylist Pro\",\n#     page_icon=\"🤖\",\n#     layout=\"wide\"\n# )\n\n# # Initialize session state for variables that need to persist across reruns\n# if 'search_results' not in st.session_state:\n#     st.session_state.search_results = []\n# if 'generated_design' not in st.session_state:\n#     st.session_state.generated_design = None\n# if 'similar_items' not in st.session_state:\n#     st.session_state.similar_items = []\n\n# # =============================================================================\n# # 2. MODEL & DATA LOADING (Cached to run only once)\n# # =============================================================================\n# @st.cache_resource\n# def load_all_models_and_data():\n#     \"\"\"Loads all models and data files into memory, cached for performance.\"\"\"\n#     print(\"Executing one-time resource loading...\")\n    \n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#     clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n#     clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n#     inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n#         \"runwayml/stable-diffusion-inpainting\",\n#         torch_dtype=torch.float16,\n#     ).to(device)\n\n#     # --- KAGGLE-SPECIFIC FILE PATHS ---\n#     data_dir = '/kaggle/input/clothe/clothes_tryon_dataset/train/cloth'\n#     embeddings_path = \"/kaggle/input/clip-embed/pytorch/default/1/clip_image_embeds (1).pt\"\n#     tagged_data_path = \"/kaggle/input/tagged-fashion-data/tagged_fashion_data.json\"\n\n#     if not os.path.exists(embeddings_path) or not os.path.exists(tagged_data_path):\n#         st.error(f\"CRITICAL ERROR: Data files not found in '{data_dir}'. Please ensure you have attached the 'fashion-app-data' Kaggle dataset containing 'clip_image_embeds.pt' and 'tagged_fashion_data.json'.\")\n#         st.stop()\n        \n#     embedding_data = torch.load(embeddings_path, map_location='cpu')\n#     image_embeds = embedding_data[\"embeds\"]\n#     image_filenames = embedding_data[\"files\"]\n\n#     tagged_df = pd.read_json(tagged_data_path, orient='records', lines=True)\n#     tagged_df_indexed = tagged_df.set_index('filename')\n\n#     print(\"Resource loading complete.\")\n#     return device, clip_model, clip_processor, inpaint_pipe, image_embeds, image_filenames, tagged_df, tagged_df_indexed\n\n# DEVICE, clip_model, clip_processor, inpaint_pipe, image_embeds, image_filenames, tagged_df, tagged_df_indexed = load_all_models_and_data()\n\n# # !!! CRITICAL !!! UPDATE THIS PATH to match your clothing image dataset on Kaggle.\n# IMAGE_DIR = \"/kaggle/input/clothe/clothes_tryon_dataset/train/cloth/\"\n# if not os.path.exists(IMAGE_DIR):\n#     st.error(f\"Image directory not found at '{IMAGE_DIR}'. Please update the IMAGE_DIR path in the `app.py` script to match your Kaggle dataset path.\")\n#     st.stop()\n\n# # =============================================================================\n# # 3. BACKEND FUNCTIONS\n# # =============================================================================\n# @st.cache_data\n# def generate_dropdown_options(_df):\n#     options = defaultdict(set)\n#     for tag_list in _df['tags']:\n#         if not isinstance(tag_list, list): continue\n#         for tag in tag_list:\n#             try:\n#                 attribute, value = tag.split('_', 1)\n#                 options[attribute].add(value.replace('_', ' '))\n#             except ValueError: continue\n#     return {attribute: [\"\"] + sorted(list(values)) for attribute, values in options.items()}\n\n# def search_by_text(query, top_k=200):\n#     text_inputs = clip_processor(text=[query], return_tensors=\"pt\").to(DEVICE)\n#     with torch.no_grad():\n#         text_feat = clip_model.get_text_features(**text_inputs)\n#         text_feat = F.normalize(text_feat, dim=1)\n#     sims = cosine_similarity(text_feat.cpu().numpy(), image_embeds.numpy())[0]\n#     top_idx = sims.argsort()[::-1][:top_k]\n#     return [(image_filenames[i], sims[i]) for i in top_idx]\n\n# def search_by_image(query_image, top_k=200):\n#     if query_image is None: return []\n#     inputs = clip_processor(images=query_image, return_tensors=\"pt\").to(DEVICE)\n#     with torch.no_grad():\n#         query_embed = clip_model.get_image_features(**inputs)\n#         query_embed = F.normalize(query_embed, dim=1)\n#     sims = cosine_similarity(query_embed.cpu().numpy(), image_embeds.numpy())[0]\n#     top_idx = sims.argsort()[::-1][:top_k]\n#     return [(image_filenames[i], sims[i]) for i in top_idx]\n\n# def multi_modal_search(query_text, query_image, image_weight=0.5, top_k=200):\n#     text_feat = F.normalize(clip_model.get_text_features(**clip_processor(text=[query_text], return_tensors=\"pt\").to(DEVICE)))\n#     img_feat = F.normalize(clip_model.get_image_features(**clip_processor(images=query_image, return_tensors=\"pt\").to(DEVICE)))\n#     combined_feat = F.normalize((image_weight * img_feat) + ((1.0 - image_weight) * text_feat))\n#     sims = cosine_similarity(combined_feat.cpu().detach().numpy(), image_embeds.numpy())[0]\n#     top_idx = sims.argsort()[::-1][:top_k]\n#     return [(image_filenames[i], sims[i]) for i in top_idx]\n\n# def flexible_ranked_filter(initial_results, filters):\n#     if not any(filters.values()): return initial_results\n#     expected_tags = {f\"{attr}_{val.replace(' ', '_')}\" for attr, val in filters.items() if val}\n#     scored_results = []\n#     for filename, original_score in initial_results:\n#         try:\n#             item_tags = set(tagged_df_indexed.loc[filename, 'tags'])\n#             match_count = len(expected_tags.intersection(item_tags))\n#             if match_count > 0:\n#                 scored_results.append((filename, original_score, match_count))\n#         except KeyError: continue\n#     sorted_results = sorted(scored_results, key=lambda x: (x[2], x[1]), reverse=True)\n#     return [(filename, score) for filename, score, _ in sorted_results]\n\n# def enhance_prompt(base_prompt):\n#     enhanced_positive = f\"masterpiece, photorealistic, high-quality professional product shot of a {base_prompt}, intricate, hyper-detailed, sharp focus, cinematic lighting, 8k uhd, on a mannequin, clean white background\"\n#     negative = \"lowres, blurry, bad anatomy, error, worst quality, jpeg artifacts, ugly, duplicate, morbid, out of frame, watermark, text, signature, person, model\"\n#     return enhanced_positive, negative\n\n# def create_edited_design_enhanced(base_image, mask_image, user_prompt):\n#     enhanced_prompt, negative_prompt = enhance_prompt(user_prompt)\n#     with torch.no_grad():\n#         edited_image = inpaint_pipe(\n#             prompt=enhanced_prompt, image=base_image.resize((512, 512)), \n#             mask_image=mask_image.resize((512, 512)),\n#             negative_prompt=negative_prompt, num_inference_steps=50, guidance_scale=8.5\n#         ).images[0]\n#     return edited_image\n\n# def virtual_try_on_placeholder(person_img, cloth_img):\n#     placeholder = Image.new('RGB', (768, 1024), color = 'white')\n#     draw = ImageDraw.Draw(placeholder)\n#     try: font = ImageFont.truetype(\"LiberationSans-Regular.ttf\", 30)\n#     except IOError: font = ImageFont.load_default()\n#     text = \"Virtual Try-On Output\\n\\nThis is where the magic happens!\\n\\nA VTON model would:\\n1. Parse pose (from OpenPose JSON).\\n2. Segment the body (from Human Parsing).\\n3. Isolate garment (from Clothing Mask).\\n4. Warp clothing onto the person's shape.\\n5. Generate a new, photorealistic image.\"\n#     draw.multiline_text((50, 200), text, fill='black', font=font)\n#     return placeholder\n\n# # =============================================================================\n# # 4. STREAMLIT UI LAYOUT\n# # =============================================================================\n\n# st.title(\"🤖 AI Fashion Stylist Pro\")\n# st.markdown(\"Discover, create, and virtually try on your next favorite outfit.\")\n\n# tab1, tab2, tab3 = st.tabs([\"🔎 Smart Search & Recommendation\", \"🎨 Creative Director\", \"👤 Virtual Try-On Hub\"])\n\n# with tab1:\n#     with st.sidebar:\n#         st.header(\"Search & Filter Controls\")\n#         text_query = st.text_input(\"Text Description\", placeholder=\"e.g., a blue floral blouse\")\n#         image_query_file = st.file_uploader(\"Upload an Image\", type=['png', 'jpg', 'jpeg'])\n#         image_weight = 0.5\n#         if text_query and image_query_file:\n#             image_weight = st.slider(\"Image vs. Text Influence\", 0.0, 1.0, 0.5, 0.1)\n#         st.markdown(\"---\")\n#         st.subheader(\"Smart Tags\")\n#         dropdowns = generate_dropdown_options(tagged_df)\n#         filters = {}\n#         for attr, options in dropdowns.items():\n#             filters[attr] = st.selectbox(attr.capitalize(), options)\n#         search_button = st.button(\"Search & Filter\", type=\"primary\")\n\n#     st.header(\"Your Personalized Recommendations\")\n#     if search_button:\n#         with st.spinner(\"Finding your style...\"):\n#             initial_results = []\n#             image_query = Image.open(image_query_file) if image_query_file else None\n#             if text_query and image_query: initial_results = multi_modal_search(text_query, image_query, image_weight)\n#             elif image_query: initial_results = search_by_image(image_query)\n#             elif text_query: initial_results = search_by_text(text_query)\n#             st.session_state.search_results = flexible_ranked_filter(initial_results, filters)\n\n#     if not st.session_state.search_results and search_button:\n#         st.warning(\"No items matched your query or filters. Please broaden your search.\")\n#     elif not st.session_state.search_results:\n#         st.info(\"Enter a query in the sidebar and click 'Search' to see recommendations.\")\n#     else:\n#         st.success(f\"Found {len(st.session_state.search_results)} matching items. Showing top results.\")\n#         cols = st.columns(6)\n#         for i, (fname, score) in enumerate(st.session_state.search_results[:12]):\n#             image_path = os.path.join(IMAGE_DIR, fname)\n#             if os.path.exists(image_path):\n#                 cols[i % 6].image(image_path, caption=f\"Score: {score:.2f}\", use_column_width=True)\n\n# with tab2:\n#     st.header(\"Unleash Your Inner Designer\")\n#     col1, col2 = st.columns(2)\n#     with col1:\n#         st.subheader(\"1. Upload Base Item & Mask\")\n#         base_img_file = st.file_uploader(\"Base Clothing Image\", type=['png', 'jpg', 'jpeg'], key=\"base\")\n#         mask_img_file = st.file_uploader(\"Mask Image (White area is replaced)\", type=['png', 'jpg', 'jpeg'], key=\"mask\")\n#     with col2:\n#         st.subheader(\"2. Describe Your Idea\")\n#         prompt = st.text_input(\"Prompt\", placeholder=\"e.g., a roaring tiger head\")\n#         generate_button = st.button(\"Generate & Find Similar\", type=\"primary\", key=\"generate\")\n\n#     if generate_button:\n#         if base_img_file and mask_img_file and prompt:\n#             with st.spinner(\"Creating your masterpiece... This can take up to a minute.\"):\n#                 base_img = Image.open(base_img_file).convert(\"RGB\")\n#                 mask_img = Image.open(mask_img_file).convert(\"RGB\")\n#                 st.session_state.generated_design = create_edited_design_enhanced(base_img, mask_img, prompt)\n#                 st.session_state.similar_items = search_by_image(st.session_state.generated_design, top_k=6)\n#         else:\n#             st.error(\"Please provide a base image, a mask image, and a text prompt.\")\n\n#     if st.session_state.generated_design:\n#         st.markdown(\"---\")\n#         st.subheader(\"3. Your Unique Creation & Similar Real Items\")\n#         res_col1, res_col2 = st.columns([1, 2])\n#         with res_col1:\n#             st.image(st.session_state.generated_design, caption=\"Your Generated Design\", use_column_width=True)\n#         with res_col2:\n#             if st.session_state.similar_items:\n#                 cols = st.columns(3)\n#                 for i, (fname, score) in enumerate(st.session_state.similar_items):\n#                     image_path = os.path.join(IMAGE_DIR, fname)\n#                     if os.path.exists(image_path):\n#                         cols[i % 3].image(image_path, use_column_width=True)\n#             else:\n#                 st.info(\"No similar items found in the database.\")\n\n# with tab3:\n#     st.header(\"Experience the Future of Fitting\")\n#     st.info(\"This is a conceptual demonstration of a Virtual Try-On system using your dataset.\")\n#     col1, col2 = st.columns(2)\n#     with col1:\n#         person_img_file = st.file_uploader(\"1. Upload Person Image\", type=['png', 'jpg', 'jpeg'], key=\"vton_p\")\n#     with col2:\n#         cloth_img_file = st.file_uploader(\"2. Upload Clothing Item\", type=['png', 'jpg', 'jpeg'], key=\"vton_c\")\n        \n#     if st.button(\"Generate Virtual Try-On\", type=\"primary\", key=\"vton_gen\"):\n#         if person_img_file and cloth_img_file:\n#             person_img = Image.open(person_img_file)\n#             cloth_img = Image.open(cloth_img_file)\n#             st.image(virtual_try_on_placeholder(person_img, cloth_img), caption=\"VTON Process Simulation\")\n#         else:\n#             st.error(\"Please upload both a person and a clothing image.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ----------------------------------------------------------------------------------\n# # Cell 2: Run the Streamlit Application with ngrok using Threads\n# # ----------------------------------------------------------------------------------\n# import os\n# import threading\n# import time\n# from pyngrok import ngrok\n# from kaggle_secrets import UserSecretsClient\n\n# def run_streamlit():\n#     os.system(\"streamlit run app.py --server.headless true --server.enableCORS false --server.port 8501\")\n\n# print(\"Setting up ngrok tunnel...\")\n# try:\n#     authtoken = UserSecretsClient().get_secret(\"NGROK_AUTH_TOKEN\")\n#     ngrok.set_auth_token(authtoken)\n#     print(\"✅ ngrok authtoken set successfully!\")\n# except Exception as e:\n#     print(f\"⚠️ ngrok authtoken not found. The tunnel will be temporary.\")\n\n# thread = threading.Thread(target=run_streamlit)\n# thread.start()\n# time.sleep(5) \n\n# public_url = ngrok.connect(8501)\n# print(\"🚀 Your Streamlit App is live!\")\n# print(f\"🔗 Public URL: {public_url}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Install a Kaggle-compatible PyTorch first.\n!pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118\n\n# Step 2: Install all other packages, including the new canvas library and compatible versions of the HF ecosystem.\n!pip install streamlit==1.28.2 streamlit-drawable-canvas==0.9.3 pyngrok==7.0.0 transformers==4.34.0 diffusers==0.23.1 accelerate==0.24.1 pandas==2.1.3 scikit-learn==1.3.2 opencv-python-headless==4.8.1.78 peft==0.6.2 --quiet\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !git clone https://github.com/levihsu/OOTDiffusion.git\n\n# # Step 2: Install a Kaggle-compatible PyTorch first.\n# !pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118\n\n# # Step 3: Install all other packages, including OOTDiffusion's specific requirements.\n# !pip install streamlit==1.28.2 streamlit-drawable-canvas==0.9.3 pyngrok==7.0.0 transformers==4.34.0 diffusers==0.23.1 accelerate==0.24.1 pandas==2.1.3 scikit-learn==1.3.2 opencv-python-headless==4.8.1.78 peft==0.6.2 ninja --quiet\n\n# # Step 4: Download the pre-trained models for OOTDiffusion\n# # This will take a significant amount of time.\n# print(\"Downloading OOTDiffusion models...\")\n# # Main VTON Model\n# !wget -O /kaggle/working/OOTDiffusion/models/vton/checkpoint.pth https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/vton/checkpoint.pth\n# # Segmentation Model\n# !wget -O /kaggle/working/OOTDiffusion/models/parsing/79999_iter.pth https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/parsing/79999_iter.pth\n# # OpenPose Model\n# !wget -O /kaggle/working/OOTDiffusion/models/openpose/body_pose_model.pth https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/openpose/body_pose_model.pth\n# print(\"Model downloads complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%writefile app.py\n\n# import streamlit as st\n# import torch\n# import pandas as pd\n# import os\n# import re\n# from PIL import Image, ImageDraw, ImageFont\n# from collections import defaultdict\n# from transformers import CLIPProcessor, CLIPModel\n# from diffusers import StableDiffusionInpaintPipeline\n# from sklearn.metrics.pairwise import cosine_similarity\n# import torch.nn.functional as F\n# from streamlit_drawable_canvas import st_canvas\n# import subprocess # To run the VTON script\n\n# # =============================================================================\n# # 1. PAGE CONFIGURATION & INITIALIZATION\n# # =============================================================================\n# st.set_page_config(page_title=\"AI Fashion Stylist Pro\", page_icon=\"🤖\", layout=\"wide\")\n\n# # Initialize session state for all the variables we need to track\n# for key in ['search_results', 'generated_design', 'similar_items', 'creative_search_results', 'selected_creative_image_fname', 'vton_result_image']:\n#     if key not in st.session_state:\n#         st.session_state[key] = None\n\n# # =============================================================================\n# # 2. MODEL & DATA LOADING (Cached to run only once)\n# # =============================================================================\n# @st.cache_resource\n# def load_all_models_and_data():\n#     \"\"\"Loads all models and data files into memory, cached for performance.\"\"\"\n#     print(\"Executing one-time resource loading...\")\n    \n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#     clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n#     clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n#     inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n#         \"runwayml/stable-diffusion-inpainting\",\n#         torch_dtype=torch.float16,\n#     ).to(device)\n\n#     data_dir = '/kaggle/input/clothe/clothes_tryon_dataset/train/cloth'\n#     embeddings_path = \"/kaggle/input/clip-embed/pytorch/default/1/clip_image_embeds (1).pt\"\n#     tagged_data_path = \"/kaggle/input/tagged-fashion-data/tagged_fashion_data.json\"\n\n#     if not os.path.exists(embeddings_path) or not os.path.exists(tagged_data_path):\n#         st.error(f\"Data files not found in '{data_dir}'. Attach the 'fashion-app-data' Kaggle dataset.\")\n#         st.stop()\n        \n#     embedding_data = torch.load(embeddings_path, map_location='cpu')\n#     image_embeds = embedding_data[\"embeds\"]\n#     image_filenames = embedding_data[\"files\"]\n\n#     tagged_df = pd.read_json(tagged_data_path, orient='records', lines=True)\n#     tagged_df_indexed = tagged_df.set_index('filename')\n#     print(\"Resource loading complete.\")\n#     return device, clip_model, clip_processor, inpaint_pipe, image_embeds, image_filenames, tagged_df, tagged_df_indexed\n\n# DEVICE, clip_model, clip_processor, inpaint_pipe, image_embeds, image_filenames, tagged_df, tagged_df_indexed = load_all_models_and_data()\n\n\n# # --- CRITICAL: Define all required paths for the dataset ---\n# BASE_DATA_PATH = \"/kaggle/input/clothe/clothes_tryon_dataset/train/\"\n# IMAGE_DIR = \"/kaggle/input/clothe/clothes_tryon_dataset/train/cloth\"\n# PERSON_IMAGE_DIR = \"/kaggle/input/clothe/clothes_tryon_dataset/train/image\"\n# CLOTH_MASK_DIR = \"/kaggle/input/clothe/clothes_tryon_dataset/train/cloth-mask\"\n# POSE_JSON_DIR = \"/kaggle/input/clothe/clothes_tryon_dataset/train/openpose_img\"\n\n# for path in [IMAGE_DIR, PERSON_IMAGE_DIR, CLOTH_MASK_DIR, POSE_JSON_DIR]:\n#     if not os.path.exists(path):\n#         st.error(f\"Required directory not found: '{path}'. Please check your dataset paths.\")\n#         st.stop()\n\n# # =============================================================================\n# # 3. BACKEND FUNCTIONS\n# # =============================================================================\n# @st.cache_data\n# def generate_dropdown_options(_df):\n#     options = defaultdict(set)\n#     for tag_list in _df['tags']:\n#         if not isinstance(tag_list, list): continue\n#         for tag in tag_list:\n#             try:\n#                 attribute, value = tag.split('_', 1)\n#                 options[attribute].add(value.replace('_', ' '))\n#             except ValueError: continue\n#     return {attribute: [\"\"] + sorted(list(values)) for attribute, values in options.items()}\n\n# def search_by_text(query, top_k=200):\n#     text_inputs = clip_processor(text=[query], return_tensors=\"pt\").to(DEVICE)\n#     with torch.no_grad():\n#         text_feat = clip_model.get_text_features(**text_inputs)\n#         text_feat = F.normalize(text_feat, dim=1)\n#     sims = cosine_similarity(text_feat.cpu().numpy(), image_embeds.numpy())[0]\n#     top_idx = sims.argsort()[::-1][:top_k]\n#     return [(image_filenames[i], sims[i]) for i in top_idx]\n\n# def search_by_image(query_image, top_k=200):\n#     if query_image is None: return []\n#     inputs = clip_processor(images=query_image, return_tensors=\"pt\").to(DEVICE)\n#     with torch.no_grad():\n#         query_embed = clip_model.get_image_features(**inputs)\n#         query_embed = F.normalize(query_embed, dim=1)\n#     sims = cosine_similarity(query_embed.cpu().numpy(), image_embeds.numpy())[0]\n#     top_idx = sims.argsort()[::-1][:top_k]\n#     return [(image_filenames[i], sims[i]) for i in top_idx]\n\n# def multi_modal_search(query_text, query_image, image_weight=0.5, top_k=200):\n#     text_feat = F.normalize(clip_model.get_text_features(**clip_processor(text=[query_text], return_tensors=\"pt\").to(DEVICE)))\n#     img_feat = F.normalize(clip_model.get_image_features(**clip_processor(images=query_image, return_tensors=\"pt\").to(DEVICE)))\n#     combined_feat = F.normalize((image_weight * img_feat) + ((1.0 - image_weight) * text_feat))\n#     sims = cosine_similarity(combined_feat.cpu().detach().numpy(), image_embeds.numpy())[0]\n#     top_idx = sims.argsort()[::-1][:top_k]\n#     return [(image_filenames[i], sims[i]) for i in top_idx]\n\n# def flexible_ranked_filter(initial_results, filters):\n#     if not any(filters.values()): return initial_results\n#     expected_tags = {f\"{attr}_{val.replace(' ', '_')}\" for attr, val in filters.items() if val}\n#     scored_results = []\n#     for filename, original_score in initial_results:\n#         try:\n#             item_tags = set(tagged_df_indexed.loc[filename, 'tags'])\n#             match_count = len(expected_tags.intersection(item_tags))\n#             if match_count > 0:\n#                 scored_results.append((filename, original_score, match_count))\n#         except KeyError: continue\n#     sorted_results = sorted(scored_results, key=lambda x: (x[2], x[1]), reverse=True)\n#     return [(filename, score) for filename, score, _ in sorted_results]\n\n# def enhance_prompt(base_prompt):\n#     enhanced_positive = f\"masterpiece, photorealistic, high-quality professional product shot of a {base_prompt}, intricate, hyper-detailed, sharp focus, cinematic lighting, 8k uhd, on a mannequin, clean white background\"\n#     negative = \"lowres, blurry, bad anatomy, error, worst quality, jpeg artifacts, ugly, duplicate, morbid, out of frame, watermark, text, signature, person, model\"\n#     return enhanced_positive, negative\n\n# def create_edited_design_enhanced(base_image, mask_image, user_prompt):\n#     enhanced_prompt, negative_prompt = enhance_prompt(user_prompt)\n#     with torch.no_grad():\n#         edited_image = inpaint_pipe(\n#             prompt=enhanced_prompt, image=base_image.resize((512, 512)), \n#             mask_image=mask_image.resize((512, 512)),\n#             negative_prompt=negative_prompt, num_inference_steps=50, guidance_scale=8.5\n#         ).images[0]\n#     return edited_image\n\n# # --- The REAL VTON Backend Function ---\n# def run_virtual_tryon(person_fname, cloth_fname):\n#     \"\"\"Constructs and runs the OOTDiffusion command with the correct working directory.\"\"\"\n#     person_img_src = os.path.join(PERSON_IMAGE_DIR, person_fname)\n#     cloth_img_src = os.path.join(IMAGE_DIR, cloth_fname)\n    \n#     session_id = f\"{os.path.splitext(person_fname)[0]}_{os.path.splitext(cloth_fname)[0]}\"\n#     output_dir = os.path.join(\"/kaggle/working/OOTDiffusion/results/\", session_id)\n#     os.makedirs(output_dir, exist_ok=True)\n    \n#     # --- KEY CHANGE 1: The script path is now relative ---\n#     # We are running 'python' on 'run_oot.py' from *within* its own directory.\n#     command = [\n#         \"python\", \"run_oot.py\",\n#         \"--model_path\", person_img_src,\n#         \"--cloth_path\", cloth_img_src,\n#         \"--model_type\", \"hd\",\n#         \"--category\", \"upperbody\",\n#         \"--output_dir\", output_dir\n#     ]\n    \n#     # --- KEY CHANGE 2: Set the Current Working Directory (cwd) ---\n#     # This tells the subprocess to execute as if it were in the OOTDiffusion folder.\n#     ootd_working_dir = \"/kaggle/working/OOTDiffusion/\"\n    \n#     try:\n#         process = subprocess.run(command, check=True, capture_output=True, text=True, cwd=ootd_working_dir)\n#         print(\"OOTDiffusion STDOUT:\", process.stdout)\n#         print(\"OOTDiffusion STDERR:\", process.stderr)\n\n#         expected_output_fname = f\"{os.path.splitext(os.path.basename(person_img_src))[0]}_{os.path.splitext(os.path.basename(cloth_img_src))[0]}.png\"\n#         output_path = os.path.join(output_dir, expected_output_fname)\n\n#         if os.path.exists(output_path):\n#             return output_path\n#         else:\n#             for file in os.listdir(output_dir):\n#                 if file.endswith(\".png\"): return os.path.join(output_dir, file)\n#             return None\n            \n#     except subprocess.CalledProcessError as e:\n#         print(f\"OOTDiffusion script failed. Error: {e.stderr}\")\n#         st.error(f\"Virtual Try-On failed. Error: {e.stderr}\")\n#         return None\n# # =============================================================================\n# # 4. STREAMLIT UI LAYOUT\n# # =============================================================================\n\n# st.title(\"🤖 AI Fashion Stylist Pro\")\n# st.markdown(\"Discover, create, and virtually try on your next favorite outfit.\")\n\n# tab1, tab2, tab3 = st.tabs([\"🔎 Smart Search & Recommendation\", \"🎨 Creative Director\", \"👤 Virtual Try-On Hub\"])\n\n# with tab1:\n#     with st.sidebar:\n#         st.header(\"Search & Filter Controls\")\n#         text_query = st.text_input(\"Text Description\", placeholder=\"e.g., a blue floral blouse\")\n#         image_query_file = st.file_uploader(\"Upload an Image\", type=['png', 'jpg', 'jpeg'])\n#         image_weight = 0.5\n#         if text_query and image_query_file:\n#             image_weight = st.slider(\"Image vs. Text Influence\", 0.0, 1.0, 0.5, 0.1)\n#         st.markdown(\"---\")\n#         st.subheader(\"Smart Tags\")\n#         dropdowns = generate_dropdown_options(tagged_df)\n#         filters = {}\n#         for attr, options in dropdowns.items():\n#             filters[attr] = st.selectbox(attr.capitalize(), options)\n#         search_button = st.button(\"Search & Filter\", type=\"primary\")\n\n#     st.header(\"Your Personalized Recommendations\")\n#     if search_button:\n#         with st.spinner(\"Finding your style...\"):\n#             initial_results = []\n#             image_query = Image.open(image_query_file) if image_query_file else None\n#             if text_query and image_query: initial_results = multi_modal_search(text_query, image_query, image_weight)\n#             elif image_query: initial_results = search_by_image(image_query)\n#             elif text_query: initial_results = search_by_text(text_query)\n#             st.session_state.search_results = flexible_ranked_filter(initial_results, filters)\n\n#     if not st.session_state.search_results and search_button:\n#         st.warning(\"No items matched your query or filters. Please broaden your search.\")\n#     elif not st.session_state.search_results:\n#         st.info(\"Enter a query in the sidebar and click 'Search' to see recommendations.\")\n#     else:\n#         st.success(f\"Found {len(st.session_state.search_results)} matching items. Showing top results.\")\n#         cols = st.columns(6)\n#         for i, (fname, score) in enumerate(st.session_state.search_results[:12]):\n#             image_path = os.path.join(IMAGE_DIR, fname)\n#             if os.path.exists(image_path):\n#                 cols[i % 6].image(image_path, caption=f\"Score: {score:.2f}\", use_column_width=True)\n\n# with tab2:\n#     st.header(\"Unleash Your Inner Designer\")\n#     st.subheader(\"1. Find a Base Item to Edit\")\n#     creative_search_term = st.text_input(\"Search for a clothing item\", key=\"creative_search\")\n#     if st.button(\"Search\", key=\"creative_search_btn\"):\n#         if creative_search_term:\n#             with st.spinner(\"Searching...\"):\n#                 st.session_state.creative_search_results = search_by_text(creative_search_term, top_k=6)\n#         else:\n#             st.session_state.creative_search_results = None\n\n#     if st.session_state.creative_search_results:\n#         st.markdown(\"---\")\n#         st.write(\"Click 'Select' on an item to start editing:\")\n#         cols = st.columns(6)\n#         for i, (fname, score) in enumerate(st.session_state.creative_search_results):\n#             image_path = os.path.join(IMAGE_DIR, fname)\n#             if os.path.exists(image_path):\n#                 cols[i].image(image_path, use_column_width=True)\n#                 if cols[i].button(\"Select\", key=f\"select_{fname}\"):\n#                     st.session_state.selected_creative_image_fname = fname\n#                     st.session_state.generated_design = None\n#                     st.session_state.similar_items = None\n#                     st.experimental_rerun()\n    \n#     st.markdown(\"---\")\n#     if st.session_state.selected_creative_image_fname:\n#         st.subheader(\"2. Paint Over the Area to Change & Describe Your Idea\")\n#         base_image_path = os.path.join(IMAGE_DIR, st.session_state.selected_creative_image_fname)\n#         base_img = Image.open(base_image_path).convert(\"RGB\")\n#         col1, col2 = st.columns(2)\n#         with col1:\n#             st.write(\"Use the thick brush to paint the area you want to change.\")\n#             canvas_result = st_canvas(\n#                 fill_color=\"rgba(255, 255, 255, 0)\", stroke_width=35,\n#                 stroke_color=\"rgba(255, 0, 0, 0.5)\", background_image=base_img.resize((512, 512)),\n#                 update_streamlit=True, height=512, width=512, drawing_mode=\"freedraw\", key=\"canvas\",\n#             )\n#         with col2:\n#             prompt = st.text_input(\"What should the new design be?\", placeholder=\"e.g., a roaring tiger head\")\n#             generate_button = st.button(\"Generate & Find Similar\", type=\"primary\", key=\"generate\")\n#             if generate_button:\n#                 if canvas_result.image_data is not None and prompt:\n#                     mask_array = canvas_result.image_data[:, :, 3] > 0\n#                     if mask_array.sum() > 0:\n#                         mask_img = Image.fromarray(mask_array.astype('uint8') * 255)\n#                         with st.spinner(\"Creating your masterpiece...\"):\n#                             st.session_state.generated_design = create_edited_design_enhanced(base_img, mask_img, prompt)\n#                             st.session_state.similar_items = search_by_image(st.session_state.generated_design, top_k=6)\n#                     else: st.warning(\"Please paint on the image to indicate the area to change.\")\n#                 else: st.error(\"Please paint on the image and provide a text prompt.\")\n\n#     if st.session_state.generated_design:\n#         st.markdown(\"---\"); st.subheader(\"3. Your Unique Creation & Similar Real Items\")\n#         res_col1, res_col2 = st.columns([1, 2])\n#         with res_col1: st.image(st.session_state.generated_design, caption=\"Your Generated Design\", use_column_width=True)\n#         with res_col2:\n#             if st.session_state.similar_items:\n#                 cols = st.columns(3)\n#                 for i, (fname, score) in enumerate(st.session_state.similar_items):\n#                     image_path = os.path.join(IMAGE_DIR, fname)\n#                     if os.path.exists(image_path): cols[i % 3].image(image_path, use_column_width=True)\n#             else: st.info(\"No similar items found in the database.\")\n\n\n# # ### CORRECTED VIRTUAL TRY-ON TAB ###\n# with tab3:\n#     st.header(\"✨ The Magic Mirror: Virtual Try-On\")\n#     st.info(\"Select a person and a clothing item from the dataset to see the try-on result. Generation can take 1-2 minutes.\")\n\n#     # Get a sample of available person and clothing images for the dropdowns\n#     person_files = sorted(os.listdir(PERSON_IMAGE_DIR))[:200]\n#     cloth_files = sorted(os.listdir(IMAGE_DIR))[:200]\n\n#     col1, col2 = st.columns(2)\n#     with col1:\n#         selected_person = st.selectbox(\"1. Choose a Person Model\", person_files)\n#         if selected_person:\n#             st.image(os.path.join(PERSON_IMAGE_DIR, selected_person), use_column_width=True)\n    \n#     with col2:\n#         selected_cloth = st.selectbox(\"2. Choose a Clothing Item\", cloth_files)\n#         if selected_cloth:\n#             st.image(os.path.join(IMAGE_DIR, selected_cloth), use_column_width=True)\n            \n#     if st.button(\"Generate Virtual Try-On\", type=\"primary\", key=\"vton_gen\"):\n#         if selected_person and selected_cloth:\n#             with st.spinner(\"Warming up the Magic Mirror... This will take a moment.\"):\n#                 # OOTDiffusion automatically finds the corresponding mask and pose files based on filename conventions\n#                 # So we just need to run the main function\n#                 result_path = run_virtual_tryon(selected_person, selected_cloth)\n                \n#                 if result_path and os.path.exists(result_path):\n#                     st.session_state.vton_result_image = Image.open(result_path)\n#                 else:\n#                     st.session_state.vton_result_image = None\n#                     st.error(\"VTON generation failed or output file not found. Check notebook logs.\")\n#         else:\n#             st.error(\"Please select both a person and a clothing item.\")\n            \n#     if st.session_state.vton_result_image:\n#         st.markdown(\"---\")\n#         st.subheader(\"🎉 Your Virtual Try-On Result\")\n#         st.image(st.session_state.vton_result_image, use_column_width=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ----------------------------------------------------------------------------------\n# # Cell 2: Run the Streamlit Application with ngrok using Threads\n# # ----------------------------------------------------------------------------------\n# import os\n# import threading\n# import time\n# from pyngrok import ngrok\n# from kaggle_secrets import UserSecretsClient\n\n# # --- Function to run Streamlit in a thread ---\n# def run_streamlit():\n#     \"\"\"This function runs the streamlit command in the shell.\"\"\"\n#     os.system(\"streamlit run app.py --server.headless true --server.enableCORS false --server.port 8501\")\n\n# # --- Setup ngrok ---\n# print(\"Setting up ngrok tunnel...\")\n# # Authenticate ngrok using the secret you added\n# try:\n#     authtoken = UserSecretsClient().get_secret(\"NGROK_AUTH_TOKEN\")\n#     ngrok.set_auth_token(authtoken)\n#     print(\"✅ ngrok authtoken set successfully!\")\n# except Exception as e:\n#     print(f\"⚠️ ngrok authtoken not found in Kaggle Secrets. The tunnel will be temporary. Error: {e}\")\n#     print(\"Create a free ngrok account and add your authtoken as a secret named NGROK_AUTH_TOKEN for longer sessions.\")\n\n# # --- Start Streamlit in a separate thread ---\n# thread = threading.Thread(target=run_streamlit)\n# thread.start()\n\n# # Give the Streamlit server a moment to start up\n# time.sleep(5) \n\n# # --- Open the ngrok tunnel to the Streamlit port (8501) ---\n# public_url = ngrok.connect(8501)\n# print(\"🚀 Your Streamlit App is live!\")\n# print(f\"🔗 Public URL: {public_url}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------------------------------------------------------------\n# Cell 1: Install All Dependencies and Write the Final Streamlit App File\n# ----------------------------------------------------------------------------------\n\n# Step 1: Install a Kaggle-compatible PyTorch first.\n!pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118\n\n# Step 2: Install all other packages with compatible versions.\n!pip install streamlit==1.28.2 streamlit-drawable-canvas==0.9.3 pyngrok==7.0.0 transformers==4.34.0 diffusers==0.23.1 accelerate==0.24.1 pandas==2.1.3 scikit-learn==1.3.2 opencv-python-headless==4.8.1.78 peft==0.6.2 ninja --quiet\n\n# # Step 3: Clone the OOTDiffusion repository if it doesn't exist.\n# !git clone https://github.com/levihsu/OOTDiffusion.git /kaggle/working/OOTDiffusion\n\n# # Step 4: Download the pre-trained models for OOTDiffusion.\n# print(\"Downloading OOTDiffusion models...\")\n# !wget -nc -O /kaggle/working/OOTDiffusion/models/vton/checkpoint.pth https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/vton/checkpoint.pth\n# !wget -nc -O /kaggle/working/OOTDiffusion/models/parsing/79999_iter.pth https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/parsing/79999_iter.pth\n# !wget -nc -O /kaggle/working/OOTDiffusion/models/openpose/body_pose_model.pth https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/openpose/body_pose_model.pth\n# print(\"Model downloads complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile app.py\n\nimport streamlit as st\nimport torch\nimport pandas as pd\nimport os\nimport re\nfrom PIL import Image, ImageDraw, ImageFont\nfrom collections import defaultdict\nfrom transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\nfrom diffusers import StableDiffusionInpaintPipeline\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch.nn.functional as F\nfrom streamlit_drawable_canvas import st_canvas\nimport subprocess\nimport numpy as np\n\n# =============================================================================\n# 1. PAGE CONFIGURATION & INITIALIZATION\n# =============================================================================\nst.set_page_config(\n    page_title=\"AI Fashion Stylist Pro\",\n    page_icon=\"🤖\",\n    layout=\"wide\"\n)\n\n# Initialize session state\nfor key in ['search_results', 'generated_design', 'similar_items', 'creative_search_results', \n            'selected_creative_image_fname', 'vton_result_image', 'sketch_results', \n            'sketch_caption', 'uploaded_sketch', 'cart', 'items_to_show', 'search_active']:\n    if key not in st.session_state:\n        st.session_state[key] = [] if key == 'cart' else False if key == 'search_active' else 12 if key == 'items_to_show' else None\n\n# =============================================================================\n# 2. MODEL & DATA LOADING\n# =============================================================================\n@st.cache_resource\ndef load_all_models_and_data():\n    print(\"Executing one-time resource loading...\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n    blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n    inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(\"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16).to(device)\n    \n    data_dir = '/kaggle/input/clothe/clothes_tryon_dataset/train/cloth'\n    embeddings_path = \"/kaggle/input/clip-embed/pytorch/default/1/clip_image_embeds (1).pt\"\n    tagged_data_path = \"/kaggle/input/tagged-fashion-data/tagged_fashion_data.json\"\n    \n    embedding_data = torch.load(embeddings_path, map_location='cpu')\n    image_embeds = embedding_data[\"embeds\"].numpy()\n    image_filenames = embedding_data[\"files\"]\n    tagged_df = pd.read_json(tagged_data_path, orient='records', lines=True)\n    tagged_df_indexed = tagged_df.set_index('filename')\n    \n    print(\"Resource loading complete.\")\n    return device, clip_model, clip_processor, blip_model, blip_processor, inpaint_pipe, image_embeds, image_filenames, tagged_df, tagged_df_indexed\n\nDEVICE, clip_model, clip_processor, blip_model, blip_processor, inpaint_pipe, image_embeds, image_filenames, tagged_df, tagged_df_indexed = load_all_models_and_data()\n\nBASE_DATA_PATH = \"/kaggle/input/clothe/clothes_tryon_dataset/train/\"\nIMAGE_DIR = os.path.join(BASE_DATA_PATH, \"cloth/\")\nPERSON_IMAGE_DIR = os.path.join(BASE_DATA_PATH, \"image/\")\n\n# =============================================================================\n# 3. BACKEND FUNCTIONS\n# =============================================================================\n@st.cache_data\ndef generate_dropdown_options(_df):\n    options = defaultdict(set)\n    for attr in ['brand', 'color', 'pattern', 'style', 'neckline']:\n        options[attr].add(\"Any\")\n    for tag_list in _df['tags']:\n        if not isinstance(tag_list, list): continue\n        for tag in tag_list:\n            try:\n                attribute, value = tag.split('_', 1)\n                options[attribute].add(value.replace('_', ' '))\n            except ValueError: continue\n    return {attribute: sorted(list(values)) for attribute, values in options.items()}\n\ndef search_by_text(query, top_k=500):\n    text_inputs = clip_processor(text=[query], return_tensors=\"pt\").to(DEVICE)\n    with torch.no_grad():\n        text_feat = clip_model.get_text_features(**text_inputs)\n        text_feat = F.normalize(text_feat, dim=1)\n    sims = cosine_similarity(text_feat.cpu().numpy(), image_embeds)[0]\n    top_idx = np.argsort(-sims)[:top_k]\n    return [(image_filenames[i], sims[i]) for i in top_idx]\n\ndef search_by_image(query_image, top_k=500):\n    if query_image is None: return []\n    inputs = clip_processor(images=query_image, return_tensors=\"pt\").to(DEVICE)\n    with torch.no_grad():\n        query_embed = clip_model.get_image_features(**inputs)\n        query_embed = F.normalize(query_embed, dim=1)\n    sims = cosine_similarity(query_embed.cpu().numpy(), image_embeds)[0]\n    top_idx = np.argsort(-sims)[:top_k]\n    return [(image_filenames[i], sims[i]) for i in top_idx]\n\ndef multi_modal_search(query_text, query_image, image_weight=0.5, top_k=500):\n    text_feat = F.normalize(clip_model.get_text_features(**clip_processor(text=[query_text], return_tensors=\"pt\").to(DEVICE)))\n    img_feat = F.normalize(clip_model.get_image_features(**clip_processor(images=query_image, return_tensors=\"pt\").to(DEVICE)))\n    combined_feat = F.normalize((image_weight * img_feat) + ((1.0 - image_weight) * text_feat))\n    sims = cosine_similarity(combined_feat.cpu().detach().numpy(), image_embeds)[0]\n    top_idx = np.argsort(-sims)[:top_k]\n    return [(image_filenames[i], sims[i]) for i in top_idx]\n\n# --- CORRECTED flexible_ranked_filter ---\ndef flexible_ranked_filter(initial_results, filters):\n    # If no filters are selected (all are \"Any\"), just return the initial results\n    if not filters: \n        return initial_results\n        \n    expected_tags = {f\"{attr}_{val.replace(' ', '_')}\" for attr, val in filters.items()}\n    scored_results = []\n    \n    for filename, original_score in initial_results:\n        try:\n            item_tags = set(tagged_df_indexed.loc[filename, 'tags'])\n            match_count = len(expected_tags.intersection(item_tags))\n            if match_count > 0:\n                scored_results.append((filename, original_score, match_count))\n        except KeyError: \n            continue\n            \n    sorted_results = sorted(scored_results, key=lambda x: (x[2], x[1]), reverse=True)\n    return [(filename, score) for filename, score, _ in sorted_results]\n\ndef add_to_cart(item_filename):\n    if item_filename not in st.session_state.cart:\n        st.session_state.cart.append(item_filename)\n\ndef remove_from_cart(item_filename):\n    if item_filename in st.session_state.cart:\n        st.session_state.cart.remove(item_filename)\n\ndef clear_cart():\n    st.session_state.cart = []\n\ndef load_more_items():\n    st.session_state.items_to_show += 12\n\ndef handle_search(text_query, image_query_file, image_weight, filters):\n    st.session_state.search_active = True\n    initial_results = []\n    image_query = Image.open(image_query_file) if image_query_file else None\n\n    # Determine the initial set of candidates based on the search query\n    if text_query and image_query:\n        initial_results = multi_modal_search(text_query, image_query, image_weight)\n    elif image_query:\n        initial_results = search_by_image(image_query)\n    elif text_query:\n        initial_results = search_by_text(text_query)\n    else:\n        # If no search query, the initial pool is all items, allowing for filtering the entire catalog\n        initial_results = [(fname, 1.0) for fname in image_filenames]\n    \n    # Apply the flexible ranked filter to the initial results\n    st.session_state.search_results = flexible_ranked_filter(initial_results, filters)\n\ndef clear_search():\n    st.session_state.search_active = False\n    st.session_state.search_results = []\n    st.session_state.items_to_show = 12\n\n# (Other backend functions for other tabs remain the same)\n# ...\n\n# =============================================================================\n# 4. STREAMLIT UI LAYOUT\n# =============================================================================\n\nst.title(\"🤖 AI Fashion Stylist Pro\")\nst.markdown(\"Discover, create, and virtually try on your next favorite outfit.\")\n\ntab1, tab2, tab3, tab4 = st.tabs([\"🛍️ Browse & Search\", \"🎨 Creative Director\", \"👤 Virtual Try-On\", \"✍️ Sketch to Fashion\"])\n\nwith tab1:\n    with st.sidebar:\n        st.header(\"Search & Filter\")\n        text_query = st.text_input(\"Text Description\", placeholder=\"e.g., a blue floral blouse\")\n        image_query_file = st.file_uploader(\"Upload an Image\", type=['png', 'jpg', 'jpeg'], key=\"main_uploader\")\n        image_weight = 0.5\n        if text_query and image_query_file:\n            image_weight = st.slider(\"Image vs. Text Influence\", 0.0, 1.0, 0.5, 0.1)\n        \n        st.subheader(\"Smart Tags\")\n        dropdowns = generate_dropdown_options(tagged_df)\n        filters = {}\n        for attr, options in dropdowns.items():\n            selected = st.selectbox(attr.capitalize(), options, index=0)\n            # Only add the filter if a specific option (not \"Any\") is chosen\n            if selected != \"Any\":\n                filters[attr] = selected\n        \n        col1, col2 = st.columns(2)\n        with col1:\n            st.button(\"Search / Filter\", on_click=handle_search, args=(text_query, image_query_file, image_weight, filters), type=\"primary\")\n        with col2:\n            st.button(\"Browse All\", on_click=clear_search)\n\n        st.markdown(\"---\")\n        st.header(\"Shopping Cart\")\n        if not st.session_state.cart:\n            st.info(\"Your cart is empty.\")\n        else:\n            for item_fname in st.session_state.cart:\n                cart_col1, cart_col2 = st.columns([1, 3])\n                with cart_col1:\n                    st.image(os.path.join(IMAGE_DIR, item_fname), use_column_width=True)\n                with cart_col2:\n                    st.write(f\"**{item_fname}**\")\n                    st.button(\"Remove\", key=f\"remove_{item_fname}\", on_click=remove_from_cart, args=(item_fname,))\n            st.markdown(\"---\")\n            if st.button(\"Clear Cart\"):\n                clear_cart()\n                st.experimental_rerun()\n\n    # --- Main Display Area Logic ---\n    if st.session_state.search_active:\n        st.header(\"Search Results\")\n        display_items = st.session_state.search_results\n        if not display_items:\n            st.warning(\"Your search returned no results. Try broadening your query or click 'Browse All'.\")\n    else:\n        st.header(\"Browse Our Collection\")\n        display_items = [(fname, None) for fname in image_filenames]\n    \n    if display_items:\n        items_to_display_now = display_items[:st.session_state.items_to_show]\n        num_cols = 6\n        cols = st.columns(num_cols)\n        for i, (fname, score) in enumerate(items_to_display_now):\n            image_path = os.path.join(IMAGE_DIR, fname)\n            if os.path.exists(image_path):\n                with cols[i % num_cols]:\n                    st.image(image_path, use_column_width=True)\n                    st.button(\"Add to Cart\", key=f\"add_{fname}\", on_click=add_to_cart, args=(fname,))\n        \n        if not st.session_state.search_active and st.session_state.items_to_show < len(display_items):\n            st.button(\"Load More\", on_click=load_more_items, use_container_width=True)\n\n\n\nwith tab2:\n    st.header(\"Unleash Your Inner Designer\")\n    col1, col2 = st.columns(2)\n    with col1:\n        st.subheader(\"1. Upload Base Item & Mask\")\n        base_img_file = st.file_uploader(\"Base Clothing Image\", type=['png', 'jpg', 'jpeg'], key=\"base\")\n        mask_img_file = st.file_uploader(\"Mask Image (White area is replaced)\", type=['png', 'jpg', 'jpeg'], key=\"mask\")\n    with col2:\n        st.subheader(\"2. Describe Your Idea\")\n        prompt = st.text_input(\"Prompt\", placeholder=\"e.g., a roaring tiger head\")\n        generate_button = st.button(\"Generate & Find Similar\", type=\"primary\", key=\"generate\")\n\n    if generate_button:\n        if base_img_file and mask_img_file and prompt:\n            with st.spinner(\"Creating your masterpiece... This can take up to a minute.\"):\n                base_img = Image.open(base_img_file).convert(\"RGB\")\n                mask_img = Image.open(mask_img_file).convert(\"RGB\")\n                st.session_state.generated_design = create_edited_design_enhanced(base_img, mask_img, prompt)\n                st.session_state.similar_items = search_by_image(st.session_state.generated_design, top_k=6)\n        else:\n            st.error(\"Please provide a base image, a mask image, and a text prompt.\")\n\n    if st.session_state.generated_design:\n        st.markdown(\"---\")\n        st.subheader(\"3. Your Unique Creation & Similar Real Items\")\n        res_col1, res_col2 = st.columns([1, 2])\n        with res_col1:\n            st.image(st.session_state.generated_design, caption=\"Your Generated Design\", use_column_width=True)\n        with res_col2:\n            if st.session_state.similar_items:\n                cols = st.columns(3)\n                for i, (fname, score) in enumerate(st.session_state.similar_items):\n                    image_path = os.path.join(IMAGE_DIR, fname)\n                    if os.path.exists(image_path):\n                        cols[i % 3].image(image_path, use_column_width=True)\n            else:\n                st.info(\"No similar items found in the database.\")\n\nwith tab3:\n    st.header(\"Experience the Future of Fitting\")\n    st.info(\"This is a conceptual demonstration of a Virtual Try-On system using your dataset.\")\n    col1, col2 = st.columns(2)\n    with col1:\n        person_img_file = st.file_uploader(\"1. Upload Person Image\", type=['png', 'jpg', 'jpeg'], key=\"vton_p\")\n    with col2:\n        cloth_img_file = st.file_uploader(\"2. Upload Clothing Item\", type=['png', 'jpg', 'jpeg'], key=\"vton_c\")\n        \n    if st.button(\"Generate Virtual Try-On\", type=\"primary\", key=\"vton_gen\"):\n        if person_img_file and cloth_img_file:\n            person_img = Image.open(person_img_file)\n            cloth_img = Image.open(cloth_img_file)\n            st.image(virtual_try_on_placeholder(person_img, cloth_img), caption=\"VTON Process Simulation\")\n        else:\n            st.error(\"Please upload both a person and a clothing image.\")\n\nwith tab4:\n    st.header(\"Sketch-Based Discovery 🖊️\")\n    sketch_file = st.file_uploader(\"Upload Sketch Image\", type=['png', 'jpg', 'jpeg'], key=\"sketch_uploader\")\n    \n    if sketch_file:\n        sketch = Image.open(sketch_file).convert(\"RGB\").resize((224, 224))\n        st.image(sketch, caption=\"Uploaded Sketch\",use_column_width=False)\n        \n        if st.button(\"Show Recommendations\", type=\"primary\"):\n            with st.spinner(\"🧠 Describing your sketch and searching...\"):\n                caption = describe_sketch(sketch)\n                sketch_embed = hybrid_clip_embedding(sketch, caption)\n                sims = image_embeds.numpy() @ sketch_embed\n                topk = np.argsort(-sims)[:6]\n\n                cols = st.columns(6)\n                for i, idx in enumerate(topk):\n                    img_path = os.path.join(IMAGE_DIR, image_filenames[idx])\n                    if os.path.exists(img_path):\n                        cols[i].image(img_path, caption=f\"Score: {sims[idx]:.2f}\", use_column_width=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------------------------------------------------------------\n# Cell 2: Run the Streamlit Application with ngrok using Threads\n# ----------------------------------------------------------------------------------\nimport os\nimport threading\nimport time\nfrom pyngrok import ngrok\nfrom kaggle_secrets import UserSecretsClient\n\ndef run_streamlit():\n    os.system(\"streamlit run app.py --server.headless true --server.enableCORS false --server.port 8501\")\n\nprint(\"Setting up ngrok tunnel...\")\ntry:\n    authtoken = UserSecretsClient().get_secret(\"NGROK_AUTH_TOKEN\")\n    ngrok.set_auth_token(authtoken)\n    print(\"✅ ngrok authtoken set successfully!\")\nexcept Exception as e:\n    print(f\"⚠️ ngrok authtoken not found. The tunnel will be temporary.\")\n\nfor tunnel in ngrok.get_tunnels():\n    ngrok.disconnect(tunnel.public_url)\n    print(f\"🔌 Disconnected old tunnel: {tunnel.public_url}\")\n\nthread = threading.Thread(target=run_streamlit)\nthread.start()\ntime.sleep(5) \n\npublic_url = ngrok.connect(8501)\nprint(\"🚀 Your Streamlit App is live!\")\nprint(f\"🔗 Public URL: {public_url}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:59:37.960508Z","iopub.execute_input":"2025-07-21T16:59:37.961014Z","iopub.status.idle":"2025-07-21T16:59:43.243896Z","shell.execute_reply.started":"2025-07-21T16:59:37.960987Z","shell.execute_reply":"2025-07-21T16:59:43.243119Z"}},"outputs":[{"name":"stdout","text":"Setting up ngrok tunnel...\n✅ ngrok authtoken set successfully!\n🔌 Disconnected old tunnel: https://3d240b3dcfb2.ngrok-free.app\n","output_type":"stream"},{"name":"stderr","text":"2025-07-21 16:59:39.073 \nWarning: the config option 'server.enableCORS=false' is not compatible with 'server.enableXsrfProtection=true'.\nAs a result, 'server.enableCORS' is being overridden to 'true'.\n\nMore information:\nIn order to protect against CSRF attacks, we send a cookie with each request.\nTo do so, we must specify allowable origins, which places a restriction on\ncross-origin resource sharing.\n\nIf cross origin resource sharing is required, please disable server.enableXsrfProtection.\n            \n2025-07-21 16:59:39.176 Port 8501 is already in use\n","output_type":"stream"},{"name":"stdout","text":"\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n\n🚀 Your Streamlit App is live!\n🔗 Public URL: NgrokTunnel: \"https://08207d102df3.ngrok-free.app\" -> \"http://localhost:8501\"\n","output_type":"stream"},{"name":"stderr","text":"2025-07-21 17:01:47.974 Please replace `st.experimental_rerun` with `st.rerun`.\n\n`st.experimental_rerun` will be removed after 2024-04-01.\n100%|██████████| 50/50 [00:10<00:00,  4.97it/s]\n2025-07-21 17:04:37.814 Please replace `st.experimental_rerun` with `st.rerun`.\n\n`st.experimental_rerun` will be removed after 2024-04-01.\n100%|██████████| 50/50 [00:10<00:00,  4.96it/s]\n100%|██████████| 50/50 [00:09<00:00,  5.03it/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile app.py\n\nimport streamlit as st\nimport torch\nimport pandas as pd\nimport os\nimport re\nfrom PIL import Image, ImageDraw, ImageFont\nfrom collections import defaultdict\nfrom transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\nfrom diffusers import StableDiffusionInpaintPipeline\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch.nn.functional as F\nfrom streamlit_drawable_canvas import st_canvas\nimport subprocess\nimport numpy as np\n\n# =============================================================================\n# 1. PAGE CONFIGURATION & INITIALIZATION\n# =============================================================================\nst.set_page_config(\n    page_title=\"AI Fashion Stylist Pro\",\n    page_icon=\"🤖\",\n    layout=\"wide\"\n)\n\n# Initialize session state for all the variables we need to track\nfor key in ['search_results', 'generated_design', 'similar_items', 'creative_search_results', \n            'selected_creative_image_fname', 'vton_result_image', 'sketch_results', \n            'sketch_caption', 'uploaded_sketch', 'cart', 'items_to_show', 'search_active']:\n    if key not in st.session_state:\n        st.session_state[key] = [] if key == 'cart' else False if key == 'search_active' else 12 if key == 'items_to_show' else None\n\n# =============================================================================\n# 2. MODEL & DATA LOADING (Cached to run only once)\n# =============================================================================\n@st.cache_resource\ndef load_all_models_and_data():\n    \"\"\"Loads all models and data files into memory, cached for performance.\"\"\"\n    print(\"Executing one-time resource loading...\")\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n    \n    blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n\n    inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-inpainting\",\n        torch_dtype=torch.float16,\n    ).to(device)\n\n    # Use the correct, verified paths from our previous debugging sessions\n    data_dir = '/kaggle/input/clothe/clothes_tryon_dataset/train/cloth'\n    embeddings_path = \"/kaggle/input/clip-embed/pytorch/default/1/clip_image_embeds (1).pt\"\n    tagged_data_path = \"/kaggle/input/tagged-fashion-data/tagged_fashion_data.json\"\n\n    if not os.path.exists(embeddings_path) or not os.path.exists(tagged_data_path):\n        st.error(f\"CRITICAL ERROR: Data files not found. Ensure you have attached the 'fashion-app-data' Kaggle dataset.\")\n        st.stop()\n        \n    embedding_data = torch.load(embeddings_path, map_location='cpu')\n    image_embeds = embedding_data[\"embeds\"].numpy()\n    image_filenames = embedding_data[\"files\"]\n\n    tagged_df = pd.read_json(tagged_data_path, orient='records', lines=True)\n    tagged_df_indexed = tagged_df.set_index('filename')\n\n    print(\"Resource loading complete.\")\n    return device, clip_model, clip_processor, inpaint_pipe, image_embeds, image_filenames, tagged_df, tagged_df_indexed, blip_model, blip_processor\n\nDEVICE, clip_model, clip_processor, inpaint_pipe, image_embeds, image_filenames, tagged_df, tagged_df_indexed, blip_model, blip_processor = load_all_models_and_data()\n\n# CRITICAL PATHS - Ensure these match your Kaggle dataset structure\nBASE_DATA_PATH = \"/kaggle/input/clothe/clothes_tryon_dataset/train/cloth\"\nIMAGE_DIR = \"/kaggle/input/clothe/clothes_tryon_dataset/train/cloth\"\nPERSON_IMAGE_DIR = \"/kaggle/input/clothe/clothes_tryon_dataset/train/image\"\n\n# =============================================================================\n# 3. BACKEND FUNCTIONS\n# =============================================================================\n@st.cache_data\ndef generate_dropdown_options(_df):\n    options = defaultdict(set)\n    for attr in ['brand', 'color', 'pattern', 'style', 'neckline']:\n        options[attr].add(\"Any\")\n    for tag_list in _df['tags']:\n        if not isinstance(tag_list, list): continue\n        for tag in tag_list:\n            try:\n                attribute, value = tag.split('_', 1)\n                options[attribute].add(value.replace('_', ' '))\n            except ValueError: continue\n    return {attribute: sorted(list(values)) for attribute, values in options.items()}\n\ndef search_by_text(query, top_k=500):\n    text_inputs = clip_processor(text=[query], return_tensors=\"pt\").to(DEVICE)\n    with torch.no_grad():\n        text_feat = clip_model.get_text_features(**text_inputs)\n        text_feat = F.normalize(text_feat, dim=1)\n    sims = cosine_similarity(text_feat.cpu().numpy(), image_embeds)[0]\n    top_idx = np.argsort(-sims)[:top_k]\n    return [(image_filenames[i], sims[i]) for i in top_idx]\n\ndef search_by_image(query_image, top_k=500):\n    if query_image is None: return []\n    inputs = clip_processor(images=query_image, return_tensors=\"pt\").to(DEVICE)\n    with torch.no_grad():\n        query_embed = clip_model.get_image_features(**inputs)\n        query_embed = F.normalize(query_embed, dim=1)\n    sims = cosine_similarity(query_embed.cpu().numpy(), image_embeds)[0]\n    top_idx = np.argsort(-sims)[:top_k]\n    return [(image_filenames[i], sims[i]) for i in top_idx]\n\ndef multi_modal_search(query_text, query_image, image_weight=0.5, top_k=500):\n    text_feat = F.normalize(clip_model.get_text_features(**clip_processor(text=[query_text], return_tensors=\"pt\").to(DEVICE)))\n    img_feat = F.normalize(clip_model.get_image_features(**clip_processor(images=query_image, return_tensors=\"pt\").to(DEVICE)))\n    combined_feat = F.normalize((image_weight * img_feat) + ((1.0 - image_weight) * text_feat))\n    sims = cosine_similarity(combined_feat.cpu().detach().numpy(), image_embeds)[0]\n    top_idx = np.argsort(-sims)[:top_k]\n    return [(image_filenames[i], sims[i]) for i in top_idx]\n\ndef flexible_ranked_filter(initial_results, filters):\n    if not filters: return initial_results\n    expected_tags = {f\"{attr}_{val.replace(' ', '_')}\" for attr, val in filters.items()}\n    scored_results = []\n    for filename, original_score in initial_results:\n        try:\n            item_tags = set(tagged_df_indexed.loc[filename, 'tags'])\n            match_count = len(expected_tags.intersection(item_tags))\n            if match_count > 0:\n                scored_results.append((filename, original_score, match_count))\n        except KeyError: continue\n    sorted_results = sorted(scored_results, key=lambda x: (x[2], x[1]), reverse=True)\n    return [(filename, score) for filename, score, _ in sorted_results]\n\ndef add_to_cart(item_filename):\n    if item_filename not in st.session_state.cart:\n        st.session_state.cart.append(item_filename)\n\ndef remove_from_cart(item_filename):\n    if item_filename in st.session_state.cart:\n        st.session_state.cart.remove(item_filename)\n\ndef clear_cart():\n    st.session_state.cart = []\n\ndef load_more_items():\n    st.session_state.items_to_show += 12\n\ndef handle_search(text_query, image_query_file, image_weight, filters):\n    st.session_state.search_active = True\n    initial_results = []\n    image_query = Image.open(image_query_file) if image_query_file else None\n    if text_query and image_query: initial_results = multi_modal_search(text_query, image_query, image_weight)\n    elif image_query: initial_results = search_by_image(image_query)\n    elif text_query: initial_results = search_by_text(text_query)\n    else: initial_results = [(fname, 1.0) for fname in image_filenames]\n    st.session_state.search_results = flexible_ranked_filter(initial_results, filters)\n\ndef clear_search():\n    st.session_state.search_active = False\n    st.session_state.search_results = []\n    st.session_state.items_to_show = 12\n\ndef describe_sketch(img: Image.Image) -> str:\n    inputs = blip_processor(images=img, return_tensors=\"pt\").to(DEVICE)\n    with torch.no_grad():\n        out = blip_model.generate(**inputs, max_new_tokens=50)\n    return blip_processor.decode(out[0], skip_special_tokens=True)\n\ndef hybrid_clip_embedding(image: Image.Image, text: str):\n    inputs = clip_processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(DEVICE)\n    with torch.no_grad():\n        img_feat  = clip_model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n        text_feat = clip_model.get_text_features(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n    img_feat  = F.normalize(img_feat, dim=-1); text_feat = F.normalize(text_feat, dim=-1)\n    hybrid_feat = (img_feat + text_feat) / 2\n    return hybrid_feat.cpu().numpy()[0]\n\ndef find_top_k(query_emb, dataset_embs, k=5):\n    sims = dataset_embs @ query_emb\n    idx = np.argsort(-sims)[:k]\n    return idx, sims[idx]\n\ndef enhance_prompt(base_prompt):\n    enhanced_positive = f\"masterpiece, photorealistic, high-quality professional product shot of a {base_prompt}, intricate, hyper-detailed, sharp focus, cinematic lighting, 8k uhd, on a mannequin, clean white background\"\n    negative = \"lowres, blurry, bad anatomy, error, worst quality, jpeg artifacts, ugly, duplicate, morbid, out of frame, watermark, text, signature, person, model\"\n    return enhanced_positive, negative\n\ndef create_edited_design_enhanced(base_image, mask_image, user_prompt):\n    enhanced_prompt, negative_prompt = enhance_prompt(user_prompt)\n    with torch.no_grad():\n        edited_image = inpaint_pipe(\n            prompt=enhanced_prompt, image=base_image.resize((512, 512)), \n            mask_image=mask_image.resize((512, 512)),\n            negative_prompt=negative_prompt, num_inference_steps=50, guidance_scale=8.5\n        ).images[0]\n    return edited_image\n\ndef virtual_try_on_placeholder():\n    # Placeholder function, VTON is not implemented in this version\n    pass\n\n# =============================================================================\n# 4. STREAMLIT UI LAYOUT\n# =============================================================================\n\nst.title(\"🤖 AI Fashion Stylist Pro\")\nst.markdown(\"Discover, create, and virtually try on your next favorite outfit.\")\n\ntab1, tab2, tab3, tab4 = st.tabs([\"🛍️ Browse & Search\", \"🎨 Creative Director\", \"👤 Virtual Try-On\", \"✍️ Sketch to Fashion\"])\n\nwith tab1:\n    with st.sidebar:\n        st.header(\"Search & Filter\")\n        text_query = st.text_input(\"Text Description\", placeholder=\"e.g., a blue floral blouse\")\n        image_query_file = st.file_uploader(\"Upload an Image\", type=['png', 'jpg', 'jpeg'], key=\"main_uploader\")\n        image_weight = 0.5\n        if text_query and image_query_file:\n            image_weight = st.slider(\"Image vs. Text Influence\", 0.0, 1.0, 0.5, 0.1)\n        st.subheader(\"Smart Tags\")\n        dropdowns = generate_dropdown_options(tagged_df)\n        filters = {}\n        for attr, options in dropdowns.items():\n            selected = st.selectbox(attr.capitalize(), options, index=0)\n            if selected != \"Any\":\n                filters[attr] = selected\n        col1, col2 = st.columns(2)\n        with col1:\n            st.button(\"Search / Filter\", on_click=handle_search, args=(text_query, image_query_file, image_weight, filters), type=\"primary\")\n        with col2:\n            st.button(\"Browse All\", on_click=clear_search)\n        st.markdown(\"---\")\n        st.header(\"Shopping Cart\")\n        if not st.session_state.cart:\n            st.info(\"Your cart is empty.\")\n        else:\n            for item_fname in st.session_state.cart:\n                cart_col1, cart_col2 = st.columns([1, 3])\n                with cart_col1:\n                    st.image(os.path.join(IMAGE_DIR, item_fname), use_column_width=True)\n                with cart_col2:\n                    st.write(f\"**{item_fname}**\")\n                    st.button(\"Remove\", key=f\"remove_{item_fname}\", on_click=remove_from_cart, args=(item_fname,))\n            st.markdown(\"---\")\n            if st.button(\"Clear Cart\"):\n                clear_cart()\n                st.experimental_rerun()\n\n    if st.session_state.search_active:\n        st.header(\"Search Results\")\n        display_items = st.session_state.search_results\n        if not display_items:\n            st.warning(\"Your search returned no results. Try broadening your query or click 'Browse All'.\")\n    else:\n        st.header(\"Browse Our Collection\")\n        display_items = [(fname, None) for fname in image_filenames]\n    \n    if display_items:\n        items_to_display_now = display_items[:st.session_state.items_to_show]\n        num_cols = 6\n        cols = st.columns(num_cols)\n        for i, (fname, score) in enumerate(items_to_display_now):\n            image_path = os.path.join(IMAGE_DIR, fname)\n            if os.path.exists(image_path):\n                with cols[i % num_cols]:\n                    st.image(image_path, use_column_width=True)\n                    st.button(\"Add to Cart\", key=f\"add_{fname}\", on_click=add_to_cart, args=(fname,))\n        if not st.session_state.search_active and st.session_state.items_to_show < len(display_items):\n            st.button(\"Load More\", on_click=load_more_items, use_container_width=True)\n\nwith tab2:\n    st.header(\"Unleash Your Inner Designer\")\n    st.subheader(\"1. Find a Base Item to Edit\")\n    creative_search_term = st.text_input(\"Search for a clothing item\", key=\"creative_search\")\n    if st.button(\"Search\", key=\"creative_search_btn\"):\n        if creative_search_term:\n            with st.spinner(\"Searching...\"):\n                st.session_state.creative_search_results = search_by_text(creative_search_term, top_k=6)\n        else:\n            st.session_state.creative_search_results = None\n    if st.session_state.creative_search_results:\n        st.markdown(\"---\")\n        st.write(\"Click 'Select' on an item to start editing:\")\n        cols = st.columns(6)\n        for i, (fname, score) in enumerate(st.session_state.creative_search_results):\n            image_path = os.path.join(IMAGE_DIR, fname)\n            if os.path.exists(image_path):\n                cols[i].image(image_path, use_column_width=True)\n                if cols[i].button(\"Select\", key=f\"select_{fname}\"):\n                    st.session_state.selected_creative_image_fname = fname\n                    st.session_state.generated_design = None\n                    st.session_state.similar_items = None\n                    st.experimental_rerun()\n    st.markdown(\"---\")\n    if st.session_state.selected_creative_image_fname:\n        st.subheader(\"2. Paint Over the Area to Change & Describe Your Idea\")\n        base_image_path = os.path.join(IMAGE_DIR, st.session_state.selected_creative_image_fname)\n        base_img = Image.open(base_image_path).convert(\"RGB\")\n        col1, col2 = st.columns(2)\n        with col1:\n            st.write(\"Use the thick brush to paint the area you want to change.\")\n            canvas_result = st_canvas(\n                fill_color=\"rgba(255, 255, 255, 0)\", stroke_width=35,\n                stroke_color=\"rgba(255, 0, 0, 0.5)\", background_image=base_img.resize((512, 512)),\n                update_streamlit=True, height=512, width=512, drawing_mode=\"freedraw\", key=\"canvas\",\n            )\n        with col2:\n            prompt = st.text_input(\"What should the new design be?\", placeholder=\"e.g., a roaring tiger head\")\n            generate_button = st.button(\"Generate & Find Similar\", type=\"primary\", key=\"generate\")\n            if generate_button:\n                if canvas_result.image_data is not None and prompt:\n                    mask_array = canvas_result.image_data[:, :, 3] > 0\n                    if mask_array.sum() > 0:\n                        mask_img = Image.fromarray(mask_array.astype('uint8') * 255)\n                        with st.spinner(\"Creating your masterpiece...\"):\n                            st.session_state.generated_design = create_edited_design_enhanced(base_img, mask_img, prompt)\n                            st.session_state.similar_items = search_by_image(st.session_state.generated_design, top_k=6)\n                    else: st.warning(\"Please paint on the image to indicate the area to change.\")\n                else: st.error(\"Please paint on the image and provide a text prompt.\")\n    if st.session_state.generated_design:\n        st.markdown(\"---\"); st.subheader(\"3. Your Unique Creation & Similar Real Items\")\n        res_col1, res_col2 = st.columns([1, 2])\n        with res_col1: st.image(st.session_state.generated_design, caption=\"Your Generated Design\", use_column_width=True)\n        with res_col2:\n            if st.session_state.similar_items:\n                cols = st.columns(3)\n                for i, (fname, score) in enumerate(st.session_state.similar_items):\n                    image_path = os.path.join(IMAGE_DIR, fname)\n                    if os.path.exists(image_path): cols[i % 3].image(image_path, use_column_width=True)\n            else: st.info(\"No similar items found in the database.\")\n\nwith tab3:\n    # (Placeholder VTON code)\n    st.header(\"Experience the Future of Fitting\")\n    st.info(\"This is a conceptual demonstration of a Virtual Try-On system using your dataset.\")\n    col1, col2 = st.columns(2)\n    with col1:\n        person_img_file = st.file_uploader(\"1. Upload Person Image\", type=['png', 'jpg', 'jpeg'], key=\"vton_p\")\n    with col2:\n        cloth_img_file = st.file_uploader(\"2. Upload Clothing Item\", type=['png', 'jpg', 'jpeg'], key=\"vton_c\")\n    if st.button(\"Generate Virtual Try-On\", type=\"primary\", key=\"vton_gen\"):\n        if person_img_file and cloth_img_file:\n            person_img = Image.open(person_img_file)\n            cloth_img = Image.open(cloth_img_file)\n            st.image(virtual_try_on_placeholder(person_img, cloth_img), caption=\"VTON Process Simulation\")\n        else:\n            st.error(\"Please upload both a person and a clothing image.\")\n\nwith tab4:\n    st.header(\"Sketch-Based Discovery 🖊️\")\n    sketch_file = st.file_uploader(\"Upload Sketch Image\", type=['png', 'jpg', 'jpeg'], key=\"sketch_uploader\")\n    if sketch_file:\n        sketch = Image.open(sketch_file).convert(\"RGB\").resize((224, 224))\n        st.image(sketch, caption=\"Uploaded Sketch\", use_column_width=False)\n        if st.button(\"Show Recommendations\", type=\"primary\", key=\"sketch_btn\"):\n            with st.spinner(\"🧠 Describing your sketch and searching...\"):\n                caption = describe_sketch(sketch)\n                st.session_state.sketch_caption = caption\n                sketch_embed = hybrid_clip_embedding(sketch, caption)\n                sims = image_embeds @ sketch_embed\n                topk = np.argsort(-sims)[:6]\n                st.session_state.sketch_results = []\n                for i, idx in enumerate(topk):\n                    st.session_state.sketch_results.append((image_filenames[idx], sims[idx]))\n\n    if st.session_state.sketch_results:\n        st.markdown(\"---\")\n        #st.subheader(f\"AI Caption: *\\\"{st.session_state.sketch_caption}\\\"*\")\n        st.subheader(\"Top 6 Matches Found:\")\n        cols = st.columns(6)\n        for i, (fname, score) in enumerate(st.session_state.sketch_results):\n            image_path = os.path.join(IMAGE_DIR, fname)\n            if os.path.exists(image_path):\n                cols[i].image(image_path, caption=f\"Score: {score:.2f}\", use_column_width=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:59:23.433451Z","iopub.execute_input":"2025-07-21T16:59:23.434210Z","iopub.status.idle":"2025-07-21T16:59:23.446733Z","shell.execute_reply.started":"2025-07-21T16:59:23.434183Z","shell.execute_reply":"2025-07-21T16:59:23.446083Z"}},"outputs":[{"name":"stdout","text":"Overwriting app.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# File: evaluate_similarity.py\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom tqdm import tqdm\nimport os\n\n# =============================================================================\n# CONFIGURATION\n# =============================================================================\n# --- KAGGLE-SPECIFIC FILE PATHS ---\n# Ensure these paths are correct for your Kaggle environment or local setup.\nEMBEDDINGS_PATH = \"/kaggle/input/clip-embed/pytorch/default/1/clip_image_embeds (1).pt\"\nTAGGED_DATA_PATH = \"/kaggle/input/tagged-fashion-data/tagged_fashion_data.json\"\n\n# --- EVALUATION PARAMETERS ---\n# K defines how many top results we should check for a match.\nK = 5 \n\n# =============================================================================\n# MAIN EVALUATION SCRIPT\n# =============================================================================\n\ndef main():\n    \"\"\"\n    Main function to load data and run the similarity evaluation.\n    \"\"\"\n    print(\"--- Starting Recommendation Dataset Similarity Evaluation ---\")\n\n    # --- 1. Load Data ---\n    print(f\"Loading embeddings from: {EMBEDDINGS_PATH}\")\n    if not os.path.exists(EMBEDDINGS_PATH):\n        print(f\"ERROR: Embeddings file not found at {EMBEDDINGS_PATH}\")\n        return\n        \n    embedding_data = torch.load(EMBEDDINGS_PATH, map_location='cpu')\n    image_embeds = embedding_data[\"embeds\"].numpy()\n    image_filenames = embedding_data[\"files\"]\n\n    print(f\"Loading tagged data from: {TAGGED_DATA_PATH}\")\n    if not os.path.exists(TAGGED_DATA_PATH):\n        print(f\"ERROR: Tagged data file not found at {TAGGED_DATA_PATH}\")\n        return\n        \n    tagged_df = pd.read_json(TAGGED_DATA_PATH, orient='records', lines=True)\n    tagged_df_indexed = tagged_df.set_index('filename')\n\n    print(f\"Loaded {len(image_filenames)} items to evaluate.\")\n    print(f\"Will check top {K} recommendations for each item.\")\n\n    # --- 2. Initialize Counters ---\n    total_items_processed = 0\n    hits_at_k = 0\n\n    # --- 3. Calculate Similarity Matrix ---\n    # This is a one-time, memory-intensive operation but much faster than per-item calculation.\n    print(\"\\nCalculating similarity matrix for all items... (This may take a moment)\")\n    similarity_matrix = cosine_similarity(image_embeds)\n    \n    # --- 4. Iterate and Evaluate Each Item ---\n    print(\"Evaluating each item as a query...\")\n    # Use tqdm for a nice progress bar\n    for i, query_filename in enumerate(tqdm(image_filenames, desc=\"Processing Items\")):\n        try:\n            # Get the ground truth tags for our query item\n            query_tags = set(tagged_df_indexed.loc[query_filename, 'tags'])\n            if not query_tags:\n                continue # Skip items that have no tags to compare against\n        except KeyError:\n            # Skip if the item from embeddings isn't in our tagged data\n            continue\n            \n        total_items_processed += 1\n        \n        # Get the similarity scores for the current item against all others\n        sim_scores = similarity_matrix[i]\n        \n        # Get the indices of the top K+1 most similar items (because the top one is itself)\n        # We use np.argsort to sort and get indices\n        top_indices = np.argsort(-sim_scores)[1:K+1] # Exclude the first one (itself)\n\n        # --- 5. Check for a \"Hit\" ---\n        found_hit = False\n        for result_idx in top_indices:\n            try:\n                result_filename = image_filenames[result_idx]\n                result_tags = set(tagged_df_indexed.loc[result_filename, 'tags'])\n                \n                # A \"hit\" is defined as having at least one tag in common\n                if query_tags.intersection(result_tags):\n                    found_hit = True\n                    break # We found a match, no need to check other results for this query\n            except KeyError:\n                continue\n        \n        if found_hit:\n            hits_at_k += 1\n\n    # --- 6. Calculate and Display Final Score ---\n    if total_items_processed == 0:\n        print(\"\\nEvaluation could not be completed. No items with tags were found to process.\")\n        return\n        \n    percentage_similarity = (hits_at_k / total_items_processed) * 100\n    \n    print(\"\\n--- 📈 Evaluation Complete ---\")\n    print(f\"Total Items with Tags Evaluated: {total_items_processed}\")\n    print(f\"Successful Recommendations (Hits @ {K}): {hits_at_k}\")\n    print(f\"Percentage Similarity (Recall@{K}): {percentage_similarity:.2f}%\")\n    print(\"\\nThis means that for a given clothing item, there is a \"\n          f\"{percentage_similarity:.2f}% chance that at least one of the top {K} \"\n          \"visually similar recommendations also shares a semantic tag (like color, brand, or style) with it.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:31:01.571500Z","iopub.execute_input":"2025-07-21T17:31:01.571814Z","iopub.status.idle":"2025-07-21T17:31:08.486876Z","shell.execute_reply.started":"2025-07-21T17:31:01.571788Z","shell.execute_reply":"2025-07-21T17:31:08.486111Z"}},"outputs":[{"name":"stdout","text":"--- Starting Recommendation Dataset Similarity Evaluation ---\nLoading embeddings from: /kaggle/input/clip-embed/pytorch/default/1/clip_image_embeds (1).pt\nLoading tagged data from: /kaggle/input/tagged-fashion-data/tagged_fashion_data.json\nLoaded 11647 items to evaluate.\nWill check top 5 recommendations for each item.\n\nCalculating similarity matrix for all items... (This may take a moment)\nEvaluating each item as a query...\n","output_type":"stream"},{"name":"stderr","text":"Processing Items: 100%|██████████| 11647/11647 [00:03<00:00, 3087.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n--- 📈 Evaluation Complete ---\nTotal Items with Tags Evaluated: 11076\nSuccessful Recommendations (Hits @ 5): 10682\nPercentage Similarity (Recall@5): 96.44%\n\nThis means that for a given clothing item, there is a 96.44% chance that at least one of the top 5 visually similar recommendations also shares a semantic tag (like color, brand, or style) with it.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}