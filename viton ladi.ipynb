{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12302277,"sourceType":"datasetVersion","datasetId":7754175},{"sourceId":472070,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":380528,"modelId":400239}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport cv2\n\nclass FashionVTONDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset for the Fashion VTON task.\n    This version dynamically finds the clothing label to be more robust.\n    \"\"\"\n    def __init__(self, data_root, image_size=(256, 192), original_image_size=(768, 1024)):\n        self.data_root = data_root\n        self.image_size = image_size\n        self.original_image_size = original_image_size\n        \n        self.image_dir = os.path.join(data_root, 'image')\n        self.cloth_dir = os.path.join(data_root, 'cloth')\n        self.cloth_mask_dir = os.path.join(data_root, 'cloth-mask')\n        self.pose_dir = os.path.join(data_root, 'openpose_json')\n        self.parse_dir = os.path.join(data_root, 'image-parse-v3')\n\n        self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))])\n\n        self.transform = transforms.Compose([\n            transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        \n        self.mask_transform = transforms.Compose([\n            transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.NEAREST),\n            transforms.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def find_upper_cloth_label(self, pose_keypoints, parse_array):\n        \"\"\"\n        Dynamically finds the clothing label by looking at the most common\n        pixel value in the torso area defined by pose keypoints.\n        \"\"\"\n        # BODY_25 keypoint indices for the torso area\n        torso_indices = [1, 2, 5, 8, 9, 12]  # Neck, Shoulders, MidHip, Hips\n        \n        visible_points = []\n        for i in torso_indices:\n            x, y, conf = pose_keypoints[i]\n            if conf > 0.1:  # Use only confident keypoints\n                visible_points.append((int(x), int(y)))\n        \n        # If not enough points to define an area, return a fallback\n        if len(visible_points) < 3:\n            return 5  # Fallback to the original hardcoded label\n\n        # Define a bounding box around the torso\n        x_coords, y_coords = zip(*visible_points)\n        min_x, max_x = min(x_coords), max(x_coords)\n        min_y, max_y = min(y_coords), max(y_coords)\n\n        # Ensure the bounding box has a valid area\n        if max_x <= min_x or max_y <= min_y:\n            return 5\n\n        # Crop the parse map to the torso area\n        torso_parse_area = parse_array[min_y:max_y, min_x:max_x]\n\n        # Find the most frequent non-zero label\n        unique_labels, counts = np.unique(torso_parse_area, return_counts=True)\n        non_zero_mask = (unique_labels != 0) # Ignore background\n        \n        if np.any(non_zero_mask):\n            # Get the label with the highest count among non-zero labels\n            mode_label = unique_labels[non_zero_mask][np.argmax(counts[non_zero_mask])]\n            return mode_label\n        else:\n            return 5 # Fallback if only background is found\n\n    def __getitem__(self, idx):\n        image_name = self.image_files[idx]\n        base_name = os.path.splitext(image_name)[0]\n\n        # Load images\n        person_image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB')\n        cloth_image = Image.open(os.path.join(self.cloth_dir, image_name)).convert('RGB')\n        cloth_mask = Image.open(os.path.join(self.cloth_mask_dir, image_name)).convert('L')\n\n        # Load pose data\n        pose_path = os.path.join(self.pose_dir, f\"{base_name}_keypoints.json\")\n        try:\n            with open(pose_path, 'r') as f:\n                pose_data = json.load(f)\n            pose_keypoints = np.array(pose_data['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n        except (FileNotFoundError, IndexError):\n            pose_keypoints = np.zeros((25, 3), dtype=np.float32)\n\n        # Load segmentation map (at original resolution for label finding)\n        parse_path = os.path.join(self.parse_dir, f\"{base_name}.png\")\n        parse_image_orig = Image.open(parse_path).convert('L')\n        parse_array_orig = np.array(parse_image_orig)\n\n        # --- Use the new robust method to find the clothing label ---\n        upper_cloth_label = self.find_upper_cloth_label(pose_keypoints, parse_array_orig)\n        \n        # Resize parse map for creating masks\n        parse_array_resized = cv2.resize(parse_array_orig, self.image_size[::-1], interpolation=cv2.INTER_NEAREST)\n\n        # Create masks using the dynamically found label\n        person_cloth_mask = (parse_array_resized == upper_cloth_label).astype(np.float32)\n        \n        # --- The rest of the processing remains the same ---\n        person_image_tensor = self.transform(person_image)\n        cloth_image_tensor = self.transform(cloth_image)\n        cloth_mask_tensor = self.mask_transform(cloth_mask)\n        pose_map = self.create_pose_map(pose_keypoints) # Assuming create_pose_map is part of this class\n        pose_map_tensor = torch.from_numpy(pose_map).float()\n        \n        blurred_mask = cv2.GaussianBlur(person_cloth_mask, (5, 5), 0)\n        blurred_mask_tensor = torch.from_numpy(blurred_mask).unsqueeze(0)\n        \n        agnostic_person_tensor = person_image_tensor * (1 - blurred_mask_tensor)\n        warped_cloth_tensor = person_image_tensor * torch.from_numpy(person_cloth_mask).unsqueeze(0)\n\n        return {\n            'person_image': person_image_tensor,\n            'cloth_image': cloth_image_tensor,\n            'cloth_mask': cloth_mask_tensor,\n            'agnostic_person': agnostic_person_tensor,\n            'pose_map': pose_map_tensor,\n            'warped_cloth': warped_cloth_tensor\n        }\n\n    def create_pose_map(self, keypoints):\n        h, w = self.image_size\n        orig_w, orig_h = self.original_image_size\n        num_keypoints = keypoints.shape[0]\n        pose_map = np.zeros((num_keypoints, h, w), dtype=np.float32)\n        \n        for i, point in enumerate(keypoints):\n            if point[2] > 0.1:\n                x = int(point[0] * w / orig_w)\n                y = int(point[1] * h / orig_h)\n                if 0 <= x < w and 0 <= y < h:\n                    cv2.circle(pose_map[i], (x, y), radius=3, color=1, thickness=-1)\n        return pose_map","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n# Corrected import line:\nfrom torch.utils.data import DataLoader \nimport matplotlib.pyplot as plt\nimport numpy as np\n# This transform is just for visualization, so we define it locally here\nfrom torchvision import transforms \n\ndef tensor_to_pil(tensor):\n    \"\"\"Converts a [-1, 1] tensor to a PIL Image for visualization.\"\"\"\n    tensor = (tensor + 1) / 2\n    tensor = tensor.clamp(0, 1)\n    # Ensure tensor is on CPU before converting to numpy/PIL\n    return transforms.ToPILImage()(tensor.cpu())\n\n# --- Configuration ---\nDATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'\nBATCH_SIZE = 1\nORIGINAL_SIZE = (768, 1024) \n\n# --- Create Dataset and DataLoader ---\n# The class name is FashionVTONDataset\ndataset = FashionVTONDataset(data_root=DATA_ROOT, original_image_size=ORIGINAL_SIZE)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# --- Fetch and Visualize One Sample ---\n# Make sure to handle the case where the dataloader might be empty\ntry:\n    sample = next(iter(dataloader))\n    print(\"Successfully loaded a sample from the DataLoader.\")\nexcept StopIteration:\n    print(\"DataLoader is empty. Please check your data directory and file names.\")\n    exit()\n\n\nprint(\"Keys in the sample batch:\", sample.keys())\nprint(\"Shape of person_image:\", sample['person_image'].shape)\nprint(\"Shape of cloth_image:\", sample['cloth_image'].shape)\nprint(\"Shape of pose_map:\", sample['pose_map'].shape)\nprint(\"Shape of agnostic_person:\", sample['agnostic_person'].shape)\n\n# --- Visualization ---\nfig, axs = plt.subplots(1, 5, figsize=(20, 4))\naxs[0].imshow(tensor_to_pil(sample['person_image'][0]))\naxs[0].set_title(\"Original Person\")\naxs[0].axis('off')\n\naxs[1].imshow(tensor_to_pil(sample['cloth_image'][0]))\naxs[1].set_title(\"Cloth Item\")\naxs[1].axis('off')\n\n# Summing along the channel dimension to visualize the pose map\npose_visualization = np.sum(sample['pose_map'][0].cpu().numpy(), axis=0)\naxs[2].imshow(pose_visualization, cmap='gray')\naxs[2].set_title(\"Pose Map\")\naxs[2].axis('off')\n\naxs[3].imshow(tensor_to_pil(sample['agnostic_person'][0]))\naxs[3].set_title(\"Agnostic Person\")\naxs[3].axis('off')\n\naxs[4].imshow(tensor_to_pil(sample['warped_cloth'][0]))\naxs[4].set_title(\"Ground Truth Warped\")\naxs[4].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GMM(nn.Module):\n    \"\"\"\n    Geometric Matching Module.\n    A U-Net-like architecture that takes a cloth image and a pose map\n    and predicts a flow field to warp the cloth.\n    \"\"\"\n    def __init__(self, in_channels_cloth=3, in_channels_pose=18, out_channels_flow=2):\n        \"\"\"\n        Args:\n            in_channels_cloth (int): Number of channels in the cloth image (3 for RGB).\n            in_channels_pose (int): Number of channels in the pose map (e.g., 18 for 18 keypoints).\n            out_channels_flow (int): Number of channels for the output flow (2 for x and y).\n        \"\"\"\n        super(GMM, self).__init__()\n\n        # Encoder part\n        self.encoder1 = self.conv_block(in_channels_cloth + in_channels_pose, 64)\n        self.encoder2 = self.conv_block(64, 128)\n        self.encoder3 = self.conv_block(128, 256)\n        self.encoder4 = self.conv_block(256, 512)\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # Bottleneck\n        self.bottleneck = self.conv_block(512, 1024)\n\n        # Decoder part\n        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.decoder4 = self.conv_block(1024, 512)\n        \n        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.decoder3 = self.conv_block(512, 256)\n        \n        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.decoder2 = self.conv_block(256, 128)\n        \n        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.decoder1 = self.conv_block(128, 64)\n\n        # Final output layer\n        # This layer predicts the 2-channel flow field.\n        # The output is activated with tanh to keep values between -1 and 1.\n        self.conv_out = nn.Conv2d(64, out_channels_flow, kernel_size=1)\n        self.tanh = nn.Tanh()\n\n    def conv_block(self, in_channels, out_channels):\n        \"\"\"Helper function for a standard convolutional block.\"\"\"\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, cloth_image, pose_map):\n        \"\"\"\n        Forward pass for the GMM.\n        \n        Args:\n            cloth_image (Tensor): The clothing image tensor. Shape: [B, 3, H, W]\n            pose_map (Tensor): The pose map tensor. Shape: [B, 18, H, W]\n            \n        Returns:\n            Tensor: The predicted flow field. Shape: [B, 2, H, W]\n        \"\"\"\n        # Concatenate inputs along the channel dimension\n        x = torch.cat([cloth_image, pose_map], dim=1)\n\n        # Encoder path\n        e1 = self.encoder1(x)\n        p1 = self.pool(e1)\n        \n        e2 = self.encoder2(p1)\n        p2 = self.pool(e2)\n        \n        e3 = self.encoder3(p2)\n        p3 = self.pool(e3)\n        \n        e4 = self.encoder4(p3)\n        p4 = self.pool(e4)\n\n        # Bottleneck\n        b = self.bottleneck(p4)\n\n        # Decoder path\n        d4 = self.upconv4(b)\n        d4 = torch.cat([d4, e4], dim=1)\n        d4 = self.decoder4(d4)\n        \n        d3 = self.upconv3(d4)\n        d3 = torch.cat([d3, e3], dim=1)\n        d3 = self.decoder3(d3)\n        \n        d2 = self.upconv2(d3)\n        d2 = torch.cat([d2, e2], dim=1)\n        d2 = self.decoder2(d2)\n        \n        d1 = self.upconv1(d2)\n        d1 = torch.cat([d1, e1], dim=1)\n        d1 = self.decoder1(d1)\n\n        # Output flow field\n        flow_field = self.conv_out(d1)\n        flow_field = self.tanh(flow_field)\n        \n        return flow_field\n\n\ndef warp_cloth_with_flow(cloth_image, flow_field):\n    \"\"\"\n    Warps a cloth image using a predicted flow field.\n    \n    Args:\n        cloth_image (Tensor): The clothing image to warp. Shape: [B, C, H, W]\n        flow_field (Tensor): The predicted flow field. Shape: [B, 2, H, W]\n        \n    Returns:\n        Tensor: The warped cloth image.\n    \"\"\"\n    # The flow field from the network is in range [-1, 1].\n    # grid_sample expects the flow field to be permuted to [B, H, W, 2]\n    # where the last dimension contains (x, y) coordinates.\n    flow_field = flow_field.permute(0, 2, 3, 1)\n    \n    # torch.nn.functional.grid_sample applies the warping.\n    # 'align_corners=True' is important for consistency.\n    warped_image = F.grid_sample(cloth_image, flow_field, mode='bilinear', padding_mode='zeros', align_corners=True)\n    \n    return warped_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\nimport os\nfrom tqdm import tqdm\nimport torchvision.models as models\n\n# This assumes the FashionVTONDataset, GMM, and warp_cloth_with_flow are defined\n# from dataset import FashionVTONDataset\n# from geometric_matching import GMM, warp_cloth_with_flow\n\nclass VGGPerceptualLoss(nn.Module):\n    # ... (This class definition remains exactly the same as the previous version) ...\n    def __init__(self, resize=True):\n        super(VGGPerceptualLoss, self).__init__()\n        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n        self.vgg_layers = vgg[:35].eval()\n        for param in self.vgg_layers.parameters(): param.requires_grad = False\n        self.l1 = nn.L1Loss()\n        self.transform = nn.functional.interpolate\n        self.resize = resize\n        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n    def forward(self, pred, target):\n        pred = (pred + 1) / 2; target = (target + 1) / 2\n        pred = (pred - self.mean) / self.std; target = (target - self.mean) / self.std\n        if self.resize:\n            pred = self.transform(pred, mode='bilinear', size=(224, 224), align_corners=False)\n            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n        pred_features = self.vgg_layers(pred); target_features = self.vgg_layers(target)\n        return self.l1(pred_features, target_features)\n\nclass TVLoss(nn.Module):\n    # ... (This class definition remains unchanged) ...\n    def __init__(self): super(TVLoss, self).__init__()\n    def forward(self, x):\n        batch_size, c, h, w = x.size(); tv_h = torch.pow(x[:,:,1:,:] - x[:,:,:-1,:], 2).sum(); tv_w = torch.pow(x[:,:,:,1:] - x[:,:,:,:-1], 2).sum()\n        return (tv_h + tv_w) / (batch_size * c * h * w)\n\n\ndef main():\n    \"\"\"Main training loop for the GMM with MASKED L1 Loss and re-balanced weights.\"\"\"\n    # --- Setup & Config (Same as before) ---\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n    DATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'; CHECKPOINT_DIR = '/kaggle/working/'; VISUALIZATION_DIR = '/kaggle/working/'\n    BATCH_SIZE = 8; NUM_EPOCHS = 50; LEARNING_RATE = 2e-5; IMAGE_SIZE = (256, 192); ORIGINAL_IMAGE_SIZE = (768, 1024)\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True); os.makedirs(VISUALIZATION_DIR, exist_ok=True)\n\n    # --- Data Loading (Same as before) ---\n    train_dataset = FashionVTONDataset(data_root=DATA_ROOT, image_size=IMAGE_SIZE, original_image_size=ORIGINAL_IMAGE_SIZE)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    print(f\"Dataset loaded with {len(train_dataset)} samples.\")\n\n    # --- Model, Optimizer, and Loss (Same as before) ---\n    gmm = GMM(in_channels_pose=25).to(device)\n    optimizer = optim.Adam(gmm.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n    l1_loss_fn = nn.L1Loss().to(device)\n    perceptual_loss_fn = VGGPerceptualLoss().to(device)\n    tv_loss_fn = TVLoss().to(device)\n    \n    # --- Resumption Logic (Same as before) ---\n    start_epoch = 0; checkpoint_path = os.path.join(CHECKPOINT_DIR, 'gmm_latest.pth')\n    if os.path.isfile(checkpoint_path):\n        print(f\"Resuming GMM training from checkpoint: {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path); gmm.load_state_dict(checkpoint['model_state_dict']); optimizer.load_state_dict(checkpoint['optimizer_state_dict']); start_epoch = checkpoint['epoch'] + 1\n        print(f\"Resumed from Epoch {start_epoch}\")\n    else: print(\"Starting GMM training from scratch.\")\n\n    # --- Training Loop ---\n    for epoch in range(start_epoch, NUM_EPOCHS):\n        gmm.train(); epoch_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n\n        for i, batch in enumerate(progress_bar):\n            cloth_image = batch['cloth_image'].to(device)\n            cloth_mask = batch['cloth_mask'].to(device) # We need the cloth mask now\n            pose_map = batch['pose_map'].to(device)\n            ground_truth_warped = batch['warped_cloth'].to(device)\n            \n            optimizer.zero_grad()\n            predicted_flow = gmm(cloth_image, pose_map)\n            predicted_warped = warp_cloth_with_flow(cloth_image, predicted_flow)\n            \n            # Warp the cloth mask as well to know where the cloth is in the warped image\n            warped_cloth_mask = warp_cloth_with_flow(cloth_mask, predicted_flow)\n\n            # =================== UPDATED LOSS CALCULATION ===================\n            # 1. Masked L1 Loss: Only calculate loss on the cloth pixels\n            loss_l1 = l1_loss_fn(predicted_warped * warped_cloth_mask, ground_truth_warped * warped_cloth_mask)\n            \n            # 2. Perceptual Loss (same as before)\n            loss_p = perceptual_loss_fn(predicted_warped, ground_truth_warped)\n            \n            # 3. TV Loss (same as before)\n            loss_tv = tv_loss_fn(predicted_flow)\n            \n            # 4. Re-balanced Total Loss: Give L1 loss a stronger weight\n            total_loss = (10 * loss_l1) + loss_p + (0.5 * loss_tv)\n            # ===============================================================\n\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(gmm.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            epoch_loss += total_loss.item()\n            progress_bar.set_postfix(loss=total_loss.item(), l1=loss_l1.item(), perceptual=loss_p.item())\n            \n            # (Visualization code is the same)\n            if i == 0:\n                visual_comparison = torch.cat([(cloth_image.cpu() + 1) / 2, (predicted_warped.cpu().detach() + 1) / 2, (ground_truth_warped.cpu() + 1) / 2], dim=0)\n                save_image(visual_comparison, os.path.join(VISUALIZATION_DIR, f'epoch_{epoch+1}_comparison.png'), nrow=BATCH_SIZE)\n\n        avg_epoch_loss = epoch_loss / len(train_loader)\n        print(f\"End of Epoch {epoch+1}/{NUM_EPOCHS}, Average Loss: {avg_epoch_loss:.4f}\")\n\n        # (Checkpointing code is the same)\n        latest_checkpoint_state = {'epoch': epoch, 'model_state_dict': gmm.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}\n        torch.save(latest_checkpoint_state, checkpoint_path)\n\n    final_model_path = os.path.join(CHECKPOINT_DIR, 'gmm_final.pth')\n    torch.save(gmm.state_dict(), final_model_path)\n    print(f\"Final model saved to {final_model_path}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\nimport os\nfrom tqdm import tqdm\n\n# # Import our custom modules\n# from dataset import FashionVTONDataset\n# from geometric_matching import GMM, warp_cloth_with_flow\n\n# --- Training Configuration ---\nDATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'\nCHECKPOINT_DIR = '/kaggle/working/'\nVISUALIZATION_DIR = '/kaggle/working/'\nBATCH_SIZE = 8 # Adjust based on your VRAM\nNUM_EPOCHS = 5 # Start with 50 and increase if needed\nLEARNING_RATE = 0.0001\nIMAGE_SIZE = (256, 192) # Use a smaller size for faster training initially\nORIGINAL_IMAGE_SIZE = (768, 1024) # Adjust if your source is different\n\n# Create directories if they don't exist\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(VISUALIZATION_DIR, exist_ok=True)\n\n\nclass TVLoss(nn.Module):\n    \"\"\"Total Variation Loss for flow field regularization.\"\"\"\n    def __init__(self):\n        super(TVLoss, self).__init__()\n\n    def forward(self, x):\n        batch_size, c, h, w = x.size()\n        tv_h = torch.pow(x[:,:,1:,:] - x[:,:,:-1,:], 2).sum()\n        tv_w = torch.pow(x[:,:,:,1:] - x[:,:,:,:-1], 2).sum()\n        return (tv_h + tv_w) / (batch_size * c * h * w)\n\n\ndef main():\n    \"\"\"Main training loop for the GMM.\"\"\"\n    # --- Setup ---\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # --- Data Loading ---\n    print(\"Loading dataset...\")\n    train_dataset = FashionVTONDataset(\n        data_root=DATA_ROOT,\n        image_size=IMAGE_SIZE,\n        original_image_size=ORIGINAL_IMAGE_SIZE\n    )\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True\n    )\n    print(f\"Dataset loaded with {len(train_dataset)} samples.\")\n\n    # --- Model, Loss, and Optimizer ---\n    gmm = GMM(in_channels_pose=25).to(device)\n    l1_loss = nn.L1Loss().to(device)\n    tv_loss = TVLoss().to(device)\n    optimizer = optim.Adam(gmm.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999)) # LEARNING_RATE is now lower\n\n    # --- Training Loop ---\n    print(\"Starting training...\")\n    for epoch in range(NUM_EPOCHS):\n        gmm.train()\n        epoch_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n\n        for i, batch in enumerate(progress_bar):\n            cloth_image = batch['cloth_image'].to(device)\n            pose_map = batch['pose_map'].to(device)\n            ground_truth_warped = batch['warped_cloth'].to(device)\n            \n            optimizer.zero_grad()\n\n            predicted_flow = gmm(cloth_image, pose_map)\n            predicted_warped = warp_cloth_with_flow(cloth_image, predicted_flow)\n\n            loss_l1 = l1_loss(predicted_warped, ground_truth_warped)\n            loss_tv = tv_loss(predicted_flow)\n            total_loss = loss_l1 + 0.5 * loss_tv\n\n            total_loss.backward()\n            \n            # Add gradient clipping as a safety measure\n            torch.nn.utils.clip_grad_norm_(gmm.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            \n            epoch_loss += total_loss.item()\n            progress_bar.set_postfix(loss=total_loss.item(), l1=loss_l1.item(), tv=loss_tv.item())\n            \n            if i == 0:\n                cloth_image_vis = (cloth_image + 1) / 2\n                predicted_warped_vis = (predicted_warped + 1) / 2\n                ground_truth_vis = (ground_truth_warped + 1) / 2\n                visual_comparison = torch.cat([cloth_image_vis, predicted_warped_vis, ground_truth_vis], dim=0)\n                save_image(visual_comparison, os.path.join(VISUALIZATION_DIR, f'epoch_{epoch+1}_comparison.png'), nrow=BATCH_SIZE)\n\n        avg_epoch_loss = epoch_loss / len(train_loader)\n        print(f\"End of Epoch {epoch+1}/{NUM_EPOCHS}, Average Loss: {avg_epoch_loss:.4f}\")\n\n        if (epoch + 1) % 5 == 0:\n            checkpoint_path = os.path.join(CHECKPOINT_DIR, f'gmm_epoch_{epoch+1}.pth')\n            torch.save(gmm.state_dict(), checkpoint_path)\n            print(f\"Checkpoint saved to {checkpoint_path}\")\n\n    print(\"Training finished.\")\n    final_model_path = os.path.join(CHECKPOINT_DIR, 'gmm_final.pth')\n    torch.save(gmm.state_dict(), final_model_path)\n    print(f\"Final model saved to {final_model_path}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T14:50:29.717187Z","iopub.execute_input":"2025-07-14T14:50:29.717955Z","iopub.status.idle":"2025-07-14T15:32:43.304034Z","shell.execute_reply.started":"2025-07-14T14:50:29.717929Z","shell.execute_reply":"2025-07-14T15:32:43.303115Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading dataset...\nDataset loaded with 11647 samples.\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 1456/1456 [08:33<00:00,  2.83it/s, l1=0.4, loss=0.4, tv=4.29e-6]     \n","output_type":"stream"},{"name":"stdout","text":"End of Epoch 1/5, Average Loss: 0.5723\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1456/1456 [08:26<00:00,  2.87it/s, l1=0.608, loss=0.608, tv=5.42e-5] \n","output_type":"stream"},{"name":"stdout","text":"End of Epoch 2/5, Average Loss: 0.5744\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1456/1456 [08:25<00:00,  2.88it/s, l1=0.648, loss=0.648, tv=0.000177]\n","output_type":"stream"},{"name":"stdout","text":"End of Epoch 3/5, Average Loss: 0.5731\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 1456/1456 [08:24<00:00,  2.89it/s, l1=0.559, loss=0.559, tv=0.000269]\n","output_type":"stream"},{"name":"stdout","text":"End of Epoch 4/5, Average Loss: 0.5705\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 1456/1456 [08:22<00:00,  2.90it/s, l1=0.632, loss=0.632, tv=0.000102]\n","output_type":"stream"},{"name":"stdout","text":"End of Epoch 5/5, Average Loss: 0.5759\nCheckpoint saved to /kaggle/working/gmm_epoch_5.pth\nTraining finished.\nFinal model saved to /kaggle/working/gmm_final.pth\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# %%writefile train_gmm_ddp.py\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader, Dataset\n# from torchvision.utils import save_image\n# import os\n# from tqdm import tqdm\n# import torch.distributed as dist\n# from torch.nn.parallel import DistributedDataParallel as DDP\n# from torch.utils.data.distributed import DistributedSampler\n# import torchvision.models as models\n# import json\n# import cv2\n# import numpy as np\n# from PIL import Image\n# from torchvision import transforms\n# import socket # NEW IMPORT\n# import torch.nn.functional as F # <<< THIS IS THE FIX\n\n# # ===================================================================\n# # ALL CLASS DEFINITIONS (These are unchanged)\n# # ===================================================================\n# class FashionVTONDataset(Dataset):\n#     def __init__(self, data_root, image_size=(256, 192), original_image_size=(768, 1024)):\n#         self.data_root = data_root; self.image_size = image_size; self.original_image_size = original_image_size; self.image_dir = os.path.join(data_root, 'image'); self.cloth_dir = os.path.join(data_root, 'cloth'); self.cloth_mask_dir = os.path.join(data_root, 'cloth-mask'); self.pose_dir = os.path.join(data_root, 'openpose_json'); self.parse_dir = os.path.join(data_root, 'image-parse-v3'); self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))]); self.transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BILINEAR), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]); self.mask_transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])\n#     def __len__(self): return len(self.image_files)\n#     def find_upper_cloth_label(self, pose_keypoints, parse_array):\n#         torso_indices = [1, 2, 5, 8, 9, 12]; visible_points = []\n#         for i in torso_indices:\n#             if i < len(pose_keypoints): x, y, conf = pose_keypoints[i];\n#             if conf > 0.1: visible_points.append((int(x), int(y)))\n#         if len(visible_points) < 3: return 5\n#         x_coords, y_coords = zip(*visible_points); min_x, max_x = min(x_coords), max(x_coords); min_y, max_y = min(y_coords), max(y_coords)\n#         if max_x <= min_x or max_y <= min_y: return 5\n#         torso_parse_area = parse_array[min_y:max_y, min_x:max_x]; unique_labels, counts = np.unique(torso_parse_area, return_counts=True); non_zero_mask = (unique_labels != 0)\n#         if np.any(non_zero_mask): return unique_labels[non_zero_mask][np.argmax(counts[non_zero_mask])]\n#         else: return 5\n#     def __getitem__(self, idx):\n#         image_name = self.image_files[idx]; base_name = os.path.splitext(image_name)[0]\n#         person_image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB'); cloth_image = Image.open(os.path.join(self.cloth_dir, image_name)).convert('RGB'); cloth_mask = Image.open(os.path.join(self.cloth_mask_dir, image_name)).convert('L')\n#         try:\n#             with open(os.path.join(self.pose_dir, f\"{base_name}_keypoints.json\"), 'r') as f: pose_data = json.load(f)\n#             pose_keypoints = np.array(pose_data['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n#         except (FileNotFoundError, IndexError): pose_keypoints = np.zeros((25, 3), dtype=np.float32)\n#         parse_path = os.path.join(self.parse_dir, f\"{base_name}.png\")\n#         if not os.path.exists(parse_path): parse_path = os.path.join(self.parse_dir, f\"{base_name}.jpg\")\n#         parse_array_orig = np.array(Image.open(parse_path).convert('L'))\n#         upper_cloth_label = self.find_upper_cloth_label(pose_keypoints, parse_array_orig)\n#         parse_array_resized = cv2.resize(parse_array_orig, self.image_size[::-1], interpolation=cv2.INTER_NEAREST)\n#         person_cloth_mask = (parse_array_resized == upper_cloth_label).astype(np.float32)\n#         person_image_tensor = self.transform(person_image); cloth_image_tensor = self.transform(cloth_image); cloth_mask_tensor = self.mask_transform(cloth_mask)\n#         pose_map_tensor = torch.from_numpy(self.create_pose_map(pose_keypoints)).float()\n#         blurred_mask_tensor = torch.from_numpy(cv2.GaussianBlur(person_cloth_mask, (5, 5), 0)).unsqueeze(0)\n#         agnostic_person_tensor = person_image_tensor * (1 - blurred_mask_tensor)\n#         warped_cloth_tensor = person_image_tensor * torch.from_numpy(person_cloth_mask).unsqueeze(0)\n#         return {'person_image': person_image_tensor, 'cloth_image': cloth_image_tensor, 'cloth_mask': cloth_mask_tensor, 'agnostic_person': agnostic_person_tensor, 'pose_map': pose_map_tensor, 'warped_cloth': warped_cloth_tensor}\n#     def create_pose_map(self, keypoints):\n#         h, w = self.image_size; orig_w, orig_h = self.original_image_size; num_keypoints = keypoints.shape[0]\n#         pose_map = np.zeros((num_keypoints, h, w), dtype=np.float32)\n#         for i, point in enumerate(keypoints):\n#             if point[2] > 0.1:\n#                 x, y = int(point[0] * w / orig_w), int(point[1] * h / orig_h)\n#                 if 0 <= x < w and 0 <= y < h: cv2.circle(pose_map[i], (x, y), radius=3, color=1, thickness=-1)\n#         return pose_map\n\n# class GMM(nn.Module):\n#     def __init__(self, in_channels_cloth=3, in_channels_pose=25, out_channels_flow=2):\n#         super(GMM, self).__init__(); self.encoder1 = self.conv_block(in_channels_cloth + in_channels_pose, 64); self.encoder2 = self.conv_block(64, 128); self.encoder3 = self.conv_block(128, 256); self.encoder4 = self.conv_block(256, 512); self.pool = nn.MaxPool2d(2, 2); self.bottleneck = self.conv_block(512, 1024); self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2); self.decoder4 = self.conv_block(1024, 512); self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2); self.decoder3 = self.conv_block(512, 256); self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2); self.decoder2 = self.conv_block(256, 128); self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2); self.decoder1 = self.conv_block(128, 64); self.conv_out = nn.Conv2d(64, out_channels_flow, kernel_size=1); self.tanh = nn.Tanh()\n#     def conv_block(self, in_channels, out_channels): return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True))\n#     def forward(self, cloth_image, pose_map):\n#         x = torch.cat([cloth_image, pose_map], dim=1); e1 = self.encoder1(x); p1 = self.pool(e1); e2 = self.encoder2(p1); p2 = self.pool(e2); e3 = self.encoder3(p2); p3 = self.pool(e3); e4 = self.encoder4(p3); p4 = self.pool(e4); b = self.bottleneck(p4); d4 = self.upconv4(b); d4 = torch.cat([d4, e4], dim=1); d4 = self.decoder4(d4); d3 = self.upconv3(d4); d3 = torch.cat([d3, e3], dim=1); d3 = self.decoder3(d3); d2 = self.upconv2(d3); d2 = torch.cat([d2, e2], dim=1); d2 = self.decoder2(d2); d1 = self.upconv1(d2); d1 = torch.cat([d1, e1], dim=1); d1 = self.decoder1(d1); flow_field = self.conv_out(d1); flow_field = self.tanh(flow_field); return flow_field\n\n# def warp_cloth_with_flow(cloth_image, flow_field):\n#     flow_field = flow_field.permute(0, 2, 3, 1); warped_image = F.grid_sample(cloth_image, flow_field, mode='bilinear', padding_mode='zeros', align_corners=True); return warped_image\n\n# class VGGPerceptualLoss(nn.Module):\n#     def __init__(self, resize=True):\n#         super(VGGPerceptualLoss, self).__init__(); vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features; self.vgg_layers = vgg[:35].eval();\n#         for param in self.vgg_layers.parameters(): param.requires_grad = False\n#         self.l1 = nn.L1Loss(); self.transform = nn.functional.interpolate; self.resize = resize\n#         self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)); self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n#     def forward(self, pred, target):\n#         pred = (pred + 1) / 2; target = (target + 1) / 2; pred = (pred - self.mean) / self.std; target = (target - self.mean) / self.std\n#         if self.resize: pred = self.transform(pred, mode='bilinear', size=(224, 224), align_corners=False); target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n#         pred_features = self.vgg_layers(pred); target_features = self.vgg_layers(target); return self.l1(pred_features, target_features)\n\n# class TVLoss(nn.Module):\n#     def __init__(self): super(TVLoss, self).__init__()\n#     def forward(self, x):\n#         batch_size, c, h, w = x.size(); tv_h = torch.pow(x[:,:,1:,:] - x[:,:,:-1,:], 2).sum(); tv_w = torch.pow(x[:,:,:,1:] - x[:,:,:,:-1], 2).sum()\n#         return (tv_h + tv_w) / (batch_size * c * h * w)\n\n# # --- CORRECTED Distributed Training Setup ---\n# def setup(rank, world_size):\n#     # Try to get the master address from the environment variables set by torchrun\n#     master_addr = os.environ.get(\"MASTER_ADDR\", \"localhost\")\n#     master_port = os.environ.get(\"MASTER_PORT\", \"12355\")\n    \n#     # If the master address is still localhost, try to find the actual IP.\n#     # This is a robust fallback for environments like Kaggle.\n#     if master_addr == \"localhost\":\n#         try:\n#             hostname = socket.gethostname()\n#             master_addr = socket.gethostbyname(hostname)\n#         except socket.gaierror:\n#             # If that fails, fall back to the loopback address.\n#             master_addr = \"127.0.0.1\"\n\n#     os.environ['MASTER_ADDR'] = master_addr\n#     os.environ['MASTER_PORT'] = master_port\n    \n#     if rank == 0:\n#         print(f\"Initializing process group... MASTER_ADDR={master_addr}, MASTER_PORT={master_port}\")\n        \n#     dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n# def cleanup():\n#     dist.destroy_process_group()\n\n# # --- Main Training Function (Unchanged) ---\n# def train(rank, world_size):\n#     setup(rank, world_size)\n    \n#     # --- Configuration ---\n#     DATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'\n#     CHECKPOINT_DIR = '/kaggle/working/'\n#     VISUALIZATION_DIR = '/kaggle/working/'\n#     BATCH_SIZE = 8; NUM_EPOCHS = 50; LEARNING_RATE = 2e-5\n#     IMAGE_SIZE = (256, 192); ORIGINAL_IMAGE_SIZE = (768, 1024); ACCUMULATION_STEPS = 4\n\n#     # --- Setup Device and Model ---\n#     torch.cuda.set_device(rank)\n#     gmm = GMM(in_channels_pose=25).to(rank)\n#     gmm = DDP(gmm, device_ids=[rank], find_unused_parameters=True) # find_unused_parameters can help with complex models\n    \n#     optimizer = optim.Adam(gmm.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n#     l1_loss_fn = nn.L1Loss().to(rank); perceptual_loss_fn = VGGPerceptualLoss().to(rank); tv_loss_fn = TVLoss().to(rank)\n#     scaler = torch.cuda.amp.GradScaler()\n\n#     # --- Resumption Logic ---\n#     start_epoch = 0; checkpoint_path = os.path.join(CHECKPOINT_DIR, 'gmm_latest_ddp.pth')\n#     if os.path.isfile(checkpoint_path) and rank == 0:\n#         print(f\"Loading checkpoint: {checkpoint_path}\")\n#         checkpoint = torch.load(checkpoint_path, map_location={'cuda:0': f'cuda:{rank}'})\n#         gmm.module.load_state_dict(checkpoint['model_state_dict'])\n#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#         scaler.load_state_dict(checkpoint['scaler_state_dict'])\n#         start_epoch = checkpoint['epoch'] + 1\n#         print(f\"Resumed from Epoch {start_epoch}\")\n    \n#     dist.barrier() \n\n#     # --- Data Loading with Distributed Sampler ---\n#     dataset = FashionVTONDataset(data_root=DATA_ROOT, image_size=IMAGE_SIZE, original_image_size=ORIGINAL_IMAGE_SIZE)\n#     sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n#     loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)\n\n#     # --- Training Loop ---\n#     for epoch in range(start_epoch, NUM_EPOCHS):\n#         sampler.set_epoch(epoch); gmm.train()\n#         progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", disable=(rank != 0))\n\n#         for i, batch in enumerate(progress_bar):\n#             cloth_image = batch['cloth_image'].to(rank); cloth_mask = batch['cloth_mask'].to(rank)\n#             pose_map = batch['pose_map'].to(rank); ground_truth_warped = batch['warped_cloth'].to(rank)\n            \n#             with torch.cuda.amp.autocast():\n#                 predicted_flow = gmm(cloth_image, pose_map)\n#                 predicted_warped = warp_cloth_with_flow(cloth_image, predicted_flow)\n#                 warped_cloth_mask = warp_cloth_with_flow(cloth_mask, predicted_flow)\n#                 loss_l1 = l1_loss_fn(predicted_warped * warped_cloth_mask, ground_truth_warped * warped_cloth_mask)\n#                 loss_p = perceptual_loss_fn(predicted_warped, ground_truth_warped)\n#                 loss_tv = tv_loss_fn(predicted_flow)\n#                 total_loss = (10 * loss_l1) + loss_p + (0.5 * loss_tv)\n#                 total_loss = total_loss / ACCUMULATION_STEPS\n\n#             scaler.scale(total_loss).backward()\n            \n#             if (i + 1) % ACCUMULATION_STEPS == 0:\n#                 scaler.step(optimizer); scaler.update(); optimizer.zero_grad()\n            \n#             if rank == 0: progress_bar.set_postfix(loss=total_loss.item() * ACCUMULATION_STEPS)\n        \n#         if rank == 0:\n#             latest_checkpoint_state = {\n#                 'epoch': epoch, 'model_state_dict': gmm.module.state_dict(),\n#                 'optimizer_state_dict': optimizer.state_dict(), 'scaler_state_dict': scaler.state_dict(),\n#             }\n#             torch.save(latest_checkpoint_state, checkpoint_path)\n            \n#             with torch.no_grad():\n#                 vis_batch = torch.cat([(cloth_image.cpu() + 1) / 2, (predicted_warped.cpu().detach() + 1) / 2, (ground_truth_warped.cpu() + 1) / 2], dim=0)\n#                 save_image(vis_batch, os.path.join(VISUALIZATION_DIR, f'epoch_{epoch+1}_comparison.png'), nrow=BATCH_SIZE)\n\n#             print(f\"Epoch {epoch+1} finished. Checkpoint saved.\")\n\n#     if rank == 0:\n#         final_model_path = os.path.join(CHECKPOINT_DIR, 'gmm_final.pth')\n#         torch.save(gmm.module.state_dict(), final_model_path)\n#         print(f\"Final model saved to {final_model_path}\")\n        \n#     cleanup()\n\n# if __name__ == '__main__':\n#     world_size = torch.cuda.device_count()\n#     if world_size < 2:\n#         print(\"Distributed training requires at least 2 GPUs.\")\n#     else:\n#         # Use torch.multiprocessing.spawn which is often more stable in notebooks than torchrun\n#         torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:25:22.633537Z","iopub.execute_input":"2025-07-16T15:25:22.636374Z","iopub.status.idle":"2025-07-16T15:25:22.664123Z","shell.execute_reply.started":"2025-07-16T15:25:22.636337Z","shell.execute_reply":"2025-07-16T15:25:22.662365Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!python train_gmm_ddp.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:43:09.512768Z","iopub.execute_input":"2025-07-16T14:43:09.513035Z","iopub.status.idle":"2025-07-16T15:25:22.631752Z","shell.execute_reply.started":"2025-07-16T14:43:09.513015Z","shell.execute_reply":"2025-07-16T15:25:22.628736Z"}},"outputs":[{"name":"stdout","text":"Found 2 GPUs. Starting DDP with file sync at /kaggle/working/ddp_sync_file\nInitializing process group with: file:///kaggle/working/ddp_sync_file\n/kaggle/working/train_gmm_ddp.py:115: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(); start_epoch = 0; checkpoint_path = os.path.join(CHECKPOINT_DIR, 'gmm_latest_ddp.pth')\n/kaggle/working/train_gmm_ddp.py:115: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(); start_epoch = 0; checkpoint_path = os.path.join(CHECKPOINT_DIR, 'gmm_latest_ddp.pth')\nEpoch 1/50:   0%|                                       | 0/728 [00:00<?, ?it/s]/kaggle/working/train_gmm_ddp.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/kaggle/working/train_gmm_ddp.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n[rank1]:[W716 14:43:33.373235397 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n[rank0]:[W716 14:43:33.470983007 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\nEpoch 1/50: 100%|██████████████████| 728/728 [04:18<00:00,  2.82it/s, loss=8.14]\nEpoch 1 finished. Checkpoint saved.\nEpoch 2/50: 100%|██████████████████| 728/728 [03:55<00:00,  3.09it/s, loss=7.64]\nEpoch 2 finished. Checkpoint saved.\nEpoch 3/50: 100%|██████████████████| 728/728 [03:54<00:00,  3.10it/s, loss=6.68]\nEpoch 3 finished. Checkpoint saved.\nEpoch 4/50: 100%|██████████████████| 728/728 [03:53<00:00,  3.12it/s, loss=6.15]\nEpoch 4 finished. Checkpoint saved.\nEpoch 5/50: 100%|██████████████████| 728/728 [04:00<00:00,  3.03it/s, loss=8.42]\nEpoch 5 finished. Checkpoint saved.\nEpoch 6/50: 100%|██████████████████| 728/728 [04:02<00:00,  3.00it/s, loss=7.32]\nEpoch 6 finished. Checkpoint saved.\nEpoch 7/50: 100%|███████████████████| 728/728 [03:59<00:00,  3.03it/s, loss=8.3]\nEpoch 7 finished. Checkpoint saved.\nEpoch 8/50: 100%|██████████████████| 728/728 [03:56<00:00,  3.07it/s, loss=7.19]\nEpoch 8 finished. Checkpoint saved.\nEpoch 9/50: 100%|██████████████████| 728/728 [03:55<00:00,  3.10it/s, loss=7.01]\nEpoch 9 finished. Checkpoint saved.\nEpoch 10/50: 100%|█████████████████| 728/728 [03:53<00:00,  3.12it/s, loss=6.81]\nEpoch 10 finished. Checkpoint saved.\nEpoch 11/50:  50%|████████▍        | 363/728 [02:01<01:41,  3.59it/s, loss=1.56]^C\nTraceback (most recent call last):\n  File \"/kaggle/working/train_gmm_ddp.py\", line 161, in <module>\n    torch.multiprocessing.spawn(train, args=(world_size, sync_file_path), nprocs=world_size, join=True)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 340, in spawn\n    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 296, in start_processes\n    while not context.join():\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 144, in join\n    ready = multiprocessing.connection.wait(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n    ready = selector.select(timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n    fd_event_list = self._selector.poll(timeout)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"%%writefile train_gmm_ddp.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image\nimport os\nfrom tqdm import tqdm\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nimport torchvision.models as models\nimport json\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\nimport time # Import time for the wait mechanism\nimport torch.nn.functional as F # <<< THIS IS THE FIX\n\n# ===================================================================\n# ALL CLASS DEFINITIONS (These are unchanged)\n# ===================================================================\nclass FashionVTONDataset(Dataset):\n    def __init__(self, data_root, image_size=(256, 192), original_image_size=(768, 1024)):\n        self.data_root = data_root; self.image_size = image_size; self.original_image_size = original_image_size; self.image_dir = os.path.join(data_root, 'image'); self.cloth_dir = os.path.join(data_root, 'cloth'); self.cloth_mask_dir = os.path.join(data_root, 'cloth-mask'); self.pose_dir = os.path.join(data_root, 'openpose_json'); self.parse_dir = os.path.join(data_root, 'image-parse-v3'); self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))]); self.transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BILINEAR), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]); self.mask_transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])\n    def __len__(self): return len(self.image_files)\n    def find_upper_cloth_label(self, pose_keypoints, parse_array):\n        torso_indices = [1, 2, 5, 8, 9, 12]; visible_points = []\n        for i in torso_indices:\n            if i < len(pose_keypoints): x, y, conf = pose_keypoints[i];\n            if conf > 0.1: visible_points.append((int(x), int(y)))\n        if len(visible_points) < 3: return 5\n        x_coords, y_coords = zip(*visible_points); min_x, max_x = min(x_coords), max(x_coords); min_y, max_y = min(y_coords), max(y_coords)\n        if max_x <= min_x or max_y <= min_y: return 5\n        torso_parse_area = parse_array[min_y:max_y, min_x:max_x]; unique_labels, counts = np.unique(torso_parse_area, return_counts=True); non_zero_mask = (unique_labels != 0)\n        if np.any(non_zero_mask): return unique_labels[non_zero_mask][np.argmax(counts[non_zero_mask])]\n        else: return 5\n    def __getitem__(self, idx):\n        image_name = self.image_files[idx]; base_name = os.path.splitext(image_name)[0]\n        person_image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB'); cloth_image = Image.open(os.path.join(self.cloth_dir, image_name)).convert('RGB'); cloth_mask = Image.open(os.path.join(self.cloth_mask_dir, image_name)).convert('L')\n        try:\n            with open(os.path.join(self.pose_dir, f\"{base_name}_keypoints.json\"), 'r') as f: pose_data = json.load(f)\n            pose_keypoints = np.array(pose_data['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n        except (FileNotFoundError, IndexError): pose_keypoints = np.zeros((25, 3), dtype=np.float32)\n        parse_path = os.path.join(self.parse_dir, f\"{base_name}.png\")\n        if not os.path.exists(parse_path): parse_path = os.path.join(self.parse_dir, f\"{base_name}.jpg\")\n        parse_array_orig = np.array(Image.open(parse_path).convert('L'))\n        upper_cloth_label = self.find_upper_cloth_label(pose_keypoints, parse_array_orig)\n        parse_array_resized = cv2.resize(parse_array_orig, self.image_size[::-1], interpolation=cv2.INTER_NEAREST)\n        person_cloth_mask = (parse_array_resized == upper_cloth_label).astype(np.float32)\n        person_image_tensor = self.transform(person_image); cloth_image_tensor = self.transform(cloth_image); cloth_mask_tensor = self.mask_transform(cloth_mask)\n        pose_map_tensor = torch.from_numpy(self.create_pose_map(pose_keypoints)).float()\n        blurred_mask_tensor = torch.from_numpy(cv2.GaussianBlur(person_cloth_mask, (5, 5), 0)).unsqueeze(0)\n        agnostic_person_tensor = person_image_tensor * (1 - blurred_mask_tensor)\n        warped_cloth_tensor = person_image_tensor * torch.from_numpy(person_cloth_mask).unsqueeze(0)\n        return {'person_image': person_image_tensor, 'cloth_image': cloth_image_tensor, 'cloth_mask': cloth_mask_tensor, 'agnostic_person': agnostic_person_tensor, 'pose_map': pose_map_tensor, 'warped_cloth': warped_cloth_tensor}\n    def create_pose_map(self, keypoints):\n        h, w = self.image_size; orig_w, orig_h = self.original_image_size; num_keypoints = keypoints.shape[0]\n        pose_map = np.zeros((num_keypoints, h, w), dtype=np.float32)\n        for i, point in enumerate(keypoints):\n            if point[2] > 0.1:\n                x, y = int(point[0] * w / orig_w), int(point[1] * h / orig_h)\n                if 0 <= x < w and 0 <= y < h: cv2.circle(pose_map[i], (x, y), radius=3, color=1, thickness=-1)\n        return pose_map\n\nclass GMM(nn.Module):\n    def __init__(self, in_channels_cloth=3, in_channels_pose=25, out_channels_flow=2):\n        super(GMM, self).__init__(); self.encoder1 = self.conv_block(in_channels_cloth + in_channels_pose, 64); self.encoder2 = self.conv_block(64, 128); self.encoder3 = self.conv_block(128, 256); self.encoder4 = self.conv_block(256, 512); self.pool = nn.MaxPool2d(2, 2); self.bottleneck = self.conv_block(512, 1024); self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2); self.decoder4 = self.conv_block(1024, 512); self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2); self.decoder3 = self.conv_block(512, 256); self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2); self.decoder2 = self.conv_block(256, 128); self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2); self.decoder1 = self.conv_block(128, 64); self.conv_out = nn.Conv2d(64, out_channels_flow, kernel_size=1); self.tanh = nn.Tanh()\n    def conv_block(self, in_channels, out_channels): return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True))\n    def forward(self, cloth_image, pose_map):\n        x = torch.cat([cloth_image, pose_map], dim=1); e1 = self.encoder1(x); p1 = self.pool(e1); e2 = self.encoder2(p1); p2 = self.pool(e2); e3 = self.encoder3(p2); p3 = self.pool(e3); e4 = self.encoder4(p3); p4 = self.pool(e4); b = self.bottleneck(p4); d4 = self.upconv4(b); d4 = torch.cat([d4, e4], dim=1); d4 = self.decoder4(d4); d3 = self.upconv3(d4); d3 = torch.cat([d3, e3], dim=1); d3 = self.decoder3(d3); d2 = self.upconv2(d3); d2 = torch.cat([d2, e2], dim=1); d2 = self.decoder2(d2); d1 = self.upconv1(d2); d1 = torch.cat([d1, e1], dim=1); d1 = self.decoder1(d1); flow_field = self.conv_out(d1); flow_field = self.tanh(flow_field); return flow_field\n\ndef warp_cloth_with_flow(cloth_image, flow_field):\n    flow_field = flow_field.permute(0, 2, 3, 1); warped_image = F.grid_sample(cloth_image, flow_field, mode='bilinear', padding_mode='zeros', align_corners=True); return warped_image\n\nclass VGGPerceptualLoss(nn.Module):\n    def __init__(self, resize=True):\n        super(VGGPerceptualLoss, self).__init__(); vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features; self.vgg_layers = vgg[:35].eval();\n        for param in self.vgg_layers.parameters(): param.requires_grad = False\n        self.l1 = nn.L1Loss(); self.transform = nn.functional.interpolate; self.resize = resize\n        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)); self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n    def forward(self, pred, target):\n        pred = (pred + 1) / 2; target = (target + 1) / 2; pred = (pred - self.mean) / self.std; target = (target - self.mean) / self.std\n        if self.resize: pred = self.transform(pred, mode='bilinear', size=(224, 224), align_corners=False); target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n        pred_features = self.vgg_layers(pred); target_features = self.vgg_layers(target); return self.l1(pred_features, target_features)\n\nclass TVLoss(nn.Module):\n    def __init__(self): super(TVLoss, self).__init__()\n    def forward(self, x):\n        batch_size, c, h, w = x.size(); tv_h = torch.pow(x[:,:,1:,:] - x[:,:,:-1,:], 2).sum(); tv_w = torch.pow(x[:,:,:,1:] - x[:,:,:,:-1], 2).sum()\n        return (tv_h + tv_w) / (batch_size * c * h * w)\n\n# --- CORRECTED Distributed Training Setup using File Store ---\ndef setup(rank, world_size, sync_file):\n    # Set the initialization method to use the shared file system\n    init_method = f'file://{sync_file}'\n    \n    if rank == 0:\n        print(f\"Initializing process group with: {init_method}\")\n        \n    dist.init_process_group(\"nccl\", init_method=init_method, rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n# --- Main Training Function (Unchanged) ---\ndef train(rank, world_size, sync_file):\n    setup(rank, world_size, sync_file)\n    \n    # ... (The rest of the train function is exactly the same as the previous version) ...\n    DATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'; CHECKPOINT_DIR = '/kaggle/working/'; VISUALIZATION_DIR = '/kaggle/working/'\n    BATCH_SIZE = 8; NUM_EPOCHS = 50; LEARNING_RATE = 2e-5; IMAGE_SIZE = (256, 192); ORIGINAL_IMAGE_SIZE = (768, 1024); ACCUMULATION_STEPS = 4\n    torch.cuda.set_device(rank); gmm = GMM(in_channels_pose=25).to(rank); gmm = DDP(gmm, device_ids=[rank], find_unused_parameters=True)\n    optimizer = optim.Adam(gmm.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999)); l1_loss_fn = nn.L1Loss().to(rank); perceptual_loss_fn = VGGPerceptualLoss().to(rank); tv_loss_fn = TVLoss().to(rank)\n    scaler = torch.cuda.amp.GradScaler(); start_epoch = 0; checkpoint_path = os.path.join(CHECKPOINT_DIR, 'gmm_latest_ddp.pth')\n    if os.path.isfile(checkpoint_path) and rank == 0:\n        print(f\"Loading checkpoint: {checkpoint_path}\"); checkpoint = torch.load(checkpoint_path, map_location={'cuda:0': f'cuda:{rank}'})\n        gmm.module.load_state_dict(checkpoint['model_state_dict']); optimizer.load_state_dict(checkpoint['optimizer_state_dict']); scaler.load_state_dict(checkpoint['scaler_state_dict']); start_epoch = checkpoint['epoch'] + 1\n        print(f\"Resumed from Epoch {start_epoch}\")\n    dist.barrier(); dataset = FashionVTONDataset(data_root=DATA_ROOT, image_size=IMAGE_SIZE, original_image_size=ORIGINAL_IMAGE_SIZE)\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True); loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)\n    for epoch in range(start_epoch, NUM_EPOCHS):\n        sampler.set_epoch(epoch); gmm.train()\n        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", disable=(rank != 0))\n        for i, batch in enumerate(progress_bar):\n            cloth_image = batch['cloth_image'].to(rank); cloth_mask = batch['cloth_mask'].to(rank); pose_map = batch['pose_map'].to(rank); ground_truth_warped = batch['warped_cloth'].to(rank)\n            with torch.cuda.amp.autocast():\n                predicted_flow = gmm(cloth_image, pose_map); predicted_warped = warp_cloth_with_flow(cloth_image, predicted_flow); warped_cloth_mask = warp_cloth_with_flow(cloth_mask, predicted_flow)\n                loss_l1 = l1_loss_fn(predicted_warped * warped_cloth_mask, ground_truth_warped * warped_cloth_mask); loss_p = perceptual_loss_fn(predicted_warped, ground_truth_warped); loss_tv = tv_loss_fn(predicted_flow)\n                total_loss = (10 * loss_l1) + loss_p + (0.5 * loss_tv); total_loss = total_loss / ACCUMULATION_STEPS\n            scaler.scale(total_loss).backward()\n            if (i + 1) % ACCUMULATION_STEPS == 0: scaler.step(optimizer); scaler.update(); optimizer.zero_grad()\n            if rank == 0: progress_bar.set_postfix(loss=total_loss.item() * ACCUMULATION_STEPS)\n        if rank == 0:\n            latest_checkpoint_state = {'epoch': epoch, 'model_state_dict': gmm.module.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'scaler_state_dict': scaler.state_dict()}\n            torch.save(latest_checkpoint_state, checkpoint_path)\n            with torch.no_grad():\n                vis_batch = torch.cat([(cloth_image.cpu() + 1) / 2, (predicted_warped.cpu().detach() + 1) / 2, (ground_truth_warped.cpu() + 1) / 2], dim=0)\n                save_image(vis_batch, os.path.join(VISUALIZATION_DIR, f'epoch_{epoch+1}_comparison.png'), nrow=BATCH_SIZE)\n            print(f\"Epoch {epoch+1} finished. Checkpoint saved.\")\n    if rank == 0:\n        final_model_path = os.path.join(CHECKPOINT_DIR, 'gmm_final.pth')\n        torch.save(gmm.module.state_dict(), final_model_path)\n        print(f\"Final model saved to {final_model_path}\")\n    cleanup()\n\nif __name__ == '__main__':\n    world_size = torch.cuda.device_count()\n    if world_size < 2:\n        print(\"Distributed training requires at least 2 GPUs.\")\n    else:\n        # Define the path for the sync file in a writable directory\n        sync_file_path = os.path.join('/kaggle/working', 'ddp_sync_file')\n        \n        # Ensure the sync file doesn't exist from a previous failed run\n        if os.path.exists(sync_file_path):\n            os.remove(sync_file_path)\n            \n        print(f\"Found {world_size} GPUs. Starting DDP with file sync at {sync_file_path}\")\n        # Pass the sync_file_path to the train function\n        torch.multiprocessing.spawn(train, args=(world_size, sync_file_path), nprocs=world_size, join=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:43:00.349512Z","iopub.execute_input":"2025-07-16T14:43:00.350046Z","iopub.status.idle":"2025-07-16T14:43:00.359546Z","shell.execute_reply.started":"2025-07-16T14:43:00.350020Z","shell.execute_reply":"2025-07-16T14:43:00.358631Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_gmm_ddp.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ===================================================================\n# FINAL KAGGLE NOTEBOOK CELL\n# Includes all class definitions and Option 2 (robust, resumable training)\n# ===================================================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport os\nimport json\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom tqdm import tqdm\nfrom diffusers import AutoencoderKL, DDPMScheduler\nfrom transformers import logging\n\n# Suppress verbose messages from transformers\nlogging.set_verbosity_error()\n\n\n# --- CLASS DEFINITIONS ---\nclass FashionVTONDataset(Dataset):\n    def __init__(self, data_root, image_size=(256, 192), original_image_size=(768, 1024)):\n        self.data_root = data_root; self.image_size = image_size; self.original_image_size = original_image_size; self.image_dir = os.path.join(data_root, 'image'); self.cloth_dir = os.path.join(data_root, 'cloth'); self.cloth_mask_dir = os.path.join(data_root, 'cloth-mask'); self.pose_dir = os.path.join(data_root, 'openpose_json'); self.parse_dir = os.path.join(data_root, 'image-parse-v3'); self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))]); self.transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BILINEAR), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]); self.mask_transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])\n    def __len__(self): return len(self.image_files)\n    def find_upper_cloth_label(self, pose_keypoints, parse_array):\n        torso_indices = [1, 2, 5, 8, 9, 12]; visible_points = []\n        for i in torso_indices:\n            if i < len(pose_keypoints):\n                x, y, conf = pose_keypoints[i]\n                if conf > 0.1: visible_points.append((int(x), int(y)))\n        if len(visible_points) < 3: return 5\n        x_coords, y_coords = zip(*visible_points); min_x, max_x = min(x_coords), max(x_coords); min_y, max_y = min(y_coords), max(y_coords)\n        if max_x <= min_x or max_y <= min_y: return 5\n        torso_parse_area = parse_array[min_y:max_y, min_x:max_x]; unique_labels, counts = np.unique(torso_parse_area, return_counts=True); non_zero_mask = (unique_labels != 0)\n        if np.any(non_zero_mask): return unique_labels[non_zero_mask][np.argmax(counts[non_zero_mask])]\n        else: return 5\n    def __getitem__(self, idx):\n        image_name = self.image_files[idx]; base_name = os.path.splitext(image_name)[0]\n        person_image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB'); cloth_image = Image.open(os.path.join(self.cloth_dir, image_name)).convert('RGB'); cloth_mask = Image.open(os.path.join(self.cloth_mask_dir, image_name)).convert('L')\n        try:\n            with open(os.path.join(self.pose_dir, f\"{base_name}_keypoints.json\"), 'r') as f: pose_data = json.load(f)\n            pose_keypoints = np.array(pose_data['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n        except (FileNotFoundError, IndexError): pose_keypoints = np.zeros((25, 3), dtype=np.float32)\n        parse_path = os.path.join(self.parse_dir, f\"{base_name}.png\")\n        if not os.path.exists(parse_path): parse_path = os.path.join(self.parse_dir, f\"{base_name}.jpg\")\n        parse_array_orig = np.array(Image.open(parse_path).convert('L'))\n        upper_cloth_label = self.find_upper_cloth_label(pose_keypoints, parse_array_orig)\n        parse_array_resized = cv2.resize(parse_array_orig, self.image_size[::-1], interpolation=cv2.INTER_NEAREST)\n        person_cloth_mask = (parse_array_resized == upper_cloth_label).astype(np.float32)\n        person_image_tensor = self.transform(person_image); cloth_image_tensor = self.transform(cloth_image); cloth_mask_tensor = self.mask_transform(cloth_mask)\n        pose_map_tensor = torch.from_numpy(self.create_pose_map(pose_keypoints)).float()\n        blurred_mask_tensor = torch.from_numpy(cv2.GaussianBlur(person_cloth_mask, (5, 5), 0)).unsqueeze(0)\n        agnostic_person_tensor = person_image_tensor * (1 - blurred_mask_tensor)\n        warped_cloth_tensor = person_image_tensor * torch.from_numpy(person_cloth_mask).unsqueeze(0)\n        return {'person_image': person_image_tensor, 'cloth_image': cloth_image_tensor, 'cloth_mask': cloth_mask_tensor, 'agnostic_person': agnostic_person_tensor, 'pose_map': pose_map_tensor, 'warped_cloth': warped_cloth_tensor}\n    def create_pose_map(self, keypoints):\n        h, w = self.image_size; orig_w, orig_h = self.original_image_size; num_keypoints = keypoints.shape[0]\n        pose_map = np.zeros((num_keypoints, h, w), dtype=np.float32)\n        for i, point in enumerate(keypoints):\n            if point[2] > 0.1:\n                x, y = int(point[0] * w / orig_w), int(point[1] * h / orig_h)\n                if 0 <= x < w and 0 <= y < h: cv2.circle(pose_map[i], (x, y), radius=3, color=1, thickness=-1)\n        return pose_map\n\nclass GMM(nn.Module):\n    def __init__(self, in_channels_cloth=3, in_channels_pose=25, out_channels_flow=2):\n        super(GMM, self).__init__(); self.encoder1 = self.conv_block(in_channels_cloth + in_channels_pose, 64); self.encoder2 = self.conv_block(64, 128); self.encoder3 = self.conv_block(128, 256); self.encoder4 = self.conv_block(256, 512); self.pool = nn.MaxPool2d(2, 2); self.bottleneck = self.conv_block(512, 1024); self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2); self.decoder4 = self.conv_block(1024, 512); self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2); self.decoder3 = self.conv_block(512, 256); self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2); self.decoder2 = self.conv_block(256, 128); self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2); self.decoder1 = self.conv_block(128, 64); self.conv_out = nn.Conv2d(64, out_channels_flow, kernel_size=1); self.tanh = nn.Tanh()\n    def conv_block(self, in_channels, out_channels): return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True))\n    def forward(self, cloth_image, pose_map):\n        x = torch.cat([cloth_image, pose_map], dim=1); e1 = self.encoder1(x); p1 = self.pool(e1); e2 = self.encoder2(p1); p2 = self.pool(e2); e3 = self.encoder3(p2); p3 = self.pool(e3); e4 = self.encoder4(p3); p4 = self.pool(e4); b = self.bottleneck(p4); d4 = self.upconv4(b); d4 = torch.cat([d4, e4], dim=1); d4 = self.decoder4(d4); d3 = self.upconv3(d4); d3 = torch.cat([d3, e3], dim=1); d3 = self.decoder3(d3); d2 = self.upconv2(d3); d2 = torch.cat([d2, e2], dim=1); d2 = self.decoder2(d2); d1 = self.upconv1(d2); d1 = torch.cat([d1, e1], dim=1); d1 = self.decoder1(d1); flow_field = self.conv_out(d1); flow_field = self.tanh(flow_field); return flow_field\n\ndef warp_cloth_with_flow(cloth_image, flow_field):\n    flow_field = flow_field.permute(0, 2, 3, 1); warped_image = F.grid_sample(cloth_image, flow_field, mode='bilinear', padding_mode='zeros', align_corners=True); return warped_image\n\nclass SinusoidalPositionEmbeddings(nn.Module):\n    def __init__(self, dim): super().__init__(); self.dim = dim\n    def forward(self, time): device = time.device; half_dim = self.dim // 2; embeddings = math.log(10000) / (half_dim - 1); embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings); embeddings = time[:, None] * embeddings[None, :]; embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1); return embeddings\nclass ResnetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, *, time_emb_dim=None):\n        super().__init__(); self.mlp = (nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, out_channels)) if time_emb_dim is not None else None); self.block1 = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.GroupNorm(8, out_channels), nn.SiLU()); self.block2 = nn.Sequential(nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.GroupNorm(8, out_channels), nn.SiLU()); self.res_conv = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n    def forward(self, x, time_emb=None):\n        h = self.block1(x)\n        if self.mlp is not None and time_emb is not None:\n            time_emb = self.mlp(time_emb); h = h + time_emb.unsqueeze(-1).unsqueeze(-1)\n        h = self.block2(h); return h + self.res_conv(x)\nclass AttentionBlock(nn.Module):\n    def __init__(self, channels): super().__init__(); self.gn = nn.GroupNorm(8, channels); self.qkv = nn.Conv2d(channels, channels * 3, 1); self.out = nn.Conv2d(channels, channels, 1)\n    def forward(self, x, time_emb=None):\n        b, c, h, w = x.shape; x_in = x; x = self.gn(x); x = self.qkv(x); q, k, v = torch.chunk(x, 3, dim=1); q = q.view(b, c, h * w); k = k.view(b, c, h * w); v = v.view(b, c, h * w); k = k.softmax(dim=-1); attn = torch.einsum(\"b c i, b c j -> b i j\", q, k); out = torch.einsum(\"b i j, b c j -> b c i\", attn, v); out = out.view(b, c, h, w); return self.out(out) + x_in\nclass ConditionalUNet(nn.Module):\n    def __init__(self, in_channels, model_channels, out_channels, time_emb_dim=256, condition_channels=6):\n        super().__init__(); self.time_mlp = nn.Sequential(SinusoidalPositionEmbeddings(time_emb_dim), nn.Linear(time_emb_dim, time_emb_dim), nn.ReLU()); self.init_conv = nn.Conv2d(in_channels + condition_channels, model_channels, kernel_size=3, padding=1); self.down1 = ResnetBlock(model_channels, 128, time_emb_dim=time_emb_dim); self.down2 = ResnetBlock(128, 128, time_emb_dim=time_emb_dim); self.down3 = ResnetBlock(128, 256, time_emb_dim=time_emb_dim); self.down4 = AttentionBlock(256); self.down5 = ResnetBlock(256, 256, time_emb_dim=time_emb_dim); self.down6 = ResnetBlock(256, 512, time_emb_dim=time_emb_dim); self.pool = nn.MaxPool2d(2); self.mid1 = ResnetBlock(512, 1024, time_emb_dim=time_emb_dim); self.mid_attn = AttentionBlock(1024); self.mid2 = ResnetBlock(1024, 512, time_emb_dim=time_emb_dim); self.up1 = nn.ConvTranspose2d(512, 256, 2, 2); self.up_res1 = ResnetBlock(512, 256, time_emb_dim=time_emb_dim); self.up_attn1 = AttentionBlock(256); self.up_res2 = ResnetBlock(256, 256, time_emb_dim=time_emb_dim); self.up2 = nn.ConvTranspose2d(256, 128, 2, 2); self.up_res3 = ResnetBlock(256, 128, time_emb_dim=time_emb_dim); self.up_res4 = ResnetBlock(128, 128, time_emb_dim=time_emb_dim); self.out_res = ResnetBlock(128 + model_channels, 64, time_emb_dim=time_emb_dim); self.out_conv = nn.Conv2d(64, out_channels, 1)\n    def forward(self, x, time, condition):\n        t = self.time_mlp(time); condition_downsampled = F.interpolate(condition, size=x.shape[2:], mode='bilinear', align_corners=False); x = torch.cat([x, condition_downsampled], dim=1); x = self.init_conv(x); r0 = x.clone(); x = self.down1(x, t); x = self.down2(x, t); r1 = x.clone(); x = self.pool(x); x = self.down3(x, t); x = self.down4(x, t); x = self.down5(x, t); r2 = x.clone(); x = self.pool(x); x = self.down6(x, t); x = self.mid1(x, t); x = self.mid_attn(x, t); x = self.mid2(x, t); x = self.up1(x); x = torch.cat([x, r2], dim=1); x = self.up_res1(x, t); x = self.up_attn1(x, t); x = self.up_res2(x, t); x = self.up2(x); x = torch.cat([x, r1], dim=1); x = self.up_res3(x, t); x = self.up_res4(x, t); x = torch.cat([x, r0], dim=1); x = self.out_res(x, t); return self.out_conv(x)\n\n\n# --- CONFIGURATION & TRAINING SCRIPT ---\nDATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'\nOUTPUT_DIR = '/kaggle/working/'\nGMM_CHECKPOINT_PATH = '/kaggle/input/gmm_5epoch/pytorch/default/1/gmm_final (works but low epochs).pth'\nCHECKPOINT_DIR_DIFF = os.path.join(OUTPUT_DIR, 'checkpoints_diffusion')\nVISUALIZATION_DIR_DIFF = os.path.join(OUTPUT_DIR, 'visualizations_diffusion')\n\nBATCH_SIZE = 2; NUM_EPOCHS = 100; LEARNING_RATE = 1e-4; IMAGE_SIZE = (256, 192); ORIGINAL_IMAGE_SIZE = (768, 1024); VALIDATION_SPLIT = 0.1\nVAE_MODEL_ID = \"stabilityai/stable-diffusion-2-1-base\"; VAE_SUBFOLDER = \"vae\"; NUM_TRAIN_TIMESTEPS = 1000\n\nos.makedirs(CHECKPOINT_DIR_DIFF, exist_ok=True); os.makedirs(VISUALIZATION_DIR_DIFF, exist_ok=True)\n\n@torch.no_grad()\ndef evaluate_and_visualize(epoch, unet, gmm, vae, noise_scheduler, val_loader, device):\n    unet.eval(); gmm.eval(); val_loss = 0.0; progress_bar = tqdm(val_loader, desc=\"Validating\", leave=False)\n    for batch in progress_bar:\n        person_image = batch['person_image'].to(device); cloth_image = batch['cloth_image'].to(device); pose_map = batch['pose_map'].to(device); agnostic_person = batch['agnostic_person'].to(device)\n        flow = gmm(cloth_image, pose_map); warped_cloth = warp_cloth_with_flow(cloth_image, flow); condition = torch.cat([agnostic_person, warped_cloth], dim=1)\n        latents = vae.encode(person_image).latent_dist.sample() * vae.config.scaling_factor\n        noise = torch.randn_like(latents); timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps); predicted_noise = unet(noisy_latents, timesteps, condition); loss = F.mse_loss(predicted_noise, noise); val_loss += loss.item()\n    avg_val_loss = val_loss / len(val_loader)\n    sample_condition = condition[0:1]; original_vis_image = person_image[0:1]; sample_latents = torch.randn(1, 4, IMAGE_SIZE[0] // 8, IMAGE_SIZE[1] // 8).to(device)\n    for t in tqdm(noise_scheduler.timesteps, desc=\"Generating sample\", leave=False):\n        pred_noise = unet(sample_latents, t.unsqueeze(0).to(device), sample_condition); sample_latents = noise_scheduler.step(pred_noise, t, sample_latents).prev_sample\n    sample_latents = 1 / vae.config.scaling_factor * sample_latents; generated_image = vae.decode(sample_latents).sample\n    original_vis = (original_vis_image + 1) / 2; condition_vis = (sample_condition[:, :3] + 1) / 2; warped_vis = (sample_condition[:, 3:] + 1) / 2; generated_vis = (generated_image + 1) / 2\n    comparison = torch.cat([original_vis, condition_vis, warped_vis, generated_vis], dim=0); save_image(comparison, os.path.join(VISUALIZATION_DIR_DIFF, f'epoch_{epoch+1}_sample.png'), nrow=4)\n    unet.train(); return avg_val_loss\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n    vae = AutoencoderKL.from_pretrained(VAE_MODEL_ID, subfolder=VAE_SUBFOLDER).to(device)\n    gmm = GMM(in_channels_pose=25).to(device)\n    unet = ConditionalUNet(in_channels=4, model_channels=128, out_channels=4, condition_channels=6).to(device)\n    optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n    noise_scheduler = DDPMScheduler(num_train_timesteps=NUM_TRAIN_TIMESTEPS, beta_schedule='squaredcos_cap_v2')\n    \n    if not os.path.isfile(GMM_CHECKPOINT_PATH): print(f\"FATAL ERROR: GMM checkpoint file not found at '{GMM_CHECKPOINT_PATH}'\"); return\n    gmm.load_state_dict(torch.load(GMM_CHECKPOINT_PATH, map_location=device))\n    vae.requires_grad_(False); gmm.requires_grad_(False)\n\n    start_epoch = 0; best_val_loss = float('inf')\n    checkpoint_path = os.path.join(CHECKPOINT_DIR_DIFF, 'latest_checkpoint.pth')\n    if os.path.isfile(checkpoint_path):\n        print(f\"Resuming training from checkpoint: {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path)\n        unet.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_val_loss = checkpoint['best_val_loss']\n        print(f\"Resumed from Epoch {start_epoch}, Best Val Loss: {best_val_loss:.4f}\")\n    else: print(\"Starting training from scratch.\")\n\n    full_dataset = FashionVTONDataset(data_root=DATA_ROOT, image_size=IMAGE_SIZE, original_image_size=ORIGINAL_IMAGE_SIZE)\n    val_size = int(len(full_dataset) * VALIDATION_SPLIT); train_size = len(full_dataset) - val_size\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n    print(f\"Dataset loaded: {train_size} training samples, {val_size} validation samples.\")\n    \n    print(f\"Starting training from epoch {start_epoch + 1}...\")\n    for epoch in range(start_epoch, NUM_EPOCHS):\n        unet.train(); train_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n        for batch in progress_bar:\n            with torch.no_grad():\n                person_image = batch['person_image'].to(device); cloth_image = batch['cloth_image'].to(device); pose_map = batch['pose_map'].to(device); agnostic_person = batch['agnostic_person'].to(device)\n                flow = gmm(cloth_image, pose_map); warped_cloth = warp_cloth_with_flow(cloth_image, flow); condition = torch.cat([agnostic_person, warped_cloth], dim=1)\n                latents = vae.encode(person_image).latent_dist.sample() * vae.config.scaling_factor\n            noise = torch.randn_like(latents); timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n            optimizer.zero_grad(); predicted_noise = unet(noisy_latents, timesteps, condition); loss = F.mse_loss(predicted_noise, noise); loss.backward(); optimizer.step()\n            train_loss += loss.item(); progress_bar.set_postfix(loss=loss.item())\n        \n        avg_train_loss = train_loss / len(train_loader)\n        avg_val_loss = evaluate_and_visualize(epoch, unet, gmm, vae, noise_scheduler, val_loader, device)\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            best_model_path = os.path.join(CHECKPOINT_DIR_DIFF, 'unet_best.pth')\n            torch.save(unet.state_dict(), best_model_path)\n            print(f\"🎉 New best model saved with validation loss: {best_val_loss:.4f}\")\n\n        latest_checkpoint_state = {'epoch': epoch, 'model_state_dict': unet.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'best_val_loss': best_val_loss}\n        torch.save(latest_checkpoint_state, checkpoint_path)\n        print(f\"Latest checkpoint updated at {checkpoint_path}\")\n\n    print(\"Training finished.\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T13:04:07.203697Z","iopub.execute_input":"2025-07-15T13:04:07.203868Z","iopub.status.idle":"2025-07-15T13:05:16.118485Z","shell.execute_reply.started":"2025-07-15T13:04:07.203852Z","shell.execute_reply":"2025-07-15T13:05:16.117342Z"}},"outputs":[{"name":"stderr","text":"2025-07-15 13:04:34.297754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752584674.666142      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752584674.775957      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc047a1e2d404909ba547ad2d5669736"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f6e5bf87abf4858b429d681c9025a83"}},"metadata":{}},{"name":"stdout","text":"Starting training from scratch.\nDataset loaded: 10483 training samples, 1164 validation samples.\nStarting training from epoch 1...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100:   1%|          | 63/5242 [00:12<17:10,  5.03it/s, loss=0.672]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/873801774.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/873801774.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mnoisy_latents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpredicted_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_latents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass SinusoidalPositionEmbeddings(nn.Module):\n    \"\"\"\n    Module to generate sinusoidal time embeddings.\n    Used to inform the U-Net of the current noise level (timestep).\n    \"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, time):\n        device = time.device\n        half_dim = self.dim // 2\n        embeddings = math.log(10000) / (half_dim - 1)\n        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n        embeddings = time[:, None] * embeddings[None, :]\n        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n        return embeddings\n\nclass ResnetBlock(nn.Module):\n    \"\"\"A standard ResNet block with two convolutions.\"\"\"\n    def __init__(self, in_channels, out_channels, *, time_emb_dim=None):\n        super().__init__()\n        self.mlp = (\n            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, out_channels))\n            if time_emb_dim is not None\n            else None\n        )\n\n        self.block1 = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.GroupNorm(8, out_channels), nn.SiLU())\n        self.block2 = nn.Sequential(nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.GroupNorm(8, out_channels), nn.SiLU())\n        self.res_conv = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n\n    def forward(self, x, time_emb=None):\n        h = self.block1(x)\n        if self.mlp is not None and time_emb is not None:\n            time_emb = self.mlp(time_emb)\n            h = h + time_emb.unsqueeze(-1).unsqueeze(-1)\n        h = self.block2(h)\n        return h + self.res_conv(x)\n\nclass AttentionBlock(nn.Module):\n    \"\"\"Self-attention block.\"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.gn = nn.GroupNorm(8, channels)\n        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n        self.out = nn.Conv2d(channels, channels, 1)\n\n    class AttentionBlock(nn.Module):\n    \"\"\"Self-attention block.\"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.gn = nn.GroupNorm(8, channels)\n        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n        self.out = nn.Conv2d(channels, channels, 1)\n\n    # CORRECTED LINE: Add the time_emb argument\n    def forward(self, x, time_emb=None):\n        b, c, h, w = x.shape\n        x_in = x\n        x = self.gn(x)\n        x = self.qkv(x)\n        q, k, v = torch.chunk(x, 3, dim=1)\n\n        q = q.view(b, c, h * w)\n        k = k.view(b, c, h * w)\n        v = v.view(b, c, h * w)\n\n        # The rest of the logic remains exactly the same\n        k = k.softmax(dim=-1)\n        attn = torch.einsum(\"b c i, b c j -> b i j\", q, k)\n        out = torch.einsum(\"b i j, b c j -> b c i\", attn, v)\n        out = out.view(b, c, h, w)\n        return self.out(out) + x_in\nclass ConditionalUNet(nn.Module):\n    \"\"\"\n    The main U-Net for the diffusion model.\n    This version has the corrected upsampling path.\n    \"\"\"\n    def __init__(self, in_channels, model_channels, out_channels, time_emb_dim=256, condition_channels=6):\n        super().__init__()\n        \n        # --- Time embedding and Initial Conv (No change) ---\n        self.time_mlp = nn.Sequential(\n            SinusoidalPositionEmbeddings(time_emb_dim),\n            nn.Linear(time_emb_dim, time_emb_dim),\n            nn.ReLU()\n        )\n        self.init_conv = nn.Conv2d(in_channels + condition_channels, model_channels, kernel_size=3, padding=1)\n\n        # --- Downsampling path (No change) ---\n        self.down_blocks = nn.ModuleList([\n            ResnetBlock(model_channels, 128, time_emb_dim=time_emb_dim),\n            ResnetBlock(128, 128, time_emb_dim=time_emb_dim),\n            nn.Conv2d(128, 128, 3, 2, 1),\n            ResnetBlock(128, 256, time_emb_dim=time_emb_dim),\n            AttentionBlock(256, time_emb_dim=time_emb_dim),\n            ResnetBlock(256, 256, time_emb_dim=time_emb_dim),\n            nn.Conv2d(256, 256, 3, 2, 1),\n            ResnetBlock(256, 512, time_emb_dim=time_emb_dim),\n        ])\n\n        # --- Bottleneck (No change) ---\n        self.mid_block1 = ResnetBlock(512, 1024, time_emb_dim=time_emb_dim)\n        self.mid_attn = AttentionBlock(1024, time_emb_dim=time_emb_dim)\n        self.mid_block2 = ResnetBlock(1024, 512, time_emb_dim=time_emb_dim)\n\n        # =================== FIX STARTS HERE: CORRECTED UPSAMPLING ARCHITECTURE ===================\n        #\n        # Adjust the in_channels for the ResNet blocks to account for the concatenation\n        # with the skip connection from the downsampling path.\n        #\n        self.up_blocks = nn.ModuleList([\n            # Input to this block is 512 (from mid_block2) + 512 (from residual) = 1024\n            ResnetBlock(1024, 256, time_emb_dim=time_emb_dim),\n            nn.ConvTranspose2d(256, 256, 4, 2, 1), # Upsample\n            \n            # Input to this block is 256 (from above) + 256 (from residual) = 512\n            ResnetBlock(512, 128, time_emb_dim=time_emb_dim),\n            AttentionBlock(128, time_emb_dim=time_emb_dim),\n            \n            # Input to this block is 128 (from above) + 128 (from residual) = 256\n            ResnetBlock(256, 128, time_emb_dim=time_emb_dim),\n            nn.ConvTranspose2d(128, 128, 4, 2, 1), # Upsample\n            \n            # Input to this block is 128 (from above) + 128 (from residual) = 256\n            ResnetBlock(256, 64, time_emb_dim=time_emb_dim),\n\n            # Input to this block is 64 (from above) + 64 (from residual) = 128\n            ResnetBlock(128, 64, time_emb_dim=time_emb_dim),\n        ])\n\n        self.final_res_block = ResnetBlock(model_channels + 64, model_channels, time_emb_dim=time_emb_dim)\n        self.out_conv = nn.Conv2d(model_channels, out_channels, 1)\n\n    def forward(self, x, time, condition):\n        # Initial projection, time embeddings, and downsampling path (No change)\n        t = self.time_mlp(time)\n        condition_downsampled = F.interpolate(condition, size=x.shape[2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, condition_downsampled], dim=1)\n        x = self.init_conv(x)\n        \n        residuals = [x.clone()]\n        for block in self.down_blocks:\n            x = block(x, t)\n            residuals.append(x)\n            \n        x = self.mid_block1(x, t)\n        x = self.mid_attn(x, t)\n        x = self.mid_block2(x, t)\n\n        # =================== FIX STARTS HERE: CORRECTED FORWARD PASS LOGIC ===================\n        #\n        # Implement the correct U-Net upsampling logic:\n        # 1. Process with ResNet/Attention blocks.\n        # 2. If it's an upsampling layer, apply it.\n        # 3. Concatenate with the skip connection AFTER upsampling.\n        #\n        for block in self.up_blocks:\n            # Concatenate the skip connection from the downsampling path\n            res = residuals.pop()\n            x = torch.cat([x, res], dim=1)\n            \n            # Pass through the ResNet/Attention blocks\n            x = block(x, t)\n            \n            # If the block is an upsampling layer, the architecture definition handles it.\n            # We just need to ensure the logic flow is correct. Let's simplify the loop.\n            # The architecture definition is the main fix. Let's rewrite the loop to match.\n            \n        # Let's use a more explicit loop that matches the corrected architecture\n        x = self.up_blocks[0](torch.cat([x, residuals.pop()], dim=1), t) # ResNet: 1024 -> 256\n        x = self.up_blocks[1](x) # Upsample: 256 -> 256\n        \n        x = self.up_blocks[2](torch.cat([x, residuals.pop()], dim=1), t) # ResNet: 512 -> 128\n        x = self.up_blocks[3](x, t) # Attention\n        x = self.up_blocks[4](torch.cat([x, residuals.pop()], dim=1), t) # ResNet: 256 -> 128\n        x = self.up_blocks[5](x) # Upsample: 128 -> 128\n        \n        x = self.up_blocks[6](torch.cat([x, residuals.pop()], dim=1), t) # ResNet: 256 -> 64\n        x = self.up_blocks[7](torch.cat([x, residuals.pop()], dim=1), t) # ResNet: 128 -> 64\n        \n        # --- Final block ---\n        x = self.final_res_block(torch.cat([x, residuals.pop()], dim=1), t)\n        return self.out_conv(x)\n        \n    def forward(self, x, time, condition):\n        # x: noisy latents [B, 4, H/8, W/8]\n        # time: timestep [B]\n        # condition: concatenated agnostic_person and warped_cloth [B, 6, H, W]\n        \n        # Initial projection and time embeddings\n        t = self.time_mlp(time)\n        \n        # We need to downsample the condition to match the latent space dimensions\n        condition_downsampled = F.interpolate(condition, size=x.shape[2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, condition_downsampled], dim=1)\n        x = self.init_conv(x)\n        \n        # Store residuals for skip connections\n        residuals = [x.clone()]\n        \n        # Downsampling\n        for block in self.down_blocks:\n            if isinstance(block, nn.Conv2d): # Downsampling layer\n                x = block(x)\n            else: # ResNet or Attention block\n                x = block(x, t)\n            residuals.append(x)\n            \n        # Bottleneck\n        x = self.mid_block1(x, t)\n        x = self.mid_attn(x)\n        x = self.mid_block2(x, t)\n\n        # Upsampling\n        for block in self.up_blocks:\n            if isinstance(block, nn.ConvTranspose2d): # Upsampling layer\n                res = residuals.pop()\n                x = torch.cat([x, res], dim=1)\n                x = block(x)\n            else: # ResNet or Attention block\n                res = residuals.pop()\n                x = torch.cat([x, res], dim=1)\n                x = block(x, t)\n        \n        # Final block\n        x = torch.cat([x, residuals.pop()], dim=1)\n        x = self.final_res_block(x, t)\n        \n        return self.out_conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:22:03.676387Z","iopub.execute_input":"2025-07-14T17:22:03.676782Z","iopub.status.idle":"2025-07-14T17:22:03.694694Z","shell.execute_reply.started":"2025-07-14T17:22:03.676760Z","shell.execute_reply":"2025-07-14T17:22:03.693673Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_36/4015150922.py\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    \"\"\"Self-attention block.\"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after class definition on line 54\n"],"ename":"IndentationError","evalue":"expected an indented block after class definition on line 54 (4015150922.py, line 55)","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\nfrom torchvision.utils import save_image\nfrom tqdm import tqdm\nfrom diffusers import AutoencoderKL, DDPMScheduler\nfrom transformers import logging\n\n# Suppress verbose messages from transformers\n#logging.set_verbosity_error()\n\n# =================== FIX STARTS HERE: CORRECT PATHS AND LOGIC ===================\n\n# --- Training Configuration ---\n# =================== FIX STARTS HERE: CORRECT PATHS FOR KAGGLE INPUT ===================\n\n# --- Training Configuration ---\nDATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'\nOUTPUT_DIR = '/kaggle/working/' # The main directory for all our new outputs\n\n# CORRECTED PATH: Point directly to the uploaded model file in your Kaggle dataset\nGMM_CHECKPOINT_PATH = '/kaggle/input/gmm_5epoch/pytorch/default/1/gmm_final (works but low epochs).pth'\n\n# Define subdirectories for our NEW diffusion model outputs in the working directory\nCHECKPOINT_DIR_DIFF = os.path.join(OUTPUT_DIR, 'checkpoints_diffusion')\nVISUALIZATION_DIR_DIFF = os.path.join(OUTPUT_DIR, 'visualizations_diffusion')\n\n# --- The rest of your configuration ---\nBATCH_SIZE = 2\nNUM_EPOCHS = 100\nLEARNING_RATE = 1e-4\nIMAGE_SIZE = (256, 192)\nORIGINAL_IMAGE_SIZE = (768, 1024)\nVALIDATION_SPLIT = 0.1\nVAE_MODEL_ID = \"stabilityai/stable-diffusion-2-1-base\"\nVAE_SUBFOLDER = \"vae\"\nNUM_TRAIN_TIMESTEPS = 1000\n\n# =================== FIX ENDS HERE ===================\n# =================== FIX ENDS HERE ===================\n\n\n# Create directories if they don't exist\nos.makedirs(CHECKPOINT_DIR_DIFF, exist_ok=True)\nos.makedirs(VISUALIZATION_DIR_DIFF, exist_ok=True)\n\n\n@torch.no_grad()\ndef evaluate_and_visualize(epoch, unet, gmm, vae, noise_scheduler, val_loader, device):\n    # ... (paste the full evaluate_and_visualize function here, it's correct) ...\n    unet.eval(); gmm.eval(); val_loss = 0.0; progress_bar = tqdm(val_loader, desc=\"Validating\", leave=False)\n    for batch in progress_bar:\n        person_image = batch['person_image'].to(device); cloth_image = batch['cloth_image'].to(device); pose_map = batch['pose_map'].to(device); agnostic_person = batch['agnostic_person'].to(device)\n        flow = gmm(cloth_image, pose_map); warped_cloth = warp_cloth_with_flow(cloth_image, flow); condition = torch.cat([agnostic_person, warped_cloth], dim=1)\n        latents = vae.encode(person_image).latent_dist.sample() * vae.config.scaling_factor\n        noise = torch.randn_like(latents); timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps); predicted_noise = unet(noisy_latents, timesteps, condition); loss = F.mse_loss(predicted_noise, noise); val_loss += loss.item()\n    avg_val_loss = val_loss / len(val_loader)\n    sample_condition = condition[0:1]; original_vis_image = person_image[0:1]; sample_latents = torch.randn(1, 4, IMAGE_SIZE[0] // 8, IMAGE_SIZE[1] // 8).to(device)\n    for t in tqdm(noise_scheduler.timesteps, desc=\"Generating sample\", leave=False):\n        pred_noise = unet(sample_latents, t.unsqueeze(0).to(device), sample_condition); sample_latents = noise_scheduler.step(pred_noise, t, sample_latents).prev_sample\n    sample_latents = 1 / vae.config.scaling_factor * sample_latents; generated_image = vae.decode(sample_latents).sample\n    original_vis = (original_vis_image + 1) / 2; condition_vis = (sample_condition[:, :3] + 1) / 2; warped_vis = (sample_condition[:, 3:] + 1) / 2; generated_vis = (generated_image + 1) / 2\n    comparison = torch.cat([original_vis, condition_vis, warped_vis, generated_vis], dim=0); save_image(comparison, os.path.join(VISUALIZATION_DIR_DIFF, f'epoch_{epoch+1}_sample.png'), nrow=4)\n    unet.train(); return avg_val_loss\n\n\ndef main():\n    \"\"\"Main training loop with validation and best model saving.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # --- Load Pre-trained Components ---\n    print(\"Loading pre-trained VAE and GMM...\")\n    vae = AutoencoderKL.from_pretrained(VAE_MODEL_ID, subfolder=VAE_SUBFOLDER).to(device)\n    \n    # Check if the GMM checkpoint file exists\n    if not os.path.isfile(GMM_CHECKPOINT_PATH):\n        print(f\"FATAL ERROR: GMM checkpoint file not found at '{GMM_CHECKPOINT_PATH}'\")\n        print(\"Please ensure the GMM model was trained and its output file 'gmm_final.pth' is in '/kaggle/working/'.\")\n        return # Use return instead of exit() in notebooks\n    \n    print(f\"Found GMM checkpoint at: {GMM_CHECKPOINT_PATH}\")\n    gmm = GMM(in_channels_pose=25).to(device)\n    gmm.load_state_dict(torch.load(GMM_CHECKPOINT_PATH, map_location=device))\n    \n    vae.requires_grad_(False)\n    gmm.requires_grad_(False)\n    noise_scheduler = DDPMScheduler(num_train_timesteps=NUM_TRAIN_TIMESTEPS, beta_schedule='squaredcos_cap_v2')\n\n    # --- Data Loading and Splitting ---\n    print(\"Loading and splitting dataset...\")\n    full_dataset = FashionVTONDataset(\n        data_root=DATA_ROOT, image_size=IMAGE_SIZE, original_image_size=ORIGINAL_IMAGE_SIZE\n    )\n    val_size = int(len(full_dataset) * VALIDATION_SPLIT); train_size = len(full_dataset) - val_size\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True) # Reduced num_workers for Kaggle\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n    print(f\"Dataset loaded: {train_size} training samples, {val_size} validation samples.\")\n\n    # --- Model, Optimizer ---\n    unet = ConditionalUNet(in_channels=4, model_channels=128, out_channels=4, condition_channels=6).to(device)\n    optimizer = optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n\n    # --- Training Loop ---\n    print(\"Starting Diffusion Model training...\")\n    best_val_loss = float('inf')\n\n    for epoch in range(NUM_EPOCHS):\n        unet.train(); train_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n        for batch in progress_bar:\n            with torch.no_grad():\n                person_image = batch['person_image'].to(device); cloth_image = batch['cloth_image'].to(device); pose_map = batch['pose_map'].to(device); agnostic_person = batch['agnostic_person'].to(device)\n                flow = gmm(cloth_image, pose_map); warped_cloth = warp_cloth_with_flow(cloth_image, flow); condition = torch.cat([agnostic_person, warped_cloth], dim=1)\n                latents = vae.encode(person_image).latent_dist.sample() * vae.config.scaling_factor\n            noise = torch.randn_like(latents); timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n            optimizer.zero_grad(); predicted_noise = unet(noisy_latents, timesteps, condition); loss = F.mse_loss(predicted_noise, noise); loss.backward(); optimizer.step()\n            train_loss += loss.item(); progress_bar.set_postfix(loss=loss.item())\n        \n        avg_train_loss = train_loss / len(train_loader)\n        avg_val_loss = evaluate_and_visualize(epoch, unet, gmm, vae, noise_scheduler, val_loader, device)\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n        \n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            best_model_path = os.path.join(CHECKPOINT_DIR_DIFF, 'unet_best.pth')\n            torch.save(unet.state_dict(), best_model_path)\n            print(f\"🎉 New best model saved with validation loss: {best_val_loss:.4f} at {best_model_path}\")\n    \n    print(\"Diffusion model training finished.\")\n    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:14:44.176805Z","iopub.execute_input":"2025-07-14T17:14:44.177482Z","iopub.status.idle":"2025-07-14T17:14:44.503111Z","shell.execute_reply.started":"2025-07-14T17:14:44.177456Z","shell.execute_reply":"2025-07-14T17:14:44.502221Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading pre-trained VAE and GMM...\nFound GMM checkpoint at: /kaggle/input/gmm_5epoch/pytorch/default/1/gmm_final (works but low epochs).pth\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1186563273.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/1186563273.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found GMM checkpoint at: {GMM_CHECKPOINT_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mgmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels_pose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGMM_CHECKPOINT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'GMM' is not defined"],"ename":"NameError","evalue":"name 'GMM' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"# ===================================================================\n# CELL 1: ALL IMPORTS AND CLASS DEFINITIONS\n# This part fixes the \"NameError: name 'GMM' is not defined\"\n# ===================================================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport os\nimport json\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom tqdm import tqdm\nfrom diffusers import AutoencoderKL, DDPMScheduler\nfrom transformers import logging\nfrom torch.optim import AdamW\n# Suppress verbose messages from transformers\nlogging.set_verbosity_error()\n\n\n# --- Class Definition from: src/dataset.py ---\nclass FashionVTONDataset(Dataset):\n    def __init__(self, data_root, image_size=(256, 192), original_image_size=(768, 1024)):\n        self.data_root = data_root\n        self.image_size = image_size\n        self.original_image_size = original_image_size\n        self.image_dir = os.path.join(data_root, 'image')\n        self.cloth_dir = os.path.join(data_root, 'cloth')\n        self.cloth_mask_dir = os.path.join(data_root, 'cloth-mask')\n        self.pose_dir = os.path.join(data_root, 'openpose_json')\n        self.parse_dir = os.path.join(data_root, 'image-parse-v3')\n        self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))])\n        self.transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BILINEAR), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        self.mask_transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])\n    def __len__(self):\n        return len(self.image_files)\n    def find_upper_cloth_label(self, pose_keypoints, parse_array):\n        torso_indices = [1, 2, 5, 8, 9, 12]; visible_points = []\n        for i in torso_indices:\n            if i < len(pose_keypoints):\n                x, y, conf = pose_keypoints[i]\n                if conf > 0.1: visible_points.append((int(x), int(y)))\n        if len(visible_points) < 3: return 5\n        x_coords, y_coords = zip(*visible_points); min_x, max_x = min(x_coords), max(x_coords); min_y, max_y = min(y_coords), max(y_coords)\n        if max_x <= min_x or max_y <= min_y: return 5\n        torso_parse_area = parse_array[min_y:max_y, min_x:max_x]; unique_labels, counts = np.unique(torso_parse_area, return_counts=True); non_zero_mask = (unique_labels != 0)\n        if np.any(non_zero_mask): return unique_labels[non_zero_mask][np.argmax(counts[non_zero_mask])]\n        else: return 5\n    def __getitem__(self, idx):\n        image_name = self.image_files[idx]; base_name = os.path.splitext(image_name)[0]\n        person_image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB'); cloth_image = Image.open(os.path.join(self.cloth_dir, image_name)).convert('RGB'); cloth_mask = Image.open(os.path.join(self.cloth_mask_dir, image_name)).convert('L')\n        try:\n            with open(os.path.join(self.pose_dir, f\"{base_name}_keypoints.json\"), 'r') as f: pose_data = json.load(f)\n            pose_keypoints = np.array(pose_data['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n        except (FileNotFoundError, IndexError): pose_keypoints = np.zeros((25, 3), dtype=np.float32)\n        parse_path = os.path.join(self.parse_dir, f\"{base_name}.png\")\n        if not os.path.exists(parse_path): parse_path = os.path.join(self.parse_dir, f\"{base_name}.jpg\") # fallback\n        parse_array_orig = np.array(Image.open(parse_path).convert('L'))\n        upper_cloth_label = self.find_upper_cloth_label(pose_keypoints, parse_array_orig)\n        parse_array_resized = cv2.resize(parse_array_orig, self.image_size[::-1], interpolation=cv2.INTER_NEAREST)\n        person_cloth_mask = (parse_array_resized == upper_cloth_label).astype(np.float32)\n        person_image_tensor = self.transform(person_image); cloth_image_tensor = self.transform(cloth_image); cloth_mask_tensor = self.mask_transform(cloth_mask)\n        pose_map_tensor = torch.from_numpy(self.create_pose_map(pose_keypoints)).float()\n        blurred_mask_tensor = torch.from_numpy(cv2.GaussianBlur(person_cloth_mask, (5, 5), 0)).unsqueeze(0)\n        agnostic_person_tensor = person_image_tensor * (1 - blurred_mask_tensor)\n        warped_cloth_tensor = person_image_tensor * torch.from_numpy(person_cloth_mask).unsqueeze(0)\n        return {'person_image': person_image_tensor, 'cloth_image': cloth_image_tensor, 'cloth_mask': cloth_mask_tensor, 'agnostic_person': agnostic_person_tensor, 'pose_map': pose_map_tensor, 'warped_cloth': warped_cloth_tensor}\n    def create_pose_map(self, keypoints):\n        h, w = self.image_size; orig_w, orig_h = self.original_image_size; num_keypoints = keypoints.shape[0]\n        pose_map = np.zeros((num_keypoints, h, w), dtype=np.float32)\n        for i, point in enumerate(keypoints):\n            if point[2] > 0.1:\n                x, y = int(point[0] * w / orig_w), int(point[1] * h / orig_h)\n                if 0 <= x < w and 0 <= y < h: cv2.circle(pose_map[i], (x, y), radius=3, color=1, thickness=-1)\n        return pose_map\n\n# --- Class Definition from: src/geometric_matching.py ---\nclass GMM(nn.Module):\n    def __init__(self, in_channels_cloth=3, in_channels_pose=25, out_channels_flow=2):\n        super(GMM, self).__init__(); self.encoder1 = self.conv_block(in_channels_cloth + in_channels_pose, 64); self.encoder2 = self.conv_block(64, 128); self.encoder3 = self.conv_block(128, 256); self.encoder4 = self.conv_block(256, 512); self.pool = nn.MaxPool2d(2, 2); self.bottleneck = self.conv_block(512, 1024); self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2); self.decoder4 = self.conv_block(1024, 512); self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2); self.decoder3 = self.conv_block(512, 256); self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2); self.decoder2 = self.conv_block(256, 128); self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2); self.decoder1 = self.conv_block(128, 64); self.conv_out = nn.Conv2d(64, out_channels_flow, kernel_size=1); self.tanh = nn.Tanh()\n    def conv_block(self, in_channels, out_channels): return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True))\n    def forward(self, cloth_image, pose_map):\n        x = torch.cat([cloth_image, pose_map], dim=1); e1 = self.encoder1(x); p1 = self.pool(e1); e2 = self.encoder2(p1); p2 = self.pool(e2); e3 = self.encoder3(p2); p3 = self.pool(e3); e4 = self.encoder4(p3); p4 = self.pool(e4); b = self.bottleneck(p4); d4 = self.upconv4(b); d4 = torch.cat([d4, e4], dim=1); d4 = self.decoder4(d4); d3 = self.upconv3(d4); d3 = torch.cat([d3, e3], dim=1); d3 = self.decoder3(d3); d2 = self.upconv2(d3); d2 = torch.cat([d2, e2], dim=1); d2 = self.decoder2(d2); d1 = self.upconv1(d2); d1 = torch.cat([d1, e1], dim=1); d1 = self.decoder1(d1); flow_field = self.conv_out(d1); flow_field = self.tanh(flow_field); return flow_field\n\ndef warp_cloth_with_flow(cloth_image, flow_field):\n    flow_field = flow_field.permute(0, 2, 3, 1); warped_image = F.grid_sample(cloth_image, flow_field, mode='bilinear', padding_mode='zeros', align_corners=True); return warped_image\n\n# --- Class Definition from: src/diffusion_model.py ---\nclass SinusoidalPositionEmbeddings(nn.Module):\n    def __init__(self, dim): super().__init__(); self.dim = dim\n    def forward(self, time): device = time.device; half_dim = self.dim // 2; embeddings = math.log(10000) / (half_dim - 1); embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings); embeddings = time[:, None] * embeddings[None, :]; embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1); return embeddings\nclass ResnetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, *, time_emb_dim=None):\n        super().__init__(); self.mlp = (nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, out_channels)) if time_emb_dim is not None else None); self.block1 = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.GroupNorm(8, out_channels), nn.SiLU()); self.block2 = nn.Sequential(nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.GroupNorm(8, out_channels), nn.SiLU()); self.res_conv = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n    def forward(self, x, time_emb=None):\n        h = self.block1(x)\n        if self.mlp is not None and time_emb is not None:\n            time_emb = self.mlp(time_emb)\n            h = h + time_emb.unsqueeze(-1).unsqueeze(-1)\n        h = self.block2(h)\n        return h + self.res_conv(x)\nclass AttentionBlock(nn.Module):\n    def __init__(self, channels, time_emb=None): super().__init__(); self.gn = nn.GroupNorm(8, channels); self.qkv = nn.Conv2d(channels, channels * 3, 1); self.out = nn.Conv2d(channels, channels, 1)\n    def forward(self, x, time_emb=None): b, c, h, w = x.shape; x_in = x; x = self.gn(x); x = self.qkv(x); q, k, v = torch.chunk(x, 3, dim=1); q = q.view(b, c, h * w); k = k.view(b, c, h * w); v = v.view(b, c, h * w); k = k.softmax(dim=-1); attn = torch.einsum(\"b c i, b c j -> b i j\", q, k); out = torch.einsum(\"b i j, b c j -> b c i\", attn, v); out = out.view(b, c, h, w); return self.out(out) + x_in\nclass ConditionalUNet(nn.Module):\n    \"\"\"\n    The main U-Net for the diffusion model.\n    This version has the FINAL corrected upsampling path logic.\n    \"\"\"\n    def __init__(self, in_channels, model_channels, out_channels, time_emb_dim=256, condition_channels=6):\n        super().__init__()\n        \n        # --- Time embedding and Initial Conv ---\n        self.time_mlp = nn.Sequential(\n            SinusoidalPositionEmbeddings(time_emb_dim),\n            nn.Linear(time_emb_dim, time_emb_dim),\n            nn.ReLU()\n        )\n        self.init_conv = nn.Conv2d(in_channels + condition_channels, model_channels, kernel_size=3, padding=1)\n\n        # --- Downsampling Path ---\n        self.down1 = ResnetBlock(model_channels, 128, time_emb_dim=time_emb_dim)\n        self.down2 = ResnetBlock(128, 128, time_emb_dim=time_emb_dim)\n        self.down3 = ResnetBlock(128, 256, time_emb_dim=time_emb_dim)\n        self.down4 = AttentionBlock(256)\n        self.down5 = ResnetBlock(256, 256, time_emb_dim=time_emb_dim)\n        self.down6 = ResnetBlock(256, 512, time_emb_dim=time_emb_dim)\n        \n        self.pool = nn.MaxPool2d(2)\n\n        # --- Bottleneck ---\n        self.mid1 = ResnetBlock(512, 1024, time_emb_dim=time_emb_dim)\n        self.mid_attn = AttentionBlock(1024)\n        self.mid2 = ResnetBlock(1024, 512, time_emb_dim=time_emb_dim)\n\n        # --- Upsampling Path ---\n        self.up1 = nn.ConvTranspose2d(512, 256, 2, 2)\n        self.up_res1 = ResnetBlock(512, 256, time_emb_dim=time_emb_dim)\n        self.up_attn1 = AttentionBlock(256)\n        self.up_res2 = ResnetBlock(256, 256, time_emb_dim=time_emb_dim)\n        \n        self.up2 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.up_res3 = ResnetBlock(256, 128, time_emb_dim=time_emb_dim)\n        self.up_res4 = ResnetBlock(128, 128, time_emb_dim=time_emb_dim)\n\n        # --- Final Output ---\n        self.out_res = ResnetBlock(128 + model_channels, 64, time_emb_dim=time_emb_dim)\n        self.out_conv = nn.Conv2d(64, out_channels, 1)\n\n    def forward(self, x, time, condition):\n        # 1. Initial processing\n        t = self.time_mlp(time)\n        condition_downsampled = F.interpolate(condition, size=x.shape[2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, condition_downsampled], dim=1)\n        x = self.init_conv(x)\n        \n        # Store for skip connection\n        r0 = x.clone()\n\n        # 2. Downsampling\n        x = self.down1(x, t)\n        x = self.down2(x, t)\n        r1 = x.clone() # Residual after first block\n        x = self.pool(x)\n        \n        x = self.down3(x, t)\n        x = self.down4(x, t)\n        x = self.down5(x, t)\n        r2 = x.clone() # Residual after second block\n        x = self.pool(x)\n\n        x = self.down6(x, t)\n        \n        # 3. Bottleneck\n        x = self.mid1(x, t)\n        x = self.mid_attn(x, t)\n        x = self.mid2(x, t)\n\n        # 4. Upsampling\n        x = self.up1(x) # Upsample\n        x = torch.cat([x, r2], dim=1) # Concatenate with skip connection\n        x = self.up_res1(x, t) # Process combined tensor\n        x = self.up_attn1(x, t)\n        x = self.up_res2(x, t)\n\n        x = self.up2(x) # Upsample\n        x = torch.cat([x, r1], dim=1) # Concatenate with skip connection\n        x = self.up_res3(x, t) # Process combined tensor\n        x = self.up_res4(x, t)\n        \n        # 5. Final output stage\n        x = torch.cat([x, r0], dim=1) # Concatenate with initial residual\n        x = self.out_res(x, t)\n        \n        return self.out_conv(x)\n# ===================================================================\n# CELL 2: CONFIGURATION AND TRAINING SCRIPT\n# This part now has the correct paths and logic.\n# ===================================================================\n\n# --- Training Configuration ---\nDATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'\nOUTPUT_DIR = '/kaggle/working/'\n\n# CORRECTED PATH: Point directly to the uploaded model file\nGMM_CHECKPOINT_PATH = '/kaggle/input/gmm_5epoch/pytorch/default/1/gmm_final (works but low epochs).pth'\n\n# Define subdirectories for our diffusion model outputs\nCHECKPOINT_DIR_DIFF = os.path.join(OUTPUT_DIR, 'checkpoints_diffusion')\nVISUALIZATION_DIR_DIFF = os.path.join(OUTPUT_DIR, 'visualizations_diffusion')\n\nBATCH_SIZE = 2\nNUM_EPOCHS = 5\nLEARNING_RATE = 1e-4\nIMAGE_SIZE = (256, 192)\nORIGINAL_IMAGE_SIZE = (768, 1024)\nVALIDATION_SPLIT = 0.1\nVAE_MODEL_ID = \"stabilityai/stable-diffusion-2-1-base\"\nVAE_SUBFOLDER = \"vae\"\nNUM_TRAIN_TIMESTEPS = 1000\n\n# Create directories if they don't exist\nos.makedirs(CHECKPOINT_DIR_DIFF, exist_ok=True)\nos.makedirs(VISUALIZATION_DIR_DIFF, exist_ok=True)\n\n@torch.no_grad()\ndef evaluate_and_visualize(epoch, unet, gmm, vae, noise_scheduler, val_loader, device):\n    unet.eval(); gmm.eval(); val_loss = 0.0; progress_bar = tqdm(val_loader, desc=\"Validating\", leave=False)\n    for batch in progress_bar:\n        person_image = batch['person_image'].to(device); cloth_image = batch['cloth_image'].to(device); pose_map = batch['pose_map'].to(device); agnostic_person = batch['agnostic_person'].to(device)\n        flow = gmm(cloth_image, pose_map); warped_cloth = warp_cloth_with_flow(cloth_image, flow); condition = torch.cat([agnostic_person, warped_cloth], dim=1)\n        latents = vae.encode(person_image).latent_dist.sample() * vae.config.scaling_factor\n        noise = torch.randn_like(latents); timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps); predicted_noise = unet(noisy_latents, timesteps, condition); loss = F.mse_loss(predicted_noise, noise); val_loss += loss.item()\n    avg_val_loss = val_loss / len(val_loader)\n    sample_condition = condition[0:1]; original_vis_image = person_image[0:1]; sample_latents = torch.randn(1, 4, IMAGE_SIZE[0] // 8, IMAGE_SIZE[1] // 8).to(device)\n    for t in tqdm(noise_scheduler.timesteps, desc=\"Generating sample\", leave=False):\n        pred_noise = unet(sample_latents, t.unsqueeze(0).to(device), sample_condition); sample_latents = noise_scheduler.step(pred_noise, t, sample_latents).prev_sample\n    sample_latents = 1 / vae.config.scaling_factor * sample_latents; generated_image = vae.decode(sample_latents).sample\n    original_vis = (original_vis_image + 1) / 2; condition_vis = (sample_condition[:, :3] + 1) / 2; warped_vis = (sample_condition[:, 3:] + 1) / 2; generated_vis = (generated_image + 1) / 2\n    comparison = torch.cat([original_vis, condition_vis, warped_vis, generated_vis], dim=0); save_image(comparison, os.path.join(VISUALIZATION_DIR_DIFF, f'epoch_{epoch+1}_sample.png'), nrow=4)\n    unet.train(); return avg_val_loss\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    print(\"Loading pre-trained VAE and GMM...\")\n    vae = AutoencoderKL.from_pretrained(VAE_MODEL_ID, subfolder=VAE_SUBFOLDER).to(device)\n    if not os.path.isfile(GMM_CHECKPOINT_PATH):\n        print(f\"FATAL ERROR: GMM checkpoint file not found at '{GMM_CHECKPOINT_PATH}'\")\n        return\n    print(f\"Found GMM checkpoint at: {GMM_CHECKPOINT_PATH}\")\n    gmm = GMM(in_channels_pose=25).to(device)\n    gmm.load_state_dict(torch.load(GMM_CHECKPOINT_PATH, map_location=device))\n    vae.requires_grad_(False); gmm.requires_grad_(False)\n    noise_scheduler = DDPMScheduler(num_train_timesteps=NUM_TRAIN_TIMESTEPS, beta_schedule='squaredcos_cap_v2')\n    print(\"Loading and splitting dataset...\")\n    full_dataset = FashionVTONDataset(data_root=DATA_ROOT, image_size=IMAGE_SIZE, original_image_size=ORIGINAL_IMAGE_SIZE)\n    val_size = int(len(full_dataset) * VALIDATION_SPLIT); train_size = len(full_dataset) - val_size\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n    print(f\"Dataset loaded: {train_size} training samples, {val_size} validation samples.\")\n    unet = ConditionalUNet(in_channels=4, model_channels=128, out_channels=4, condition_channels=6).to(device)\n    optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n    print(\"Starting Diffusion Model training...\")\n    best_val_loss = float('inf')\n    for epoch in range(NUM_EPOCHS):\n        unet.train(); train_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n        for batch in progress_bar:\n            with torch.no_grad():\n                person_image = batch['person_image'].to(device); cloth_image = batch['cloth_image'].to(device); pose_map = batch['pose_map'].to(device); agnostic_person = batch['agnostic_person'].to(device)\n                flow = gmm(cloth_image, pose_map); warped_cloth = warp_cloth_with_flow(cloth_image, flow); condition = torch.cat([agnostic_person, warped_cloth], dim=1)\n                latents = vae.encode(person_image).latent_dist.sample() * vae.config.scaling_factor\n            noise = torch.randn_like(latents); timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n            optimizer.zero_grad(); predicted_noise = unet(noisy_latents, timesteps, condition); loss = F.mse_loss(predicted_noise, noise); loss.backward(); optimizer.step()\n            train_loss += loss.item(); progress_bar.set_postfix(loss=loss.item())\n        avg_train_loss = train_loss / len(train_loader)\n        avg_val_loss = evaluate_and_visualize(epoch, unet, gmm, vae, noise_scheduler, val_loader, device)\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            best_model_path = os.path.join(CHECKPOINT_DIR_DIFF, 'unet_best.pth')\n            torch.save(unet.state_dict(), best_model_path)\n            print(f\"🎉 New best model saved with validation loss: {best_val_loss:.4f} at {best_model_path}\")\n    print(\"Diffusion model training finished.\"); print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:34:39.303239Z","iopub.execute_input":"2025-07-14T17:34:39.303584Z","iopub.status.idle":"2025-07-14T18:59:23.570067Z","shell.execute_reply.started":"2025-07-14T17:34:39.303556Z","shell.execute_reply":"2025-07-14T18:59:23.569270Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading pre-trained VAE and GMM...\nFound GMM checkpoint at: /kaggle/input/gmm_5epoch/pytorch/default/1/gmm_final (works but low epochs).pth\nLoading and splitting dataset...\nDataset loaded: 10483 training samples, 1164 validation samples.\nStarting Diffusion Model training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 5242/5242 [15:17<00:00,  5.71it/s, loss=0.0227] \n                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 | Train Loss: 0.2367 | Val Loss: 0.2078\n🎉 New best model saved with validation loss: 0.2078 at /kaggle/working/checkpoints_diffusion/unet_best.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 5242/5242 [15:24<00:00,  5.67it/s, loss=0.199]  \n                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5 | Train Loss: 0.2005 | Val Loss: 0.1962\n🎉 New best model saved with validation loss: 0.1962 at /kaggle/working/checkpoints_diffusion/unet_best.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 5242/5242 [15:24<00:00,  5.67it/s, loss=0.00719]\n                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5 | Train Loss: 0.1925 | Val Loss: 0.1936\n🎉 New best model saved with validation loss: 0.1936 at /kaggle/working/checkpoints_diffusion/unet_best.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 5242/5242 [15:23<00:00,  5.67it/s, loss=0.475]  \n                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5 | Train Loss: 0.1897 | Val Loss: 0.1893\n🎉 New best model saved with validation loss: 0.1893 at /kaggle/working/checkpoints_diffusion/unet_best.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 5242/5242 [15:23<00:00,  5.68it/s, loss=0.00616]\n                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5 | Train Loss: 0.1809 | Val Loss: 0.1778\n🎉 New best model saved with validation loss: 0.1778 at /kaggle/working/checkpoints_diffusion/unet_best.pth\nDiffusion model training finished.\nBest validation loss achieved: 0.1778\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ===================================================================\n# FINAL KAGGLE NOTEBOOK CELL FOR HD-VITON TRAINING\n# Includes all class definitions and a robust, resumable GAN training loop\n# ===================================================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport os\nimport json\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nimport torchvision.models as models\nfrom tqdm import tqdm\n\n# --- CLASS DEFINITIONS (Dataset, GMM, VGG Loss) ---\n# These are the components we need from the previous steps.\nclass FashionVTONDataset(Dataset):\n    def __init__(self, data_root, image_size=(256, 192), original_image_size=(768, 1024)):\n        self.data_root = data_root; self.image_size = image_size; self.original_image_size = original_image_size; self.image_dir = os.path.join(data_root, 'image'); self.cloth_dir = os.path.join(data_root, 'cloth'); self.cloth_mask_dir = os.path.join(data_root, 'cloth-mask'); self.pose_dir = os.path.join(data_root, 'openpose_json'); self.parse_dir = os.path.join(data_root, 'image-parse-v3'); self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))]); self.transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BILINEAR), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]); self.mask_transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])\n    def __len__(self): return len(self.image_files)\n    def find_upper_cloth_label(self, pose_keypoints, parse_array):\n        torso_indices = [1, 2, 5, 8, 9, 12]; visible_points = []\n        for i in torso_indices:\n            if i < len(pose_keypoints): x, y, conf = pose_keypoints[i];\n            if conf > 0.1: visible_points.append((int(x), int(y)))\n        if len(visible_points) < 3: return 5\n        x_coords, y_coords = zip(*visible_points); min_x, max_x = min(x_coords), max(x_coords); min_y, max_y = min(y_coords), max(y_coords)\n        if max_x <= min_x or max_y <= min_y: return 5\n        torso_parse_area = parse_array[min_y:max_y, min_x:max_x]; unique_labels, counts = np.unique(torso_parse_area, return_counts=True); non_zero_mask = (unique_labels != 0)\n        if np.any(non_zero_mask): return unique_labels[non_zero_mask][np.argmax(counts[non_zero_mask])]\n        else: return 5\n    def __getitem__(self, idx):\n        image_name = self.image_files[idx]; base_name = os.path.splitext(image_name)[0]\n        person_image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB'); cloth_image = Image.open(os.path.join(self.cloth_dir, image_name)).convert('RGB'); cloth_mask = Image.open(os.path.join(self.cloth_mask_dir, image_name)).convert('L')\n        try:\n            with open(os.path.join(self.pose_dir, f\"{base_name}_keypoints.json\"), 'r') as f: pose_data = json.load(f)\n            pose_keypoints = np.array(pose_data['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n        except (FileNotFoundError, IndexError): pose_keypoints = np.zeros((25, 3), dtype=np.float32)\n        parse_path = os.path.join(self.parse_dir, f\"{base_name}.png\")\n        if not os.path.exists(parse_path): parse_path = os.path.join(self.parse_dir, f\"{base_name}.jpg\")\n        parse_array_orig = np.array(Image.open(parse_path).convert('L'))\n        upper_cloth_label = self.find_upper_cloth_label(pose_keypoints, parse_array_orig)\n        parse_array_resized = cv2.resize(parse_array_orig, self.image_size[::-1], interpolation=cv2.INTER_NEAREST)\n        person_cloth_mask = (parse_array_resized == upper_cloth_label).astype(np.float32)\n        person_image_tensor = self.transform(person_image); cloth_image_tensor = self.transform(cloth_image); cloth_mask_tensor = self.mask_transform(cloth_mask)\n        pose_map_tensor = torch.from_numpy(self.create_pose_map(pose_keypoints)).float()\n        blurred_mask_tensor = torch.from_numpy(cv2.GaussianBlur(person_cloth_mask, (5, 5), 0)).unsqueeze(0)\n        agnostic_person_tensor = person_image_tensor * (1 - blurred_mask_tensor)\n        warped_cloth_tensor = person_image_tensor * torch.from_numpy(person_cloth_mask).unsqueeze(0)\n        return {'person_image': person_image_tensor, 'cloth_image': cloth_image_tensor, 'cloth_mask': cloth_mask_tensor, 'agnostic_person': agnostic_person_tensor, 'pose_map': pose_map_tensor, 'warped_cloth': warped_cloth_tensor}\n    def create_pose_map(self, keypoints):\n        h, w = self.image_size; orig_w, orig_h = self.original_image_size; num_keypoints = keypoints.shape[0]\n        pose_map = np.zeros((num_keypoints, h, w), dtype=np.float32)\n        for i, point in enumerate(keypoints):\n            if point[2] > 0.1:\n                x, y = int(point[0] * w / orig_w), int(point[1] * h / orig_h)\n                if 0 <= x < w and 0 <= y < h: cv2.circle(pose_map[i], (x, y), radius=3, color=1, thickness=-1)\n        return pose_map\n\nclass GMM(nn.Module):\n    def __init__(self, in_channels_cloth=3, in_channels_pose=25, out_channels_flow=2):\n        super(GMM, self).__init__(); self.encoder1 = self.conv_block(in_channels_cloth + in_channels_pose, 64); self.encoder2 = self.conv_block(64, 128); self.encoder3 = self.conv_block(128, 256); self.encoder4 = self.conv_block(256, 512); self.pool = nn.MaxPool2d(2, 2); self.bottleneck = self.conv_block(512, 1024); self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2); self.decoder4 = self.conv_block(1024, 512); self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2); self.decoder3 = self.conv_block(512, 256); self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2); self.decoder2 = self.conv_block(256, 128); self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2); self.decoder1 = self.conv_block(128, 64); self.conv_out = nn.Conv2d(64, out_channels_flow, kernel_size=1); self.tanh = nn.Tanh()\n    def conv_block(self, in_channels, out_channels): return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True))\n    def forward(self, cloth_image, pose_map):\n        x = torch.cat([cloth_image, pose_map], dim=1); e1 = self.encoder1(x); p1 = self.pool(e1); e2 = self.encoder2(p1); p2 = self.pool(e2); e3 = self.encoder3(p2); p3 = self.pool(e3); e4 = self.encoder4(p3); p4 = self.pool(e4); b = self.bottleneck(p4); d4 = self.upconv4(b); d4 = torch.cat([d4, e4], dim=1); d4 = self.decoder4(d4); d3 = self.upconv3(d4); d3 = torch.cat([d3, e3], dim=1); d3 = self.decoder3(d3); d2 = self.upconv2(d3); d2 = torch.cat([d2, e2], dim=1); d2 = self.decoder2(d2); d1 = self.upconv1(d2); d1 = torch.cat([d1, e1], dim=1); d1 = self.decoder1(d1); flow_field = self.conv_out(d1); flow_field = self.tanh(flow_field); return flow_field\n\ndef warp_cloth_with_flow(cloth_image, flow_field):\n    flow_field = flow_field.permute(0, 2, 3, 1); warped_image = F.grid_sample(cloth_image, flow_field, mode='bilinear', padding_mode='zeros', align_corners=True); return warped_image\n\nclass VGGPerceptualLoss(nn.Module):\n    def __init__(self, resize=True):\n        super(VGGPerceptualLoss, self).__init__(); vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features; self.vgg_layers = vgg[:35].eval();\n        for param in self.vgg_layers.parameters(): param.requires_grad = False\n        self.l1 = nn.L1Loss(); self.transform = nn.functional.interpolate; self.resize = resize\n        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)); self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n    def forward(self, pred, target):\n        pred = (pred + 1) / 2; target = (target + 1) / 2; pred = (pred - self.mean) / self.std; target = (target - self.mean) / self.std\n        if self.resize: pred = self.transform(pred, mode='bilinear', size=(224, 224), align_corners=False); target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n        pred_features = self.vgg_layers(pred); target_features = self.vgg_layers(target); return self.l1(pred_features, target_features)\n\n# --- NEW HD-VITON MODEL DEFINITIONS ---\nclass GeneratorHD(nn.Module):\n    \"\"\"The main U-Net Generator for HD-VITON. Takes agnostic person and warped cloth.\"\"\"\n    def __init__(self, in_channels=6, out_channels=3):\n        super(GeneratorHD, self).__init__()\n        self.encoder1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True))\n        self.encoder2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), nn.InstanceNorm2d(128), nn.LeakyReLU(0.2, inplace=True))\n        self.encoder3 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), nn.InstanceNorm2d(256), nn.LeakyReLU(0.2, inplace=True))\n        self.encoder4 = nn.Sequential(nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1), nn.InstanceNorm2d(512), nn.LeakyReLU(0.2, inplace=True))\n        self.bottleneck = nn.Sequential(nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1), nn.ReLU(inplace=True))\n        self.decoder1 = nn.Sequential(nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1), nn.InstanceNorm2d(512), nn.ReLU(inplace=True))\n        self.decoder2 = nn.Sequential(nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1), nn.InstanceNorm2d(256), nn.ReLU(inplace=True))\n        self.decoder3 = nn.Sequential(nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1), nn.InstanceNorm2d(128), nn.ReLU(inplace=True))\n        self.decoder4 = nn.Sequential(nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1), nn.InstanceNorm2d(64), nn.ReLU(inplace=True))\n        self.final_layer = nn.Sequential(nn.ConvTranspose2d(128, out_channels, kernel_size=4, stride=2, padding=1), nn.Tanh())\n\n    def forward(self, x_agnostic, x_warped_cloth):\n        x = torch.cat([x_agnostic, x_warped_cloth], 1)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n        b = self.bottleneck(e4)\n        d1 = self.decoder1(b)\n        d2 = self.decoder2(torch.cat([d1, e4], 1))\n        d3 = self.decoder3(torch.cat([d2, e3], 1))\n        d4 = self.decoder4(torch.cat([d3, e2], 1))\n        out = self.final_layer(torch.cat([d4, e1], 1))\n        return out\n\nclass Discriminator(nn.Module):\n    \"\"\"A PatchGAN Discriminator to classify image patches as real or fake.\"\"\"\n    def __init__(self, in_channels=6):\n        super(Discriminator, self).__init__()\n        def discriminator_block(in_filters, out_filters, stride=2, norm=True):\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=stride, padding=1)]\n            if norm: layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n        self.model = nn.Sequential(\n            *discriminator_block(in_channels, 64, norm=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512, stride=1),\n            nn.Conv2d(512, 1, 4, padding=1)\n        )\n    def forward(self, img_A, img_B):\n        img_input = torch.cat((img_A, img_B), 1)\n        return self.model(img_input)\n\n# --- CONFIGURATION & TRAINING SCRIPT ---\n# Paths\nDATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'\nGMM_CHECKPOINT_PATH = '/kaggle/input/gmm_5epoch/pytorch/default/1/gmm_final (works but low epochs).pth'\nOUTPUT_DIR = '/kaggle/working/'\nCHECKPOINT_DIR_HD = os.path.join(OUTPUT_DIR, 'checkpoints_hd_viton')\nVISUALIZATION_DIR_HD = os.path.join(OUTPUT_DIR, 'visualizations_hd_viton')\n\n# Hyperparameters\nBATCH_SIZE = 4; NUM_EPOCHS = 200; LEARNING_RATE_G = 2e-4; LEARNING_RATE_D = 2e-4\nIMAGE_SIZE = (256, 192); ORIGINAL_IMAGE_SIZE = (768, 1024)\nLAMBDA_L1 = 10.0; LAMBDA_VGG = 10.0\n\nos.makedirs(CHECKPOINT_DIR_HD, exist_ok=True); os.makedirs(VISUALIZATION_DIR_HD, exist_ok=True)\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n    \n    # --- Initialize Models ---\n    generator = GeneratorHD().to(device)\n    discriminator = Discriminator().to(device)\n    gmm = GMM(in_channels_pose=25).to(device)\n    vgg_loss_fn = VGGPerceptualLoss().to(device)\n    \n    # --- Load Pre-trained GMM ---\n    if not os.path.isfile(GMM_CHECKPOINT_PATH): print(f\"FATAL ERROR: GMM checkpoint not found at '{GMM_CHECKPOINT_PATH}'\"); return\n    gmm.load_state_dict(torch.load(GMM_CHECKPOINT_PATH, map_location=device)); gmm.eval(); gmm.requires_grad_(False)\n    \n    # --- Optimizers and Loss ---\n    optimizer_G = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE_G, betas=(0.5, 0.999))\n    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_D, betas=(0.5, 0.999))\n    criterion_GAN = nn.BCEWithLogitsLoss().to(device)\n    criterion_L1 = nn.L1Loss().to(device)\n\n    # --- Resumption Logic ---\n    start_epoch = 0; checkpoint_path = os.path.join(CHECKPOINT_DIR_HD, 'hd_latest.pth')\n    if os.path.isfile(checkpoint_path):\n        print(f\"Resuming HD-VITON training from checkpoint: {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path)\n        generator.load_state_dict(checkpoint['generator_state_dict'])\n        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n        optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n        optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        print(f\"Resumed from Epoch {start_epoch}\")\n    else: print(\"Starting HD-VITON training from scratch.\")\n    \n    # --- Data Loading ---\n    train_dataset = FashionVTONDataset(data_root=DATA_ROOT, image_size=IMAGE_SIZE, original_image_size=ORIGINAL_IMAGE_SIZE)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    \n    for epoch in range(start_epoch, NUM_EPOCHS):\n        generator.train(); discriminator.train()\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n        \n        for i, batch in enumerate(progress_bar):\n            # --- Prepare Inputs ---\n            real_image = batch['person_image'].to(device)\n            cloth_image = batch['cloth_image'].to(device)\n            pose_map = batch['pose_map'].to(device)\n            agnostic_person = batch['agnostic_person'].to(device)\n            with torch.no_grad():\n                flow = gmm(cloth_image, pose_map)\n                warped_cloth = warp_cloth_with_flow(cloth_image, flow)\n\n            # --- Train Discriminator ---\n            optimizer_D.zero_grad()\n            fake_image = generator(agnostic_person, warped_cloth)\n            # Real\n            pred_real = discriminator(agnostic_person, real_image)\n            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n            # Fake\n            pred_fake = discriminator(agnostic_person, fake_image.detach())\n            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n            # Total D loss\n            loss_D = (loss_D_real + loss_D_fake) * 0.5\n            loss_D.backward(); optimizer_D.step()\n\n            # --- Train Generator ---\n            optimizer_G.zero_grad()\n            pred_fake_for_G = discriminator(agnostic_person, fake_image)\n            loss_G_gan = criterion_GAN(pred_fake_for_G, torch.ones_like(pred_fake_for_G))\n            loss_G_l1 = criterion_L1(fake_image, real_image) * LAMBDA_L1\n            loss_G_vgg = vgg_loss_fn(fake_image, real_image) * LAMBDA_VGG\n            # Total G loss\n            loss_G = loss_G_gan + loss_G_l1 + loss_G_vgg\n            loss_G.backward(); optimizer_G.step()\n            \n            progress_bar.set_postfix(D_loss=loss_D.item(), G_loss=loss_G.item(), G_gan=loss_G_gan.item(), G_l1=loss_G_l1.item())\n\n        # --- Visualization & Checkpointing ---\n        if epoch % 5 == 0:\n            with torch.no_grad():\n                vis_batch = torch.cat([real_image.cpu(), agnostic_person.cpu(), warped_cloth.cpu(), fake_image.cpu()], 0)\n                save_image((vis_batch + 1) / 2.0, os.path.join(VISUALIZATION_DIR_HD, f'epoch_{epoch+1}.png'), nrow=BATCH_SIZE)\n        \n        latest_checkpoint_state = {\n            'epoch': epoch, 'generator_state_dict': generator.state_dict(), 'discriminator_state_dict': discriminator.state_dict(),\n            'optimizer_G_state_dict': optimizer_G.state_dict(), 'optimizer_D_state_dict': optimizer_D.state_dict()\n        }\n        torch.save(latest_checkpoint_state, checkpoint_path)\n        torch.save(generator.state_dict(), os.path.join(CHECKPOINT_DIR_HD, 'generator_latest.pth'))\n        \n    print(\"Training finished.\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_tps_warper.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image\nimport os\nfrom tqdm import tqdm\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nimport json\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\nimport time\n\n# ===================================================================\n# ALL CLASS DEFINITIONS FOR THIS STAGE\n# ===================================================================\n\n# --- Dataset Class (Unchanged) ---\nclass FashionVTONDataset(Dataset):\n    def __init__(self, data_root, image_size=(256, 192), original_image_size=(768, 1024)):\n        self.data_root = data_root; self.image_size = image_size; self.original_image_size = original_image_size; self.image_dir = os.path.join(data_root, 'image'); self.cloth_dir = os.path.join(data_root, 'cloth'); self.cloth_mask_dir = os.path.join(data_root, 'cloth-mask'); self.pose_dir = os.path.join(data_root, 'openpose_json'); self.parse_dir = os.path.join(data_root, 'image-parse-v3'); self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))]); self.transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BILINEAR), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]); self.mask_transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])\n    def __len__(self): return len(self.image_files)\n    def find_upper_cloth_label(self, pose_keypoints, parse_array):\n        torso_indices = [1, 2, 5, 8, 9, 12]; visible_points = []\n        for i in torso_indices:\n            if i < len(pose_keypoints): x, y, conf = pose_keypoints[i];\n            if conf > 0.1: visible_points.append((int(x), int(y)))\n        if len(visible_points) < 3: return 5\n        x_coords, y_coords = zip(*visible_points); min_x, max_x = min(x_coords), max(x_coords); min_y, max_y = min(y_coords), max(y_coords)\n        if max_x <= min_x or max_y <= min_y: return 5\n        torso_parse_area = parse_array[min_y:max_y, min_x:max_x]; unique_labels, counts = np.unique(torso_parse_area, return_counts=True); non_zero_mask = (unique_labels != 0)\n        if np.any(non_zero_mask): return unique_labels[non_zero_mask][np.argmax(counts[non_zero_mask])]\n        else: return 5\n    def __getitem__(self, idx):\n        image_name = self.image_files[idx]; base_name = os.path.splitext(image_name)[0]\n        person_image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB'); cloth_image = Image.open(os.path.join(self.cloth_dir, image_name)).convert('RGB'); cloth_mask = Image.open(os.path.join(self.cloth_mask_dir, image_name)).convert('L')\n        try:\n            with open(os.path.join(self.pose_dir, f\"{base_name}_keypoints.json\"), 'r') as f: pose_data = json.load(f)\n            pose_keypoints = np.array(pose_data['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n        except (FileNotFoundError, IndexError): pose_keypoints = np.zeros((25, 3), dtype=np.float32)\n        parse_path = os.path.join(self.parse_dir, f\"{base_name}.png\")\n        if not os.path.exists(parse_path): parse_path = os.path.join(self.parse_dir, f\"{base_name}.jpg\")\n        parse_array_orig = np.array(Image.open(parse_path).convert('L'))\n        upper_cloth_label = self.find_upper_cloth_label(pose_keypoints, parse_array_orig)\n        parse_array_resized = cv2.resize(parse_array_orig, self.image_size[::-1], interpolation=cv2.INTER_NEAREST)\n        person_cloth_mask = (parse_array_resized == upper_cloth_label).astype(np.float32)\n        person_image_tensor = self.transform(person_image); cloth_image_tensor = self.transform(cloth_image); cloth_mask_tensor = self.mask_transform(cloth_mask)\n        pose_map_tensor = torch.from_numpy(self.create_pose_map(pose_keypoints)).float()\n        blurred_mask_tensor = torch.from_numpy(cv2.GaussianBlur(person_cloth_mask, (5, 5), 0)).unsqueeze(0)\n        agnostic_person_tensor = person_image_tensor * (1 - blurred_mask_tensor)\n        warped_cloth_tensor = person_image_tensor * torch.from_numpy(person_cloth_mask).unsqueeze(0)\n        return {'person_image': person_image_tensor, 'cloth_image': cloth_image_tensor, 'cloth_mask': cloth_mask_tensor, 'agnostic_person': agnostic_person_tensor, 'pose_map': pose_map_tensor, 'warped_cloth': warped_cloth_tensor}\n    def create_pose_map(self, keypoints):\n        h, w = self.image_size; orig_w, orig_h = self.original_image_size; num_keypoints = keypoints.shape[0]\n        pose_map = np.zeros((num_keypoints, h, w), dtype=np.float32)\n        for i, point in enumerate(keypoints):\n            if point[2] > 0.1:\n                x, y = int(point[0] * w / orig_w), int(point[1] * h / orig_h)\n                if 0 <= x < w and 0 <= y < h: cv2.circle(pose_map[i], (x, y), radius=3, color=1, thickness=-1)\n        return pose_map\n\n# --- NEW TPS Warper Network Definition ---\n# --- NEW TPS Warper Network Definition ---\nclass TPSWarper(nn.Module):\n    \"\"\"\n    A network that predicts TPS transformation parameters.\n    This version is simplified and designed to work directly with the pure PyTorch warp function.\n    \"\"\"\n    def __init__(self, feature_channels=256, num_control_points=25):\n        super().__init__()\n        self.num_control_points = num_control_points\n        \n        self.cloth_feature_extractor = self._make_feature_extractor(3, feature_channels)\n        self.person_feature_extractor = self._make_feature_extractor(28, feature_channels)\n        \n        self.regressor = nn.Sequential(\n            nn.Linear(feature_channels * 2, 256), nn.ReLU(inplace=True),\n            nn.Linear(256, 128), nn.ReLU(inplace=True),\n            nn.Linear(128, num_control_points * 2),\n            nn.Tanh() \n        )\n        \n        grid_size = int(np.sqrt(num_control_points))\n        if grid_size * grid_size != num_control_points:\n            raise ValueError(f\"num_control_points must be a perfect square. Got {num_control_points}\")\n            \n        grid = torch.linspace(-0.5, 0.5, grid_size) # Use a smaller default grid\n        x, y = torch.meshgrid(grid, grid, indexing='ij')\n        self.register_buffer('source_control_points', torch.stack([x.flatten(), y.flatten()], dim=-1))\n\n    def _make_feature_extractor(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, 64, 3, 2, 1), nn.ReLU(),\n            nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(),\n            nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n\n    def forward(self, cloth_image, pose_map, agnostic_person):\n        cloth_features = self.cloth_feature_extractor(cloth_image).view(cloth_image.size(0), -1)\n        person_input = torch.cat([agnostic_person, pose_map], dim=1)\n        person_features = self.person_feature_extractor(person_input).view(person_input.size(0), -1)\n\n        # Concatenate features instead of multiplying for more stability\n        correlation_features = torch.cat([cloth_features, person_features], dim=1)\n        offsets = self.regressor(correlation_features).view(-1, self.num_control_points, 2)\n        \n        destination_control_points = self.source_control_points + (offsets * 0.5) # Scale offsets\n        \n        return self.source_control_points, destination_control_points\n# Helper function to perform TPS warp given control points\n# Helper function to perform TPS warp given control points\n# Helper function to perform TPS warp given control points\n# Helper function to perform TPS warp given control points\ndef tps_warp(source_image, src_pts, dst_pts):\n    \"\"\"\n    Performs a batch-friendly affine warp using PyTorch only.\n    It solves for the best-fit affine matrix for each item in the batch\n    and then applies it with F.grid_sample.\n    \"\"\"\n    # List to store the affine matrices for each item in the batch\n    M_list = []\n    \n    # --- FIX: Iterate over the batch ---\n    for i in range(src_pts.size(0)):\n        # Get the points for the current item in the batch\n        src_item_pts = src_pts[i]\n        dst_item_pts = dst_pts[i]\n        \n        # Pad source and destination points to create homogeneous coordinates\n        # These are now 2D tensors [25, 3]\n        src_homo = F.pad(src_item_pts, (0, 1), \"constant\", 1.0)\n        dst_homo = F.pad(dst_item_pts, (0, 1), \"constant\", 1.0)\n        \n        # Solve the least squares problem to find the affine matrix\n        # These are now simple 2D inputs, which lstsq handles perfectly\n        try:\n            A_inv_B = torch.linalg.lstsq(src_homo, dst_homo).solution\n            M = A_inv_B.transpose(0, 1)[:2, :] # Extract the 2x3 affine matrix\n            M_list.append(M.unsqueeze(0))\n        except torch.linalg.LinAlgError:\n            # If the full solve fails, use a robust 3-point estimate\n            M_3pt = cv2.getAffineTransform(src_item_pts[:3].cpu().numpy().astype(np.float32), \n                                           dst_item_pts[:3].cpu().numpy().astype(np.float32))\n            M_list.append(torch.from_numpy(M_3pt).unsqueeze(0).to(source_image.device))\n\n\n    # Recombine the list of matrices into a single batch tensor\n    M_tensor = torch.cat(M_list, dim=0).to(source_image.device).float()\n\n    # Create the grid and warp the image for the entire batch at once\n    grid = F.affine_grid(M_tensor, source_image.size(), align_corners=False)\n    warped_image = F.grid_sample(source_image, grid, align_corners=False)\n    \n    return warped_image\n# --- Distributed Training Setup ---\ndef setup(rank, world_size, sync_file):\n    init_method = f'file://{sync_file}'\n    if rank == 0: print(f\"Initializing process group with: {init_method}\")\n    dist.init_process_group(\"nccl\", init_method=init_method, rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n# --- Main Training Function ---\ndef train(rank, world_size, sync_file):\n    setup(rank, world_size, sync_file)\n    \n    # --- Configuration ---\n    DATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'\n    CHECKPOINT_DIR = '/kaggle/working/tps_warper_checkpoints/'\n    VISUALIZATION_DIR = '/kaggle/working/tps_warper_visuals/'\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True); os.makedirs(VISUALIZATION_DIR, exist_ok=True)\n    \n    BATCH_SIZE = 32; NUM_EPOCHS = 50; LEARNING_RATE = 1e-4\n    IMAGE_SIZE = (256, 192); ORIGINAL_IMAGE_SIZE = (768, 1024)\n\n    # --- Setup Device, Model, Optimizer, Loss ---\n    torch.cuda.set_device(rank)\n    model = TPSWarper().to(rank)\n    model = DDP(model, device_ids=[rank])\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    l1_loss_fn = nn.L1Loss().to(rank)\n    scaler = torch.cuda.amp.GradScaler()\n\n    # --- Resumption Logic ---\n    start_epoch = 0; checkpoint_path = os.path.join(CHECKPOINT_DIR, 'tps_latest_ddp.pth')\n    if os.path.isfile(checkpoint_path) and rank == 0:\n        print(f\"Loading checkpoint: {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path, map_location={'cuda:0': f'cuda:{rank}'})\n        model.module.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n    dist.barrier() \n\n    # --- Data Loading ---\n    dataset = FashionVTONDataset(data_root=DATA_ROOT, image_size=IMAGE_SIZE, original_image_size=ORIGINAL_IMAGE_SIZE)\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)\n\n    # --- Training Loop ---\n    for epoch in range(start_epoch, NUM_EPOCHS):\n        sampler.set_epoch(epoch); model.train()\n        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", disable=(rank != 0))\n\n        for batch in progress_bar:\n            optimizer.zero_grad()\n            cloth_image = batch['cloth_image'].to(rank); cloth_mask = batch['cloth_mask'].to(rank)\n            pose_map = batch['pose_map'].to(rank); agnostic_person = batch['agnostic_person'].to(rank)\n            ground_truth_warped = batch['warped_cloth'].to(rank)\n            \n            with torch.cuda.amp.autocast():\n                src_pts, dst_pts = model(cloth_image, pose_map, agnostic_person)\n                # Use kornia/fallback to warp the cloth\n                predicted_warped = tps_warp(cloth_image, src_pts, dst_pts)\n                \n                # FOCUS ON L1 LOSS ONLY FOR GEOMETRY\n                total_loss = l1_loss_fn(predicted_warped, ground_truth_warped)\n\n            scaler.scale(total_loss).backward()\n            scaler.step(optimizer); scaler.update()\n            \n            if rank == 0: progress_bar.set_postfix(loss=total_loss.item())\n        \n        # --- Checkpointing & Visualization (only from rank 0) ---\n        if rank == 0:\n            latest_checkpoint_state = {\n                'epoch': epoch, 'model_state_dict': model.module.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(), 'scaler_state_dict': scaler.state_dict(),\n            }\n            torch.save(latest_checkpoint_state, checkpoint_path)\n            \n            with torch.no_grad():\n                vis_batch = torch.cat([(cloth_image.cpu() + 1) / 2, (predicted_warped.cpu().detach() + 1) / 2, (ground_truth_warped.cpu() + 1) / 2], dim=0)\n                save_image(vis_batch, os.path.join(VISUALIZATION_DIR, f'epoch_{epoch+1}_comparison.png'), nrow=BATCH_SIZE)\n\n            print(f\"Epoch {epoch+1} finished. Checkpoint saved.\")\n\n    if rank == 0:\n        final_model_path = os.path.join(CHECKPOINT_DIR, 'tps_warper_final.pth')\n        torch.save(model.module.state_dict(), final_model_path)\n        print(f\"Final model saved to {final_model_path}\")\n        \n    cleanup()\n\nif __name__ == '__main__':\n    world_size = torch.cuda.device_count()\n    if world_size < 2: print(\"Distributed training requires at least 2 GPUs.\")\n    else:\n        sync_file_path = os.path.join('/kaggle/working', 'ddp_sync_file')\n        if os.path.exists(sync_file_path): os.remove(sync_file_path)\n        torch.multiprocessing.spawn(train, args=(world_size, sync_file_path), nprocs=world_size, join=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:50:46.535233Z","iopub.execute_input":"2025-07-16T15:50:46.536029Z","iopub.status.idle":"2025-07-16T15:50:46.547120Z","shell.execute_reply.started":"2025-07-16T15:50:46.535999Z","shell.execute_reply":"2025-07-16T15:50:46.546346Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_tps_warper.py\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"!pip install kornia --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:25:50.270533Z","iopub.execute_input":"2025-07-16T15:25:50.270755Z","iopub.status.idle":"2025-07-16T15:27:00.380751Z","shell.execute_reply.started":"2025-07-16T15:25:50.270739Z","shell.execute_reply":"2025-07-16T15:27:00.380058Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!python train_tps_warper.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:50:51.758620Z","iopub.execute_input":"2025-07-16T15:50:51.759337Z","iopub.status.idle":"2025-07-16T15:51:25.600275Z","shell.execute_reply.started":"2025-07-16T15:50:51.759313Z","shell.execute_reply":"2025-07-16T15:51:25.599573Z"}},"outputs":[{"name":"stdout","text":"Initializing process group with: file:///kaggle/working/ddp_sync_file\n/kaggle/working/train_tps_warper.py:189: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n/kaggle/working/train_tps_warper.py:189: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\nEpoch 1/50:   0%|                                       | 0/182 [00:00<?, ?it/s]/kaggle/working/train_tps_warper.py:218: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/kaggle/working/train_tps_warper.py:218: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nterminate called without an active exception\nterminate called without an active exception\nEpoch 1/50:   0%|                                       | 0/182 [00:24<?, ?it/s]\n[rank0]:[W716 15:51:23.336057092 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nW0716 15:51:24.656000 1081 torch/multiprocessing/spawn.py:169] Terminating process 1085 via signal SIGTERM\nTraceback (most recent call last):\n  File \"/kaggle/working/train_tps_warper.py\", line 258, in <module>\n    torch.multiprocessing.spawn(train, args=(world_size, sync_file_path), nprocs=world_size, join=True)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 340, in spawn\n    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 296, in start_processes\n    while not context.join():\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 215, in join\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\ntorch.multiprocessing.spawn.ProcessRaisedException: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/kaggle/working/train_tps_warper.py\", line 221, in train\n    predicted_warped = tps_warp(cloth_image, src_pts, dst_pts)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/train_tps_warper.py\", line 143, in tps_warp\n    A_inv_B = torch.linalg.lstsq(src_homo, dst_homo).solution\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: torch.linalg.lstsq: input must have at least 2 dimensions.\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"%%writefile train_tps_warper.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image\nimport os\nfrom tqdm import tqdm\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nimport json\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\n\n# ===================================================================\n# ALL CLASS DEFINITIONS FOR STAGE 1\n# ===================================================================\n\nclass FashionVTONDataset(Dataset):\n    def __init__(self, data_root, image_size=(256, 192), original_image_size=(768, 1024)):\n        self.data_root = data_root; self.image_size = image_size; self.original_image_size = original_image_size; self.image_dir = os.path.join(data_root, 'image'); self.cloth_dir = os.path.join(data_root, 'cloth'); self.cloth_mask_dir = os.path.join(data_root, 'cloth-mask'); self.pose_dir = os.path.join(data_root, 'openpose_json'); self.parse_dir = os.path.join(data_root, 'image-parse-v3'); self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))]); self.transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BILINEAR), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]); self.mask_transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])\n    def __len__(self): return len(self.image_files)\n    def find_upper_cloth_label(self, pose_keypoints, parse_array):\n        torso_indices = [1, 2, 5, 8, 9, 12]; visible_points = []\n        for i in torso_indices:\n            if i < len(pose_keypoints): x, y, conf = pose_keypoints[i];\n            if conf > 0.1: visible_points.append((int(x), int(y)))\n        if len(visible_points) < 3: return 5\n        x_coords, y_coords = zip(*visible_points); min_x, max_x = min(x_coords), max(x_coords); min_y, max_y = min(y_coords), max(y_coords)\n        if max_x <= min_x or max_y <= min_y: return 5\n        torso_parse_area = parse_array[min_y:max_y, min_x:max_x]; unique_labels, counts = np.unique(torso_parse_area, return_counts=True); non_zero_mask = (unique_labels != 0)\n        if np.any(non_zero_mask): return unique_labels[non_zero_mask][np.argmax(counts[non_zero_mask])]\n        else: return 5\n    def __getitem__(self, idx):\n        image_name = self.image_files[idx]; base_name = os.path.splitext(image_name)[0]\n        person_image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB'); cloth_image = Image.open(os.path.join(self.cloth_dir, image_name)).convert('RGB'); cloth_mask = Image.open(os.path.join(self.cloth_mask_dir, image_name)).convert('L')\n        try:\n            with open(os.path.join(self.pose_dir, f\"{base_name}_keypoints.json\"), 'r') as f: pose_data = json.load(f)\n            pose_keypoints = np.array(pose_data['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n        except (FileNotFoundError, IndexError): pose_keypoints = np.zeros((25, 3), dtype=np.float32)\n        parse_path = os.path.join(self.parse_dir, f\"{base_name}.png\")\n        if not os.path.exists(parse_path): parse_path = os.path.join(self.parse_dir, f\"{base_name}.jpg\")\n        parse_array_orig = np.array(Image.open(parse_path).convert('L'))\n        upper_cloth_label = self.find_upper_cloth_label(pose_keypoints, parse_array_orig)\n        parse_array_resized = cv2.resize(parse_array_orig, self.image_size[::-1], interpolation=cv2.INTER_NEAREST)\n        person_cloth_mask = (parse_array_resized == upper_cloth_label).astype(np.float32)\n        person_image_tensor = self.transform(person_image); cloth_image_tensor = self.transform(cloth_image); cloth_mask_tensor = self.mask_transform(cloth_mask)\n        pose_map_tensor = torch.from_numpy(self.create_pose_map(pose_keypoints)).float()\n        blurred_mask_tensor = torch.from_numpy(cv2.GaussianBlur(person_cloth_mask, (5, 5), 0)).unsqueeze(0)\n        agnostic_person_tensor = person_image_tensor * (1 - blurred_mask_tensor)\n        warped_cloth_tensor = person_image_tensor * torch.from_numpy(person_cloth_mask).unsqueeze(0)\n        return {'person_image': person_image_tensor, 'cloth_image': cloth_image_tensor, 'cloth_mask': cloth_mask_tensor, 'agnostic_person': agnostic_person_tensor, 'pose_map': pose_map_tensor, 'warped_cloth': warped_cloth_tensor}\n    def create_pose_map(self, keypoints):\n        h, w = self.image_size; orig_w, orig_h = self.original_image_size; num_keypoints = keypoints.shape[0]\n        pose_map = np.zeros((num_keypoints, h, w), dtype=np.float32)\n        for i, point in enumerate(keypoints):\n            if point[2] > 0.1:\n                x, y = int(point[0] * w / orig_w), int(point[1] * h / orig_h)\n                if 0 <= x < w and 0 <= y < h: cv2.circle(pose_map[i], (x, y), radius=3, color=1, thickness=-1)\n        return pose_map\n\ndef tps_warp(source_image, src_pts, dst_pts):\n    M_list = []\n    for i in range(src_pts.size(0)):\n        src_homo = F.pad(src_pts[i], (0, 1), \"constant\", 1.0)\n        dst_homo = F.pad(dst_pts[i], (0, 1), \"constant\", 1.0)\n        try:\n            A_inv_B = torch.linalg.lstsq(src_homo, dst_homo).solution\n            M = A_inv_B.transpose(0, 1)[:2, :]\n            M_list.append(M.unsqueeze(0))\n        except torch.linalg.LinAlgError:\n            M_3pt = cv2.getAffineTransform(src_pts[i, :3].detach().cpu().numpy().astype(np.float32), \n                                           dst_pts[i, :3].detach().cpu().numpy().astype(np.float32))\n            M_list.append(torch.from_numpy(M_3pt).unsqueeze(0).to(source_image.device))\n    M_tensor = torch.cat(M_list, dim=0).to(source_image.device).float()\n    grid = F.affine_grid(M_tensor, source_image.size(), align_corners=False)\n    return F.grid_sample(source_image, grid, align_corners=False)\n\nclass TPSWarper(nn.Module):\n    def __init__(self, feature_channels=256, num_control_points=25):\n        super().__init__()\n        self.num_control_points = num_control_points\n        self.cloth_feature_extractor = self._make_feature_extractor(3, feature_channels)\n        self.person_feature_extractor = self._make_feature_extractor(28, feature_channels)\n        self.regressor = nn.Sequential(nn.Linear(feature_channels * 2, 256), nn.ReLU(inplace=True), nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Linear(128, num_control_points * 2), nn.Tanh())\n        grid_size = int(np.sqrt(num_control_points))\n        if grid_size * grid_size != num_control_points: raise ValueError(f\"num_control_points must be a perfect square. Got {num_control_points}\")\n        grid = torch.linspace(-0.5, 0.5, grid_size)\n        x, y = torch.meshgrid(grid, grid, indexing='ij')\n        self.register_buffer('source_control_points', torch.stack([x.flatten(), y.flatten()], dim=-1))\n    def _make_feature_extractor(self, in_channels, out_channels):\n        return nn.Sequential(nn.Conv2d(in_channels, 64, 3, 2, 1), nn.ReLU(), nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(), nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(), nn.AdaptiveAvgPool2d(1))\n    def forward(self, cloth_image, pose_map, agnostic_person):\n        batch_size = cloth_image.size(0)\n        cloth_features = self.cloth_feature_extractor(cloth_image).view(batch_size, -1)\n        person_input = torch.cat([agnostic_person, pose_map], dim=1)\n        person_features = self.person_feature_extractor(person_input).view(batch_size, -1)\n        correlation_features = torch.cat([cloth_features, person_features], dim=1)\n        offsets = self.regressor(correlation_features).view(batch_size, self.num_control_points, 2)\n        # --- THIS IS THE FIX: Expand the source points to match the batch size ---\n        src_pts_batch = self.source_control_points.unsqueeze(0).expand(batch_size, -1, -1)\n        dst_pts_batch = src_pts_batch + (offsets * 0.5)\n        return src_pts_batch, dst_pts_batch\n\ndef setup(rank, world_size, sync_file):\n    init_method = f'file://{sync_file}'\n    dist.init_process_group(\"nccl\", init_method=init_method, rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef train(rank, world_size, sync_file):\n    setup(rank, world_size, sync_file)\n    DATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'; CHECKPOINT_DIR = '/kaggle/working/tps_warper_checkpoints/'; VISUALIZATION_DIR = '/kaggle/working/tps_warper_visuals/'\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True); os.makedirs(VISUALIZATION_DIR, exist_ok=True)\n    BATCH_SIZE = 32; NUM_EPOCHS = 50; LEARNING_RATE = 1e-4; IMAGE_SIZE = (256, 192); ORIGINAL_IMAGE_SIZE = (768, 1024)\n    torch.cuda.set_device(rank)\n    model = TPSWarper().to(rank); model = DDP(model, device_ids=[rank])\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE); l1_loss_fn = nn.L1Loss().to(rank); scaler = torch.cuda.amp.GradScaler()\n    start_epoch = 0; checkpoint_path = os.path.join(CHECKPOINT_DIR, 'tps_latest_ddp.pth')\n    if os.path.isfile(checkpoint_path) and rank == 0:\n        checkpoint = torch.load(checkpoint_path, map_location={'cuda:0': f'cuda:{rank}'})\n        model.module.load_state_dict(checkpoint['model_state_dict']); optimizer.load_state_dict(checkpoint['optimizer_state_dict']); scaler.load_state_dict(checkpoint['scaler_state_dict']); start_epoch = checkpoint['epoch'] + 1\n    dist.barrier()\n    dataset = FashionVTONDataset(data_root=DATA_ROOT, image_size=IMAGE_SIZE, original_image_size=ORIGINAL_IMAGE_SIZE)\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)\n    for epoch in range(start_epoch, NUM_EPOCHS):\n        sampler.set_epoch(epoch); model.train()\n        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", disable=(rank != 0))\n        for batch in progress_bar:\n            optimizer.zero_grad()\n            cloth_image = batch['cloth_image'].to(rank); pose_map = batch['pose_map'].to(rank); agnostic_person = batch['agnostic_person'].to(rank); ground_truth_warped = batch['warped_cloth'].to(rank)\n            with torch.cuda.amp.autocast():\n                src_pts, dst_pts = model(cloth_image, pose_map, agnostic_person)\n                predicted_warped = tps_warp(cloth_image, src_pts, dst_pts)\n                total_loss = l1_loss_fn(predicted_warped, ground_truth_warped)\n            scaler.scale(total_loss).backward(); scaler.step(optimizer); scaler.update()\n            if rank == 0: progress_bar.set_postfix(loss=total_loss.item())\n        if rank == 0:\n            latest_checkpoint_state = {'epoch': epoch, 'model_state_dict': model.module.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'scaler_state_dict': scaler.state_dict()}\n            torch.save(latest_checkpoint_state, checkpoint_path)\n            with torch.no_grad():\n                vis_batch = torch.cat([(cloth_image.cpu() + 1) / 2, (predicted_warped.cpu().detach() + 1) / 2, (ground_truth_warped.cpu() + 1) / 2], dim=0)\n                save_image(vis_batch, os.path.join(VISUALIZATION_DIR, f'epoch_{epoch+1}_comparison.png'), nrow=BATCH_SIZE)\n            print(f\"Epoch {epoch+1} finished. Checkpoint saved.\")\n    if rank == 0:\n        final_model_path = os.path.join(CHECKPOINT_DIR, 'tps_warper_final.pth'); torch.save(model.module.state_dict(), final_model_path); print(f\"Final model saved to {final_model_path}\")\n    cleanup()\n\nif __name__ == '__main__':\n    world_size = torch.cuda.device_count()\n    if world_size < 2: print(\"Distributed training requires at least 2 GPUs.\")\n    else:\n        sync_file_path = os.path.join('/kaggle/working', 'ddp_sync_file')\n        if os.path.exists(sync_file_path): os.remove(sync_file_path)\n        print(f\"Found {world_size} GPUs. Starting DDP with file sync at {sync_file_path}\")\n        torch.multiprocessing.spawn(train, args=(world_size, sync_file_path), nprocs=world_size, join=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:55:42.453584Z","iopub.execute_input":"2025-07-16T15:55:42.454234Z","iopub.status.idle":"2025-07-16T15:55:42.464246Z","shell.execute_reply.started":"2025-07-16T15:55:42.454207Z","shell.execute_reply":"2025-07-16T15:55:42.463525Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_tps_warper.py\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"!python train_tps_warper.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:55:53.418675Z","iopub.execute_input":"2025-07-16T15:55:53.418931Z","iopub.status.idle":"2025-07-16T16:32:07.482594Z","shell.execute_reply.started":"2025-07-16T15:55:53.418911Z","shell.execute_reply":"2025-07-16T16:32:07.481450Z"}},"outputs":[{"name":"stdout","text":"Found 2 GPUs. Starting DDP with file sync at /kaggle/working/ddp_sync_file\n/kaggle/working/train_tps_warper.py:122: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE); l1_loss_fn = nn.L1Loss().to(rank); scaler = torch.cuda.amp.GradScaler()\n/kaggle/working/train_tps_warper.py:122: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE); l1_loss_fn = nn.L1Loss().to(rank); scaler = torch.cuda.amp.GradScaler()\nEpoch 1/50:   0%|                                       | 0/182 [00:00<?, ?it/s]/kaggle/working/train_tps_warper.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/kaggle/working/train_tps_warper.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1/50: 100%|█████████████████| 182/182 [03:39<00:00,  1.21s/it, loss=0.296]\nEpoch 1 finished. Checkpoint saved.\nEpoch 2/50: 100%|█████████████████| 182/182 [03:41<00:00,  1.21s/it, loss=0.297]\nEpoch 2 finished. Checkpoint saved.\nEpoch 3/50: 100%|█████████████████| 182/182 [03:36<00:00,  1.19s/it, loss=0.312]\nEpoch 3 finished. Checkpoint saved.\nEpoch 4/50: 100%|█████████████████| 182/182 [03:36<00:00,  1.19s/it, loss=0.302]\nEpoch 4 finished. Checkpoint saved.\nEpoch 5/50: 100%|█████████████████| 182/182 [03:32<00:00,  1.17s/it, loss=0.311]\nEpoch 5 finished. Checkpoint saved.\nEpoch 6/50: 100%|█████████████████| 182/182 [03:36<00:00,  1.19s/it, loss=0.306]\nEpoch 6 finished. Checkpoint saved.\nEpoch 7/50: 100%|█████████████████| 182/182 [03:35<00:00,  1.18s/it, loss=0.313]\nEpoch 7 finished. Checkpoint saved.\nEpoch 8/50: 100%|█████████████████| 182/182 [03:31<00:00,  1.16s/it, loss=0.305]\nEpoch 8 finished. Checkpoint saved.\nEpoch 9/50: 100%|█████████████████| 182/182 [03:32<00:00,  1.17s/it, loss=0.315]\nEpoch 9 finished. Checkpoint saved.\nEpoch 10/50: 100%|██████████████████| 182/182 [03:34<00:00,  1.18s/it, loss=0.3]\nEpoch 10 finished. Checkpoint saved.\nEpoch 11/50:   0%|                                      | 0/182 [00:00<?, ?it/s]^C\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f8e3bec84a0>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\nEpoch 11/50:   0%|                                      | 0/182 [00:02<?, ?it/s]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    exitcode = _main(fd, parent_sentinel)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 131, in _main\n    prepare(preparation_data)\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 246, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 297, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen runpy>\", line 291, in run_path\nTraceback (most recent call last):\n  File \"/kaggle/working/train_tps_warper.py\", line 161, in <module>\n    self._shutdown_workers()\n    torch.multiprocessing.spawn(train, args=(world_size, sync_file_path), nprocs=world_size, join=True)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1576, in _shutdown_workers\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 340, in spawn\n    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 296, in start_processes\n    while not context.join():\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 144, in join\n    ready = multiprocessing.connection.wait(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n    if self._persistent_workers or self._workers_status[worker_id]:\n                                   ^^^^^^^^^^^^^^^^^^^^\nAttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n    ready = selector.select(timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n    fd_event_list = self._selector.poll(timeout)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n  File \"<frozen runpy>\", line 98, in _run_module_code\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/kaggle/working/train_tps_warper.py\", line 6, in <module>\n    from torchvision.utils import save_image\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 10, in <module>\n    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/__init__.py\", line 2, in <module>\n    from .convnext import *\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/convnext.py\", line 8, in <module>\n    from ..ops.misc import Conv2dNormActivation, Permute\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/ops/__init__.py\", line 23, in <module>\n    from .poolers import MultiScaleRoIAlign\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/ops/poolers.py\", line 10, in <module>\n    from .roi_align import roi_align\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/ops/roi_align.py\", line 7, in <module>\n    from torch._dynamo.utils import is_compile_supported\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/__init__.py\", line 3, in <module>\n    from . import convert_frame, eval_frame, resume_execution\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 33, in <module>\n    from torch._dynamo.symbolic_convert import TensorifyState\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 27, in <module>\n    from torch._dynamo.exc import TensorifyScalarRestartAnalysis\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/exc.py\", line 11, in <module>\n    from .utils import counters\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 99, in <module>\n    import torch._numpy as tnp\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_numpy/__init__.py\", line 3, in <module>\n    from . import fft, linalg, random\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_numpy/fft.py\", line 9, in <module>\n    from . import _dtypes_impl, _util\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_numpy/_dtypes_impl.py\", line 52, in <module>\n    from . import _casting_dicts as _cd\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n  File \"<frozen importlib._bootstrap_external>\", line 1069, in get_code\n  File \"<frozen importlib._bootstrap_external>\", line 729, in _compile_bytecode\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"%%writefile train_coarse_gmm.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image\nimport os\nfrom tqdm import tqdm\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nimport json\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\n\n# ===================================================================\n# CLASS DEFINITIONS\n# ===================================================================\n\nclass FashionVTONDataset(Dataset):\n    def __init__(self, data_root, image_size=(256, 192), original_image_size=(768, 1024)):\n        self.data_root = data_root; self.image_size = image_size; self.original_image_size = original_image_size; self.image_dir = os.path.join(data_root, 'image'); self.cloth_dir = os.path.join(data_root, 'cloth'); self.cloth_mask_dir = os.path.join(data_root, 'cloth-mask'); self.pose_dir = os.path.join(data_root, 'openpose_json'); self.parse_dir = os.path.join(data_root, 'image-parse-v3'); self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))]); self.transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BILINEAR), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]); self.mask_transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])\n    def __len__(self): return len(self.image_files)\n    def find_upper_cloth_label(self, pose_keypoints, parse_array):\n        torso_indices = [1, 2, 5, 8, 9, 12]; visible_points = []\n        for i in torso_indices:\n            if i < len(pose_keypoints): x, y, conf = pose_keypoints[i];\n            if conf > 0.1: visible_points.append((int(x), int(y)))\n        if len(visible_points) < 3: return 5\n        x_coords, y_coords = zip(*visible_points); min_x, max_x = min(x_coords), max(x_coords); min_y, max_y = min(y_coords), max(y_coords)\n        if max_x <= min_x or max_y <= min_y: return 5\n        torso_parse_area = parse_array[min_y:max_y, min_x:max_x]; unique_labels, counts = np.unique(torso_parse_area, return_counts=True); non_zero_mask = (unique_labels != 0)\n        if np.any(non_zero_mask): return unique_labels[non_zero_mask][np.argmax(counts[non_zero_mask])]\n        else: return 5\n    def __getitem__(self, idx):\n        image_name = self.image_files[idx]; base_name = os.path.splitext(image_name)[0]\n        person_image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB'); cloth_image = Image.open(os.path.join(self.cloth_dir, image_name)).convert('RGB'); cloth_mask = Image.open(os.path.join(self.cloth_mask_dir, image_name)).convert('L')\n        try:\n            with open(os.path.join(self.pose_dir, f\"{base_name}_keypoints.json\"), 'r') as f: pose_data = json.load(f)\n            pose_keypoints = np.array(pose_data['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n        except (FileNotFoundError, IndexError): pose_keypoints = np.zeros((25, 3), dtype=np.float32)\n        parse_path = os.path.join(self.parse_dir, f\"{base_name}.png\")\n        if not os.path.exists(parse_path): parse_path = os.path.join(self.parse_dir, f\"{base_name}.jpg\")\n        parse_array_orig = np.array(Image.open(parse_path).convert('L'))\n        upper_cloth_label = self.find_upper_cloth_label(pose_keypoints, parse_array_orig)\n        parse_array_resized = cv2.resize(parse_array_orig, self.image_size[::-1], interpolation=cv2.INTER_NEAREST)\n        person_cloth_mask = (parse_array_resized == upper_cloth_label).astype(np.float32)\n        person_image_tensor = self.transform(person_image); cloth_image_tensor = self.transform(cloth_image); cloth_mask_tensor = self.mask_transform(cloth_mask)\n        pose_map_tensor = torch.from_numpy(self.create_pose_map(pose_keypoints)).float()\n        blurred_mask_tensor = torch.from_numpy(cv2.GaussianBlur(person_cloth_mask, (5, 5), 0)).unsqueeze(0)\n        agnostic_person_tensor = person_image_tensor * (1 - blurred_mask_tensor)\n        warped_cloth_tensor = person_image_tensor * torch.from_numpy(person_cloth_mask).unsqueeze(0)\n        return {'person_image': person_image_tensor, 'cloth_image': cloth_image_tensor, 'cloth_mask': cloth_mask_tensor, 'agnostic_person': agnostic_person_tensor, 'pose_map': pose_map_tensor, 'warped_cloth': warped_cloth_tensor}\n    def create_pose_map(self, keypoints):\n        h, w = self.image_size; orig_w, orig_h = self.original_image_size; num_keypoints = keypoints.shape[0]\n        pose_map = np.zeros((num_keypoints, h, w), dtype=np.float32)\n        for i, point in enumerate(keypoints):\n            if point[2] > 0.1:\n                x, y = int(point[0] * w / orig_w), int(point[1] * h / orig_h)\n                if 0 <= x < w and 0 <= y < h: cv2.circle(pose_map[i], (x, y), radius=3, color=1, thickness=-1)\n        return pose_map\n\n# Using the robust U-Net GMM architecture from our first successful attempt\nclass GMM(nn.Module):\n    def __init__(self, in_channels_cloth=3, in_channels_pose=25, out_channels_flow=2):\n        super(GMM, self).__init__(); self.encoder1 = self.conv_block(in_channels_cloth + in_channels_pose, 64); self.encoder2 = self.conv_block(64, 128); self.encoder3 = self.conv_block(128, 256); self.encoder4 = self.conv_block(256, 512); self.pool = nn.MaxPool2d(2, 2); self.bottleneck = self.conv_block(512, 1024); self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2); self.decoder4 = self.conv_block(1024, 512); self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2); self.decoder3 = self.conv_block(512, 256); self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2); self.decoder2 = self.conv_block(256, 128); self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2); self.decoder1 = self.conv_block(128, 64); self.conv_out = nn.Conv2d(64, out_channels_flow, kernel_size=1); self.tanh = nn.Tanh()\n    def conv_block(self, in_channels, out_channels): return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True))\n    def forward(self, cloth_image, pose_map):\n        x = torch.cat([cloth_image, pose_map], dim=1); e1 = self.encoder1(x); p1 = self.pool(e1); e2 = self.encoder2(p1); p2 = self.pool(e2); e3 = self.encoder3(p2); p3 = self.pool(e3); e4 = self.encoder4(p3); p4 = self.pool(e4); b = self.bottleneck(p4); d4 = self.upconv4(b); d4 = torch.cat([d4, e4], dim=1); d4 = self.decoder4(d4); d3 = self.upconv3(d4); d3 = torch.cat([d3, e3], dim=1); d3 = self.decoder3(d3); d2 = self.upconv2(d3); d2 = torch.cat([d2, e2], dim=1); d2 = self.decoder2(d2); d1 = self.upconv1(d2); d1 = torch.cat([d1, e1], dim=1); d1 = self.decoder1(d1); flow_field = self.conv_out(d1); flow_field = self.tanh(flow_field); return flow_field\n\ndef warp_cloth_with_flow(cloth_image, flow_field):\n    flow_field = flow_field.permute(0, 2, 3, 1); warped_image = F.grid_sample(cloth_image, flow_field, mode='bilinear', padding_mode='zeros', align_corners=True); return warped_image\n\nclass TVLoss(nn.Module):\n    def __init__(self): super(TVLoss, self).__init__()\n    def forward(self, x):\n        batch_size, c, h, w = x.size(); tv_h = torch.pow(x[:,:,1:,:] - x[:,:,:-1,:], 2).sum(); tv_w = torch.pow(x[:,:,:,1:] - x[:,:,:,:-1], 2).sum()\n        return (tv_h + tv_w) / (batch_size * c * h * w)\n\ndef setup(rank, world_size, sync_file):\n    init_method = f'file://{sync_file}'\n    dist.init_process_group(\"nccl\", init_method=init_method, rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef train(rank, world_size, sync_file):\n    setup(rank, world_size, sync_file)\n    DATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'; CHECKPOINT_DIR = '/kaggle/working/coarse_gmm_checkpoints/'; VISUALIZATION_DIR = '/kaggle/working/coarse_gmm_visuals/'\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True); os.makedirs(VISUALIZATION_DIR, exist_ok=True)\n    BATCH_SIZE = 16; NUM_EPOCHS = 50; LEARNING_RATE = 2e-5; IMAGE_SIZE = (256, 192); ORIGINAL_IMAGE_SIZE = (768, 1024)\n    torch.cuda.set_device(rank)\n    model = GMM(in_channels_pose=25).to(rank) # Using the stable GMM architecture\n    model = DDP(model, device_ids=[rank])\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n    l1_loss_fn = nn.L1Loss().to(rank); tv_loss_fn = TVLoss().to(rank)\n    scaler = torch.cuda.amp.GradScaler()\n    start_epoch = 0; checkpoint_path = os.path.join(CHECKPOINT_DIR, 'coarse_gmm_latest.pth')\n    if os.path.isfile(checkpoint_path) and rank == 0:\n        checkpoint = torch.load(checkpoint_path, map_location={'cuda:0': f'cuda:{rank}'})\n        model.module.load_state_dict(checkpoint['model_state_dict']); optimizer.load_state_dict(checkpoint['optimizer_state_dict']); scaler.load_state_dict(checkpoint['scaler_state_dict']); start_epoch = checkpoint['epoch'] + 1\n    dist.barrier()\n    dataset = FashionVTONDataset(data_root=DATA_ROOT, image_size=IMAGE_SIZE, original_image_size=ORIGINAL_IMAGE_SIZE)\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)\n    for epoch in range(start_epoch, NUM_EPOCHS):\n        sampler.set_epoch(epoch); model.train()\n        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", disable=(rank != 0))\n        for batch in progress_bar:\n            optimizer.zero_grad()\n            cloth_image = batch['cloth_image'].to(rank); pose_map = batch['pose_map'].to(rank); ground_truth_warped = batch['warped_cloth'].to(rank)\n            with torch.cuda.amp.autocast():\n                predicted_flow = model(cloth_image, pose_map)\n                predicted_warped = warp_cloth_with_flow(cloth_image, predicted_flow)\n                # --- LOSS IS L1 + TV ONLY ---\n                loss_l1 = l1_loss_fn(predicted_warped, ground_truth_warped)\n                loss_tv = tv_loss_fn(predicted_flow)\n                total_loss = loss_l1 + 0.5 * loss_tv\n            scaler.scale(total_loss).backward(); scaler.step(optimizer); scaler.update()\n            if rank == 0: progress_bar.set_postfix(loss=total_loss.item())\n        if rank == 0:\n            latest_checkpoint_state = {'epoch': epoch, 'model_state_dict': model.module.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'scaler_state_dict': scaler.state_dict()}\n            torch.save(latest_checkpoint_state, checkpoint_path)\n            with torch.no_grad():\n                vis_batch = torch.cat([(cloth_image.cpu() + 1) / 2, (predicted_warped.cpu().detach() + 1) / 2, (ground_truth_warped.cpu() + 1) / 2], dim=0)\n                save_image(vis_batch, os.path.join(VISUALIZATION_DIR, f'epoch_{epoch+1}_comparison.png'), nrow=BATCH_SIZE)\n            print(f\"Epoch {epoch+1} finished. Checkpoint saved.\")\n    if rank == 0:\n        final_model_path = os.path.join(CHECKPOINT_DIR, 'coarse_gmm_final.pth'); torch.save(model.module.state_dict(), final_model_path); print(f\"Final model saved to {final_model_path}\")\n    cleanup()\n\nif __name__ == '__main__':\n    world_size = torch.cuda.device_count()\n    if world_size < 2: print(\"Distributed training requires at least 2 GPUs.\")\n    else:\n        sync_file_path = os.path.join('/kaggle/working', 'ddp_sync_file')\n        if os.path.exists(sync_file_path): os.remove(sync_file_path)\n        torch.multiprocessing.spawn(train, args=(world_size, sync_file_path), nprocs=world_size, join=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T02:32:26.489060Z","iopub.execute_input":"2025-07-17T02:32:26.489349Z","iopub.status.idle":"2025-07-17T02:32:26.499887Z","shell.execute_reply.started":"2025-07-17T02:32:26.489322Z","shell.execute_reply":"2025-07-17T02:32:26.499253Z"}},"outputs":[{"name":"stdout","text":"Writing train_coarse_gmm.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!python train_coarse_gmm.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:31:47.709236Z","iopub.execute_input":"2025-07-17T16:31:47.709474Z","iopub.status.idle":"2025-07-17T16:31:47.945242Z","shell.execute_reply.started":"2025-07-17T16:31:47.709452Z","shell.execute_reply":"2025-07-17T16:31:47.944170Z"}},"outputs":[{"name":"stdout","text":"python3: can't open file '/kaggle/working/train_coarse_gmm.py': [Errno 2] No such file or directory\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile train_coarse_gmm.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image\nimport os\nfrom tqdm import tqdm\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nimport json\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\n\n# ===================================================================\n# CLASS DEFINITIONS (Unchanged)\n# ===================================================================\nclass FashionVTONDataset(Dataset):\n    def __init__(self, data_root, image_size=(256, 192), original_image_size=(768, 1024)):\n        self.data_root = data_root; self.image_size = image_size; self.original_image_size = original_image_size; self.image_dir = os.path.join(data_root, 'image'); self.cloth_dir = os.path.join(data_root, 'cloth'); self.cloth_mask_dir = os.path.join(data_root, 'cloth-mask'); self.pose_dir = os.path.join(data_root, 'openpose_json'); self.parse_dir = os.path.join(data_root, 'image-parse-v3'); self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))]); self.transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BILINEAR), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]); self.mask_transform = transforms.Compose([transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])\n    def __len__(self): return len(self.image_files)\n    def find_upper_cloth_label(self, pose_keypoints, parse_array):\n        torso_indices = [1, 2, 5, 8, 9, 12]; visible_points = []\n        for i in torso_indices:\n            if i < len(pose_keypoints): x, y, conf = pose_keypoints[i];\n            if conf > 0.1: visible_points.append((int(x), int(y)))\n        if len(visible_points) < 3: return 5\n        x_coords, y_coords = zip(*visible_points); min_x, max_x = min(x_coords), max(x_coords); min_y, max_y = min(y_coords), max(y_coords)\n        if max_x <= min_x or max_y <= min_y: return 5\n        torso_parse_area = parse_array[min_y:max_y, min_x:max_x]; unique_labels, counts = np.unique(torso_parse_area, return_counts=True); non_zero_mask = (unique_labels != 0)\n        if np.any(non_zero_mask): return unique_labels[non_zero_mask][np.argmax(counts[non_zero_mask])]\n        else: return 5\n    def __getitem__(self, idx):\n        image_name = self.image_files[idx]; base_name = os.path.splitext(image_name)[0]\n        person_image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB'); cloth_image = Image.open(os.path.join(self.cloth_dir, image_name)).convert('RGB'); cloth_mask = Image.open(os.path.join(self.cloth_mask_dir, image_name)).convert('L')\n        try:\n            with open(os.path.join(self.pose_dir, f\"{base_name}_keypoints.json\"), 'r') as f: pose_data = json.load(f)\n            pose_keypoints = np.array(pose_data['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n        except (FileNotFoundError, IndexError): pose_keypoints = np.zeros((25, 3), dtype=np.float32)\n        parse_path = os.path.join(self.parse_dir, f\"{base_name}.png\")\n        if not os.path.exists(parse_path): parse_path = os.path.join(self.parse_dir, f\"{base_name}.jpg\")\n        parse_array_orig = np.array(Image.open(parse_path).convert('L'))\n        upper_cloth_label = self.find_upper_cloth_label(pose_keypoints, parse_array_orig)\n        parse_array_resized = cv2.resize(parse_array_orig, self.image_size[::-1], interpolation=cv2.INTER_NEAREST)\n        person_cloth_mask = (parse_array_resized == upper_cloth_label).astype(np.float32)\n        person_image_tensor = self.transform(person_image); cloth_image_tensor = self.transform(cloth_image); cloth_mask_tensor = self.mask_transform(cloth_mask)\n        pose_map_tensor = torch.from_numpy(self.create_pose_map(pose_keypoints)).float()\n        blurred_mask_tensor = torch.from_numpy(cv2.GaussianBlur(person_cloth_mask, (5, 5), 0)).unsqueeze(0)\n        agnostic_person_tensor = person_image_tensor * (1 - blurred_mask_tensor)\n        warped_cloth_tensor = person_image_tensor * torch.from_numpy(person_cloth_mask).unsqueeze(0)\n        return {'person_image': person_image_tensor, 'cloth_image': cloth_image_tensor, 'cloth_mask': cloth_mask_tensor, 'agnostic_person': agnostic_person_tensor, 'pose_map': pose_map_tensor, 'warped_cloth': warped_cloth_tensor}\n    def create_pose_map(self, keypoints):\n        h, w = self.image_size; orig_w, orig_h = self.original_image_size; num_keypoints = keypoints.shape[0]\n        pose_map = np.zeros((num_keypoints, h, w), dtype=np.float32)\n        for i, point in enumerate(keypoints):\n            if point[2] > 0.1:\n                x, y = int(point[0] * w / orig_w), int(point[1] * h / orig_h)\n                if 0 <= x < w and 0 <= y < h: cv2.circle(pose_map[i], (x, y), radius=3, color=1, thickness=-1)\n        return pose_map\n\nclass GMM(nn.Module):\n    def __init__(self, in_channels_cloth=3, in_channels_pose=25, out_channels_flow=2):\n        super(GMM, self).__init__(); self.encoder1 = self.conv_block(in_channels_cloth + in_channels_pose, 64); self.encoder2 = self.conv_block(64, 128); self.encoder3 = self.conv_block(128, 256); self.encoder4 = self.conv_block(256, 512); self.pool = nn.MaxPool2d(2, 2); self.bottleneck = self.conv_block(512, 1024); self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2); self.decoder4 = self.conv_block(1024, 512); self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2); self.decoder3 = self.conv_block(512, 256); self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2); self.decoder2 = self.conv_block(256, 128); self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2); self.decoder1 = self.conv_block(128, 64); self.conv_out = nn.Conv2d(64, out_channels_flow, kernel_size=1); self.tanh = nn.Tanh()\n    def conv_block(self, in_channels, out_channels): return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True))\n    def forward(self, cloth_image, pose_map):\n        x = torch.cat([cloth_image, pose_map], dim=1); e1 = self.encoder1(x); p1 = self.pool(e1); e2 = self.encoder2(p1); p2 = self.pool(e2); e3 = self.encoder3(p2); p3 = self.pool(e3); e4 = self.encoder4(p3); p4 = self.pool(e4); b = self.bottleneck(p4); d4 = self.upconv4(b); d4 = torch.cat([d4, e4], dim=1); d4 = self.decoder4(d4); d3 = self.upconv3(d4); d3 = torch.cat([d3, e3], dim=1); d3 = self.decoder3(d3); d2 = self.upconv2(d3); d2 = torch.cat([d2, e2], dim=1); d2 = self.decoder2(d2); d1 = self.upconv1(d2); d1 = torch.cat([d1, e1], dim=1); d1 = self.decoder1(d1); flow_field = self.conv_out(d1); flow_field = self.tanh(flow_field); return flow_field\n\ndef warp_cloth_with_flow(cloth_image, flow_field):\n    flow_field = flow_field.permute(0, 2, 3, 1); warped_image = F.grid_sample(cloth_image, flow_field, mode='bilinear', padding_mode='zeros', align_corners=True); return warped_image\n\nclass TVLoss(nn.Module):\n    def __init__(self): super(TVLoss, self).__init__()\n    def forward(self, x):\n        batch_size, c, h, w = x.size(); tv_h = torch.pow(x[:,:,1:,:] - x[:,:,:-1,:], 2).sum(); tv_w = torch.pow(x[:,:,:,1:] - x[:,:,:,:-1], 2).sum()\n        return (tv_h + tv_w) / (batch_size * c * h * w)\n\ndef setup(rank, world_size, sync_file):\n    init_method = f'file://{sync_file}'\n    dist.init_process_group(\"nccl\", init_method=init_method, rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef train(rank, world_size, sync_file):\n    setup(rank, world_size, sync_file)\n    DATA_ROOT = '/kaggle/input/clothe/clothes_tryon_dataset/train'; CHECKPOINT_DIR = '/kaggle/working/coarse_gmm_checkpoints_new1/'; VISUALIZATION_DIR = '/kaggle/working/coarse_gmm_visuals_new/'\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True); os.makedirs(VISUALIZATION_DIR, exist_ok=True)\n    BATCH_SIZE = 16; NUM_EPOCHS = 50; LEARNING_RATE = 2e-5; IMAGE_SIZE = (256, 192); ORIGINAL_IMAGE_SIZE = (768, 1024)\n    torch.cuda.set_device(rank)\n    model = GMM(in_channels_pose=25).to(rank) \n    model = DDP(model, device_ids=[rank])\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n    l1_loss_fn = nn.L1Loss().to(rank); tv_loss_fn = TVLoss().to(rank)\n    scaler = torch.cuda.amp.GradScaler()\n    start_epoch = 0; checkpoint_path = os.path.join(CHECKPOINT_DIR, 'coarse_gmm_latest.pth')\n    if os.path.isfile(checkpoint_path) and rank == 0:\n        checkpoint = torch.load(checkpoint_path, map_location={'cuda:0': f'cuda:{rank}'})\n        model.module.load_state_dict(checkpoint['model_state_dict']); optimizer.load_state_dict(checkpoint['optimizer_state_dict']); scaler.load_state_dict(checkpoint['scaler_state_dict']); start_epoch = checkpoint['epoch'] + 1\n    dist.barrier()\n    dataset = FashionVTONDataset(data_root=DATA_ROOT, image_size=IMAGE_SIZE, original_image_size=ORIGINAL_IMAGE_SIZE)\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)\n    for epoch in range(start_epoch, NUM_EPOCHS):\n        sampler.set_epoch(epoch); model.train()\n        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", disable=(rank != 0))\n        for batch in progress_bar:\n            optimizer.zero_grad()\n            cloth_image = batch['cloth_image'].to(rank)\n            cloth_mask = batch['cloth_mask'].to(rank) # <<< LOAD THE MASK\n            pose_map = batch['pose_map'].to(rank)\n            ground_truth_warped = batch['warped_cloth'].to(rank)\n            \n            with torch.cuda.amp.autocast():\n                predicted_flow = model(cloth_image, pose_map)\n                predicted_warped = warp_cloth_with_flow(cloth_image, predicted_flow)\n                \n                # --- THIS IS THE FIX: Warp the cloth mask as well ---\n                warped_cloth_mask = F.grid_sample(cloth_mask, predicted_flow.permute(0, 2, 3, 1), mode='bilinear', padding_mode='zeros', align_corners=True)\n                \n                # --- And apply it to the L1 Loss ---\n                loss_l1 = l1_loss_fn(predicted_warped * warped_cloth_mask, ground_truth_warped * warped_cloth_mask)\n                loss_tv = tv_loss_fn(predicted_flow)\n                total_loss = loss_l1 + 0.5 * loss_tv\n\n            scaler.scale(total_loss).backward(); scaler.step(optimizer); scaler.update()\n            if rank == 0: progress_bar.set_postfix(loss=total_loss.item())\n                    # ... inside the \"if rank == 0:\" block at the end of the epoch loop ...\n            with torch.no_grad():\n                # --- THIS IS THE FIX ---\n                # We should visualize the ground_truth_warped image, not the mask.\n                vis_batch = torch.cat([\n                    (cloth_image.cpu() + 1) / 2, \n                    (predicted_warped.cpu().detach() + 1) / 2, \n                    (ground_truth_warped.cpu() + 1) / 2\n                ], dim=0)\n                save_image(vis_batch, os.path.join(VISUALIZATION_DIR, f'epoch_{epoch+1}_comparison.png'), nrow=BATCH_SIZE)\n    if rank == 0:\n        final_model_path = os.path.join(CHECKPOINT_DIR, 'coarse_gmm_final.pth'); torch.save(model.module.state_dict(), final_model_path); print(f\"Final model saved to {final_model_path}\")\n    cleanup()\n\nif __name__ == '__main__':\n    world_size = torch.cuda.device_count()\n    if world_size < 2: print(\"Distributed training requires at least 2 GPUs.\")\n    else:\n        sync_file_path = os.path.join('/kaggle/working', 'ddp_sync_file')\n        if os.path.exists(sync_file_path): os.remove(sync_file_path)\n        torch.multiprocessing.spawn(train, args=(world_size, sync_file_path), nprocs=world_size, join=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:24:51.449365Z","iopub.execute_input":"2025-07-16T19:24:51.449619Z","iopub.status.idle":"2025-07-16T19:24:51.459554Z","shell.execute_reply.started":"2025-07-16T19:24:51.449599Z","shell.execute_reply":"2025-07-16T19:24:51.458942Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_coarse_gmm.py\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"!python train_coarse_gmm.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:24:54.796335Z","iopub.execute_input":"2025-07-16T19:24:54.796896Z","iopub.status.idle":"2025-07-16T19:49:57.091471Z","shell.execute_reply.started":"2025-07-16T19:24:54.796875Z","shell.execute_reply":"2025-07-16T19:49:57.087170Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/train_coarse_gmm.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n/kaggle/working/train_coarse_gmm.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\nEpoch 1/50:   0%|                                       | 0/364 [00:00<?, ?it/s]/kaggle/working/train_coarse_gmm.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/kaggle/working/train_coarse_gmm.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1/50: 100%|█████████████████| 364/364 [05:53<00:00,  1.03it/s, loss=0.537]\nEpoch 2/50: 100%|█████████████████| 364/364 [05:52<00:00,  1.03it/s, loss=0.573]\nEpoch 3/50: 100%|█████████████████| 364/364 [05:49<00:00,  1.04it/s, loss=0.576]\nEpoch 4/50: 100%|█████████████████| 364/364 [05:47<00:00,  1.05it/s, loss=0.493]\nEpoch 5/50:  23%|████              | 83/364 [01:31<04:17,  1.09it/s, loss=0.509]^C\nTraceback (most recent call last):\n  File \"/kaggle/working/train_coarse_gmm.py\", line 150, in <module>\n    torch.multiprocessing.spawn(train, args=(world_size, sync_file_path), nprocs=world_size, join=True)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 340, in spawn\n    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 296, in start_processes\n    while not context.join():\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 144, in join\n    ready = multiprocessing.connection.wait(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n    ready = selector.select(timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n    fd_event_list = self._selector.poll(timeout)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}